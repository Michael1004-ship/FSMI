import os
import re
import time
import json
import pandas as pd
from io import BytesIO
from datetime import datetime
import sys
import subprocess
import logging

from newspaper import Article
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
from google.cloud import storage
import newspaper

# ----------------------------
# ì„¤ì •
# ----------------------------

BUCKET_RAW = "emotion-raw-data"
BUCKET_INDEX = "emotion-index-data"
GDELT_PREFIX = "news/gdelt/"
SLEEP_SECONDS = 2.5
TARGET_DATE = datetime.utcnow().strftime("%Y-%m-%d")  # UTC ê¸°ì¤€ ì‚¬ìš©
SAVE_FILENAME = "news_anxiety_index.csv"

# ì‹¤íŒ¨í•œ URL ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘ìš©
failed_urls = []

# ê²½ë¡œ ì„¤ì •
PROJECT_ROOT = "/home/hwangjeongmun691/projects/emotionFinPoli"
SCRIPTS_DIR = os.path.join(PROJECT_ROOT, "scripts")
BUILD_INDEX_DIR = os.path.join(PROJECT_ROOT, "building_index")

# ----------------------------
# ëª¨ë¸ ë¡œë”© (FinBERT)
# ----------------------------

tokenizer = AutoTokenizer.from_pretrained("yiyanghkust/finbert-tone")
model = AutoModelForSequenceClassification.from_pretrained("yiyanghkust/finbert-tone")
finbert = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)

# ----------------------------
# GCS ì—°ê²°
# ----------------------------

client = storage.Client()
bucket_raw = client.bucket(BUCKET_RAW)
bucket_index = client.bucket(BUCKET_INDEX)

# ----------------------------
# ë‚ ì§œ í´ë” ë‚´ JSON íŒŒì¼ ì°¾ê¸°
# ----------------------------

prefix_path = f"{GDELT_PREFIX}{TARGET_DATE}/"
blobs = client.list_blobs(BUCKET_RAW, prefix=prefix_path)

urls = []
for blob in blobs:
    if not blob.name.endswith(".json"):
        continue
    content = blob.download_as_bytes()
    try:
        items = json.load(BytesIO(content))
        for item in items:
            if "DocumentIdentifier" in item:
                urls.append(item["DocumentIdentifier"])
    except Exception as e:
        print(f"[!] JSON íŒŒì‹± ì˜¤ë¥˜: {blob.name} - {e}")
        continue

print(f"ğŸ”— ì´ {len(urls)}ê°œì˜ ê¸°ì‚¬ URL ìˆ˜ì§‘ë¨")

# ----------------------------
# ë³¸ë¬¸ í¬ë¡¤ë§ ë° ë¶„ì„
# ----------------------------

# ë¡œê¹… ì„¤ì •
import os
from datetime import datetime

# ë¡œê·¸ ë””ë ‰í† ë¦¬ ì„¤ì •
LOG_ROOT = "/home/hwangjeongmun691/logs"
today = datetime.utcnow().strftime("%Y-%m-%d")
LOG_DATE_DIR = f"{LOG_ROOT}/{today}"

# ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs(LOG_DATE_DIR, exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f"{LOG_DATE_DIR}/gdelt_finbert.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("gdelt_finbert")

def fetch_article_text(url):
    try:
        print(f"[*] í¬ë¡¤ë§ ì‹œë„ ì¤‘: {url}")  # ì§„í–‰ ìƒí™© í‘œì‹œ
        config = newspaper.Config()
        config.browser_user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36"
        
        article = Article(url, config=config)
        print(f"[*] ë‹¤ìš´ë¡œë“œ ì‹œì‘...")
        article.download()
        print(f"[*] íŒŒì‹± ì‹œì‘...")
        article.parse()
        text = article.text.strip()
        print(f"[+] ì„±ê³µ: {len(text)} ê¸€ì ì¶”ì¶œ")
        return text
    except Exception as e:
        print(f"[X] í¬ë¡¤ë§ ì‹¤íŒ¨: {url}")
        print(f"[X] ì—ëŸ¬ íƒ€ì…: {type(e).__name__}")
        print(f"[X] ì—ëŸ¬ ë‚´ìš©: {str(e)}")
        failed_urls.append(url)  # ì‹¤íŒ¨í•œ URL ì¶”ê°€
        return None

# ë³¸ë¬¸ í¬ë¡¤ë§ ë¶€ë¶„ë„ ìˆ˜ì •
print("\ní¬ë¡¤ë§ ì‹œì‘...")
results = []   # í‰ê·  ê³„ì‚°ìš© (negativeë§Œ)
details = []   # ì „ì²´ ê°ì • ë¶„ì„ ê²°ê³¼ ì €ì¥ìš©

for i, url in enumerate(urls, 1):
    print(f"\n[{i}/{len(urls)}] URL ì²˜ë¦¬ ì¤‘...")
    text = fetch_article_text(url)
    time.sleep(SLEEP_SECONDS)

    if text:
        try:
            result = finbert(text[:512])[0]
            print(f"[+] ê°ì„± ë¶„ì„ ê²°ê³¼: {result['label']} ({result['score']:.3f})")
            
            # í‰ê·  ê³„ì‚°ìš©: negativeë§Œ ë”°ë¡œ
            if result["label"] == "negative":
                results.append(result["score"])
            
            # ì „ì²´ ê¸°ì‚¬ ì €ì¥ìš©: ë¬´ì¡°ê±´ ì €ì¥
            details.append([url, result["label"], result["score"]])
        except Exception as e:
            logger.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
            failed_urls.append(url)  # FinBERT ë¶„ì„ ì‹¤íŒ¨í•œ URLë„ ì¶”ê°€
            continue

# ----------------------------
# ê²°ê³¼ ì§‘ê³„ ë° ì €ì¥
# ----------------------------

# ê²°ê³¼ ì €ì¥ ê²½ë¡œ
save_path = f"news/{TARGET_DATE}/{SAVE_FILENAME}"
temp_file = "/tmp/news_anxiety_index.csv"

# ì§€ìˆ˜ ê³„ì‚° - ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´ ë¹„êµ
total_count = len(details)
negatives = [row for row in details if row[1].lower() == "negative"]  # ì†Œë¬¸ìë¡œ ë³€í™˜í•˜ì—¬ ë¹„êµ
negative_ratio = len(negatives) / total_count if total_count > 0 else 0
average_negative_score = sum(row[2] for row in negatives) / len(negatives) if negatives else 0
anxiety_index = negative_ratio * average_negative_score

# CSV ì‘ì„±
with open(temp_file, "w", encoding="utf-8") as f:
    # 1ë¶€: ì§€ìˆ˜ ìš”ì•½
    f.write("negative_ratio,average_negative_score,anxiety_index\n")
    f.write(f"{negative_ratio:.3f},{average_negative_score:.3f},{anxiety_index:.3f}\n\n")

    # 2ë¶€: ê°ì • ë¶„ì„ ê²°ê³¼
    f.write("url,label,score\n")
    for row in details:
        f.write(f"{row[0]},{row[1]},{row[2]:.3f}\n")

# ì—…ë¡œë“œ
blob = bucket_index.blob(save_path)
blob.upload_from_filename(temp_file)

print(f"âœ… ì €ì¥ ì™„ë£Œ â†’ gs://{BUCKET_INDEX}/{save_path}")

# ì‹¤íŒ¨ ë¡œê·¸ ì €ì¥
if failed_urls:
    log_path = f"news/{TARGET_DATE}/failed_urls.txt"
    temp_log_file = "/tmp/failed_urls.txt"
    with open(temp_log_file, "w") as f:
        for url in failed_urls:
            f.write(url + "\n")
    log_blob = bucket_index.blob(log_path)
    log_blob.upload_from_filename(temp_log_file)
    print(f"âš ï¸ ì‹¤íŒ¨í•œ URL ë¡œê·¸ ì €ì¥ë¨ â†’ gs://{BUCKET_INDEX}/{log_path}")
    print(f"ì´ {len(failed_urls)}ê°œì˜ URL ì²˜ë¦¬ ì‹¤íŒ¨")

def run_script(script_path, description, args=None, retry=1):
    """ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ê³  ìƒì„¸ ë””ë²„ê¹… ì •ë³´ë¥¼ ë¡œê¹…í•©ë‹ˆë‹¤."""
    for attempt in range(1, retry+1):
        try:
            # íŠ¹ìˆ˜ë¬¸ì ê²½ë¡œ ë¬¸ì œ í•´ê²°ì„ ìœ„í•´ ë¬¸ìì—´ë¡œ ì „ë‹¬
            script_path = str(script_path)
            
            logger.info(f"âœ¨ {description} ì‹œì‘ (ì‹œë„ {attempt}/{retry})...")
            logger.debug(f"ì‹¤í–‰ ëª…ë ¹: {sys.executable} \"{script_path}\" {args if args else ''}")
            
            # íŠ¹ìˆ˜ë¬¸ìê°€ ìˆëŠ” ê²½ë¡œëŠ” list í˜•íƒœë¡œ ì „ë‹¬í•´ subprocessì—ì„œ ìë™ ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬
            cmd = [sys.executable, script_path]
            if args:
                cmd.extend(args)
            
            # ê²½ë¡œ ë””ë²„ê¹…
            logger.debug(f"ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€: {os.path.exists(script_path)}")
            
            result = subprocess.run(cmd, capture_output=True, text=True, check=True)
            # ë‚˜ë¨¸ì§€ ì½”ë“œ...
        except Exception as e:
            logger.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            failed_urls.append(url)  # FinBERT ë¶„ì„ ì‹¤íŒ¨í•œ URLë„ ì¶”ê°€
            continue

def main():
    # ...
    
    # ì ˆëŒ€ ê²½ë¡œë¡œ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
    gdelt_collector = os.path.join(SCRIPTS_DIR, "gdelt_realtime_collector.py")
    reddit_collector = os.path.join(SCRIPTS_DIR, "reddit_realtime_collector.py")
    gdelt_finbert = os.path.join(SCRIPTS_DIR, "gdelt_realtime_crawling&FinBERT.py")
    reddit_finbert = os.path.join(SCRIPTS_DIR, "reddit_FinBERT.py")
    reddit_roberta = os.path.join(SCRIPTS_DIR, "reddit_RoBERTa.py")
    build_index = os.path.join(BUILD_INDEX_DIR, "build_anxiety_index.py")
    
    # ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì „ ì¡´ì¬ í™•ì¸
    for script in [gdelt_collector, reddit_collector, gdelt_finbert, reddit_finbert, reddit_roberta, build_index]:
        if os.path.exists(script):
            logger.debug(f"âœ“ ìŠ¤í¬ë¦½íŠ¸ í™•ì¸: {script}")
        else:
            logger.error(f"âœ— ìŠ¤í¬ë¦½íŠ¸ ì—†ìŒ: {script}")
    
    # ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
    gdelt_success = run_script(gdelt_collector, "GDELT ë°ì´í„° ìˆ˜ì§‘", retry=2)
    # ë‚˜ë¨¸ì§€ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰...
