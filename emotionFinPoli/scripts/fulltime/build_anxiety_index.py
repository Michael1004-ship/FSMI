from datetime import datetime, timedelta
import pandas as pd
import numpy as np
from google.cloud import storage
import argparse
from pathlib import Path
from io import StringIO
import logging
import os
import sys
import concurrent.futures
from tqdm import tqdm
import time

# ë¡œê·¸ ë””ë ‰í† ë¦¬ ì„¤ì •
LOG_ROOT = "/home/hwangjeongmun691/logs"
today = datetime.utcnow().strftime("%Y-%m-%d")
LOG_DATE_DIR = f"{LOG_ROOT}/{today}"

# ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs(LOG_DATE_DIR, exist_ok=True)

# ìŠ¤í¬ë¦½íŠ¸ ì´ë¦„ ê¸°ë°˜ìœ¼ë¡œ ë¡œê·¸ íŒŒì¼ ì„¤ì •
script_name = os.path.basename(__file__)
log_file = f"{LOG_DATE_DIR}/{script_name.replace('.py', '.log')}"

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("build_anxiety_index")

# ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì •
MAX_WORKERS = 4  # ë™ì‹œ ì²˜ë¦¬í•  ìµœëŒ€ ë‚ ì§œ ìˆ˜

def compute_anxiety_score(df):
    """
    Compute the anxiety score based on the negative ratio and average negative score.
    Args:
        df: DataFrame containing the data.
    Returns:
        Computed anxiety score (negative_ratio * average_negative_score ^ 1.5).
    """
    ratio = float(df.iloc[1, 1])  # negative_ratio
    avg_score = float(df.iloc[1, 2])  # average_negative_score
    logger.debug(f"Negative Ratio: {ratio}, Average Negative Score: {avg_score}")  # ë””ë²„ê¹…ìš© ì¶œë ¥
    anxiety_score = ratio * (avg_score ** 1.5)  # ë¹„ì„ í˜• ì²˜ë¦¬
    return ratio, avg_score, anxiety_score


def compute_std(df, source):
    """
    Compute standard deviation for a given column based on source.
    Args:
        df: DataFrame containing the data.
        source: Data source ('news' or 'reddit_*').
    Returns:
        Standard deviation and scores for the column.
    """
    if source == "news":
        scores = df.iloc[4:, 2].astype(float)  # ë‰´ìŠ¤ëŠ” 3ì—´ (index 2)
    else:
        scores = df.iloc[4:, 1].astype(float)  # ë ˆë”§ì€ 2ì—´ (index 1)
    return scores.std(), scores


def load_and_process(bucket_name, blob_path, source: str):
    """
    Load CSV file from GCS, compute the anxiety score, and Z-scores.
    """
    storage_client = storage.Client()
    bucket = storage_client.bucket(bucket_name)
    blob = bucket.blob(blob_path)
    
    if not blob.exists():
        raise FileNotFoundError(f"File not found in GCS: gs://{bucket_name}/{blob_path}")
    
    content = blob.download_as_text()
    df = pd.read_csv(StringIO(content), header=None)

    if source == "news":
        # âœ… ë‰´ìŠ¤ëŠ” ë¹„ì¤‘ * ê°•ë„ ê¸°ë°˜ìœ¼ë¡œ anxiety_score ì‹œê³„ì—´ ìƒì„±
        ratio = float(df.iloc[1, 1])
        avg_score = float(df.iloc[1, 2])
        logger.debug(f"[NEWS] Negative Ratio: {ratio}, Avg Score: {avg_score}")

        # ë‰´ìŠ¤ ì‹œê³„ì—´ ì ìˆ˜ëŠ” 4ë²ˆì§¸ í–‰ë¶€í„° ì‹œì‘ (ë¶€ì • ë¬¸ì¥ë“¤ì˜ ê°ì • ì ìˆ˜)
        scores = df.iloc[4:, 2].astype(float)
        # anxiety_score ì‹œê³„ì—´: ë¹„ì¤‘ * ì ìˆ˜^1.5
        anxiety_scores = ratio * (scores ** 1.5)
        mean_anx = anxiety_scores.mean()
        std_anx = anxiety_scores.std()
        z_scores = (anxiety_scores - mean_anx) / std_anx
        z_scores = np.clip(z_scores, -3, 3)  # âœ… í´ë¦¬í•‘
        z_scores = np.exp(z_scores)         # âœ… ì§€ìˆ˜í™”
        z_scores.name = "news_z"

        return ratio, avg_score, std_anx, anxiety_scores.mean(), z_scores
    else:
        # Reddit ë°ì´í„° ì²˜ë¦¬ (ê¸°ì¡´ ë°©ì‹ ìœ ì§€)
        ratio = float(df.iloc[1, 1])  # negative_ratio
        avg_score = float(df.iloc[1, 2])  # average_negative_score
        logger.debug(f"[REDDIT] {source} - Ratio: {ratio}, Avg Score: {avg_score}")
        
        scores = df.iloc[4:, 1].astype(float)  # ë ˆë”§ì€ 2ì—´ (index 1)
        std = scores.std()
        z_scores = (scores - avg_score) / std
        z_scores = np.clip(z_scores, -5, 5)  # âœ… í´ë¦¬í•‘
        z_scores.name = source + "_z"
        
        anxiety_score = ratio * (avg_score ** 1.5)  # ë¹„ì„ í˜• ì²˜ë¦¬
        
        return ratio, avg_score, std, anxiety_score, z_scores


def upload_to_gcs(bucket_name, destination_blob_path, local_file_path):
    """
    Upload the final anxiety index CSV to GCS.
    Args:
        bucket_name: Name of the GCS bucket.
        destination_blob_path: Destination path in the GCS bucket.
        local_file_path: Local file path to upload.
    """
    storage_client = storage.Client()
    bucket = storage_client.bucket(bucket_name)
    blob = bucket.blob(destination_blob_path)
    blob.upload_from_filename(local_file_path)
    logger.info(f"â›… Uploaded to GCS: gs://{bucket_name}/{destination_blob_path}")


def check_if_already_processed(bucket_name, destination_blob_path):
    """
    ìµœì¢… ê²°ê³¼ íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    """
    storage_client = storage.Client()
    bucket = storage_client.bucket(bucket_name)
    blob = bucket.blob(destination_blob_path)
    return blob.exists()


def build_anxiety_index(date_str):
    """
    Build the final anxiety index by processing news and reddit data.
    Args:
        date_str: The date string (YYYY-MM-DD) for the folder name.
    Returns:
        Dict with status and result information
    """
    start_time = time.time()
    result_info = {
        "date": date_str,
        "success": False,
        "error": None,
        "index": None
    }
    
    try:
        bucket_name = "emotion-index-data"
        destination_blob_path = f"final_anxiety_index/{date_str}/anxiety_index_final.csv"
        
        # ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼ì¸ì§€ í™•ì¸í•˜ê³  ë¡œê·¸ë§Œ ì¶œë ¥
        if check_if_already_processed(bucket_name, destination_blob_path):
            logger.warning(f"âš ï¸ í•´ë‹¹ ë‚ ì§œ({date_str})ì˜ ì§€ìˆ˜ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. íŒŒì¼ì„ ë®ì–´ì”ë‹ˆë‹¤.")
            logger.info(f"ê¸°ì¡´ íŒŒì¼: gs://{bucket_name}/{destination_blob_path}")
        
        # ë‚˜ë¨¸ì§€ ì²˜ë¦¬ ê³„ì† ì§„í–‰
        news_blob_path = f"news/{date_str}/news_anxiety_index.csv"
        reddit_finbert_blob_path = f"reddit/{date_str}/reddit_anxiety_index.csv"  # FinBERT
        reddit_roberta_blob_path = f"reddit/{date_str}/reddit_anxiety_roberta.csv"  # RoBERTa

        # ë‰´ìŠ¤ ë°ì´í„° ì²˜ë¦¬
        news_ratio, news_avg, news_std, news_anx, news_z = load_and_process(bucket_name, news_blob_path, "news")

        # ë ˆë”§ ë°ì´í„° ì²˜ë¦¬ (FinBERTì™€ RoBERTa)
        rf_ratio, rf_avg, rf_std, rf_anx, rf_z = load_and_process(bucket_name, reddit_finbert_blob_path, "reddit_finbert")
        rr_ratio, rr_avg, rr_std, rr_anx, rr_z = load_and_process(bucket_name, reddit_roberta_blob_path, "reddit_roberta")

        # ë ˆë”§ ë°ì´í„° ê²°í•© (FinBERT + RoBERTa)
        combined_reddit_z = np.exp((rf_z + rr_z) / 2)
        reddit_combined_score = combined_reddit_z.mean()
        
        # âœ… ì—¬ê¸°ì„œë§Œ í´ë¦¬í•‘
        news_z_clipped = news_z.clip(-3, 3)
        reddit_z_clipped = combined_reddit_z.clip(-3, 3)
        
        # ìµœì¢… ì§€ìˆ˜ ê³„ì‚°
        final_index_series = np.exp(0.3 * news_z_clipped + 0.7 * reddit_z_clipped)
        final_index = final_index_series.replace([np.inf, -np.inf], np.nan).dropna().mean()

        # ë¡œì»¬ì— ì €ì¥ í›„ GCS ì—…ë¡œë“œ
        output_dir = Path("output") / date_str
        output_dir.mkdir(parents=True, exist_ok=True)
        output_file = output_dir / "anxiety_index_final.csv"
        
        # ê¸°ë³¸ DataFrame ìƒì„± (ìˆ˜ì •: ëª¨ë“  í–‰ì„ í•œ ë²ˆì— ì •ì˜)
        final_df = pd.DataFrame([
            ["Total", "", "", "", final_index],
            ["News", news_ratio, news_avg, news_std, news_anx],
            ["Reddit_FinBERT", rf_ratio, rf_avg, rf_std, rf_anx],
            ["Reddit_RoBERTa", rr_ratio, rr_avg, rr_std, rr_anx],
            ["Reddit_Combined", "", "", "", reddit_combined_score]
        ], columns=["Type", "Ratio", "Avg Score", "Std", "Anxiety Index"])
        
        final_df.to_csv(output_file, index=False)

        logger.info(f"âœ… {date_str} - ë¡œì»¬ ì €ì¥ ì™„ë£Œ: {output_file}")

        # GCSì— ì—…ë¡œë“œ
        upload_to_gcs(bucket_name, destination_blob_path, str(output_file))
        
        elapsed_time = time.time() - start_time
        logger.info(f"âœ… {date_str} - ë¶ˆì•ˆ ì§€ìˆ˜: {final_index:.4f} (ì²˜ë¦¬ ì‹œê°„: {elapsed_time:.1f}ì´ˆ)")
        
        result_info["success"] = True
        result_info["index"] = final_index
        result_info["elapsed_time"] = elapsed_time
        
        return result_info
        
    except FileNotFoundError as e:
        elapsed_time = time.time() - start_time
        logger.error(f"âŒ {date_str} ì²˜ë¦¬ ì‹¤íŒ¨: í•„ìš”í•œ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        logger.error(str(e))
        
        result_info["success"] = False
        result_info["error"] = f"FileNotFoundError: {str(e)}"
        result_info["elapsed_time"] = elapsed_time
        
        return result_info
        
    except Exception as e:
        elapsed_time = time.time() - start_time
        logger.error(f"âŒ {date_str} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
        logger.error(str(e))
        
        result_info["success"] = False
        result_info["error"] = str(e)
        result_info["elapsed_time"] = elapsed_time
        
        return result_info


def get_date_range(start_date, end_date):
    """ì‹œì‘ì¼ê³¼ ì¢…ë£Œì¼ ì‚¬ì´ì˜ ëª¨ë“  ë‚ ì§œë¥¼ ìƒì„±"""
    start = datetime.strptime(start_date, "%Y-%m-%d")
    end = datetime.strptime(end_date, "%Y-%m-%d")
    date_list = []
    current = start
    while current <= end:
        date_list.append(current.strftime("%Y-%m-%d"))
        current += timedelta(days=1)
    return date_list

def process_date_range(start_date, end_date, workers=MAX_WORKERS, debug=False):
    """ì£¼ì–´ì§„ ë‚ ì§œ ë²”ìœ„ì˜ ë°ì´í„°ë¥¼ ë³‘ë ¬ ì²˜ë¦¬"""
    date_list = get_date_range(start_date, end_date)
    logger.info(f"ì²˜ë¦¬í•  ë‚ ì§œ: {date_list} (ì´ {len(date_list)}ì¼)")
    logger.info(f"ë³‘ë ¬ ì²˜ë¦¬ ìŠ¤ë ˆë“œ ìˆ˜: {workers}ê°œ")
    
    if debug:
        logger.setLevel(logging.DEBUG)
        logger.info("ğŸ”§ ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™”")
    
    results = []
    success_count = 0
    failed_dates = []
    
    # ì „ì²´ ì‹œì‘ ì‹œê°„
    total_start_time = time.time()
    
    # ë³‘ë ¬ ì²˜ë¦¬
    with tqdm(total=len(date_list), desc="ë¶ˆì•ˆ ì§€ìˆ˜ ê³„ì‚° ì¤‘", unit="ì¼") as pbar:
        with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:
            # ë‚ ì§œë³„ ì‘ì—… ì œì¶œ
            future_to_date = {executor.submit(build_anxiety_index, date): date for date in date_list}
            
            # ê²°ê³¼ ì²˜ë¦¬
            for future in concurrent.futures.as_completed(future_to_date):
                date = future_to_date[future]
                try:
                    result = future.result()
                    results.append(result)
                    
                    if result["success"]:
                        success_count += 1
                    else:
                        failed_dates.append(date)
                    
                except Exception as e:
                    logger.error(f"âŒ {date} ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {str(e)}")
                    failed_dates.append(date)
                
                # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
                pbar.update(1)
                
                # ì£¼ê¸°ì  ìƒíƒœ ë³´ê³ 
                if pbar.n % max(1, len(date_list) // 5) == 0 or pbar.n == len(date_list):
                    elapsed = time.time() - total_start_time
                    remaining = (elapsed / pbar.n) * (len(date_list) - pbar.n) if pbar.n > 0 else 0
                    
                    logger.info(f"ğŸ”„ ì§„í–‰ë¥ : {pbar.n/len(date_list)*100:.1f}% ({pbar.n}/{len(date_list)}) - "
                              f"ì„±ê³µ: {success_count}, ì‹¤íŒ¨: {len(failed_dates)} - "
                              f"ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {remaining/60:.1f}ë¶„")
    
    # ì „ì²´ ì†Œìš” ì‹œê°„ ê³„ì‚°
    total_elapsed = time.time() - total_start_time
    
    # ìµœì¢… ê²°ê³¼ ìš”ì•½
    logger.info("\n" + "="*50)
    logger.info("ğŸ“Š ë¶ˆì•ˆ ì§€ìˆ˜ ê³„ì‚° ìµœì¢… ê²°ê³¼")
    logger.info(f"â€¢ ì²˜ë¦¬ ê¸°ê°„: {start_date} ~ {end_date}")
    logger.info(f"â€¢ ì´ ì²˜ë¦¬ ë‚ ì§œ: {len(date_list)}ì¼")
    logger.info(f"â€¢ ì„±ê³µ: {success_count}ì¼")
    logger.info(f"â€¢ ì‹¤íŒ¨: {len(failed_dates)}ì¼")
    
    if failed_dates:
        logger.info("â€¢ ì‹¤íŒ¨í•œ ë‚ ì§œ:")
        for date in failed_dates:
            logger.info(f"  - {date}")
    
    # ì„±ê³µí•œ ê²°ê³¼ì— ëŒ€í•œ ì§€ìˆ˜ í†µê³„
    successful_indices = [r["index"] for r in results if r["success"] and r["index"] is not None]
    if successful_indices:
        avg_index = sum(successful_indices) / len(successful_indices)
        min_index = min(successful_indices)
        max_index = max(successful_indices)
        
        logger.info(f"â€¢ í‰ê·  ë¶ˆì•ˆ ì§€ìˆ˜: {avg_index:.4f}")
        logger.info(f"â€¢ ìµœì†Œ ë¶ˆì•ˆ ì§€ìˆ˜: {min_index:.4f}")
        logger.info(f"â€¢ ìµœëŒ€ ë¶ˆì•ˆ ì§€ìˆ˜: {max_index:.4f}")
    
    logger.info(f"â€¢ ì´ ì†Œìš” ì‹œê°„: {total_elapsed:.1f}ì´ˆ ({total_elapsed/60:.1f}ë¶„)")
    logger.info("="*50)
    
    return success_count, len(failed_dates)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ë¶ˆì•ˆ ì§€ìˆ˜ ê³„ì‚° ë„êµ¬ (ë³‘ë ¬ ì²˜ë¦¬)")
    parser.add_argument("--start", required=True, help="ì‹œì‘ì¼ (YYYY-MM-DD)")
    parser.add_argument("--end", required=True, help="ì¢…ë£Œì¼ (YYYY-MM-DD)")
    parser.add_argument("--debug", action="store_true", help="ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™”")
    parser.add_argument("--workers", type=int, default=MAX_WORKERS, help=f"ë³‘ë ¬ ì²˜ë¦¬ ìŠ¤ë ˆë“œ ìˆ˜ (ê¸°ë³¸ê°’: {MAX_WORKERS})")
    args = parser.parse_args()
    
    try:
        process_date_range(args.start, args.end, args.workers, args.debug)
    except Exception as e:
        if args.debug:
            logger.exception("ì˜¤ë¥˜ ë°œìƒ:")
        else:
            logger.error(f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        sys.exit(1)
