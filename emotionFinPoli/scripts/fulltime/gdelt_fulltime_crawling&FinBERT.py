import argparse
from datetime import datetime, timedelta
import json
import os
from io import BytesIO
from newspaper import Article
import newspaper
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
from google.cloud import storage
import logging
import sys
import concurrent.futures
from tqdm import tqdm
import time
import random

# ----------------------------
# ì„¤ì •
# ----------------------------
BUCKET_RAW = "emotion-raw-data"
BUCKET_INDEX = "emotion-index-data"
MODEL_NAME = "yiyanghkust/finbert-tone"
LOCAL_TEMP_DIR = "/tmp"
GDELT_PREFIX = "news/gdelt/"

# ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì •
MAX_WORKERS = 8  # ë™ì‹œ ì‹¤í–‰ ìŠ¤ë ˆë“œ ìˆ˜ 
MAX_RETRIES = 3  # í¬ë¡¤ë§ ì¬ì‹œë„ íšŸìˆ˜
CRAWL_DELAY = 0.5  # í¬ë¡¤ë§ ìš”ì²­ ê°„ ì§€ì—° ì‹œê°„(ì´ˆ)
REQUEST_TIMEOUT = 10  # ìš”ì²­ íƒ€ì„ì•„ì›ƒ(ì´ˆ)

# ----------------------------
# ë¡œê·¸ ì„¤ì •
# ----------------------------
LOG_ROOT = "/home/hwangjeongmun691/logs"
today = datetime.utcnow().strftime("%Y-%m-%d")
LOG_DATE_DIR = f"{LOG_ROOT}/{today}"
os.makedirs(LOG_DATE_DIR, exist_ok=True)

# ìŠ¤í¬ë¦½íŠ¸ ì´ë¦„ ê¸°ë°˜ìœ¼ë¡œ ë¡œê·¸ íŒŒì¼ ì„¤ì •
script_name = os.path.basename(__file__)
log_file = f"{LOG_DATE_DIR}/{script_name.replace('.py', '.log')}"

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("gdelt_finbert")

# ----------------------------
# ëª¨ë¸ ë¡œë”© (FinBERT)
# ----------------------------
logger.info("ğŸ§  FinBERT ëª¨ë¸ ë¡œë”© ì¤‘...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)
finbert = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)
logger.info("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

# ----------------------------
# GCS ì—°ê²°
# ----------------------------
client = storage.Client()
bucket_raw = client.bucket(BUCKET_RAW)
bucket_index = storage.Client().bucket(BUCKET_INDEX)

# ----------------------------
# ë³¸ë¬¸ í¬ë¡¤ë§ í•¨ìˆ˜
# ----------------------------
def fetch_article_text(url, retry=0):
    """ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§ í•¨ìˆ˜ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)"""
    if retry > MAX_RETRIES:
        return None
        
    try:
        # í¬ë¡¤ë§ ê°„ ì•½ê°„ì˜ ëœë¤ ì§€ì—° ì¶”ê°€
        time.sleep(CRAWL_DELAY + random.uniform(0, 1))
        
        config = newspaper.Config()
        config.browser_user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        config.request_timeout = REQUEST_TIMEOUT
        
        article = Article(url, config=config)
        article.download()
        article.parse()
        text = article.text.strip()
        
        if not text:
            return None
            
        return text
    except Exception as e:
        # ì¬ì‹œë„ ë¡œì§
        if retry < MAX_RETRIES:
            time.sleep(retry * 2)  # ì¬ì‹œë„ë§ˆë‹¤ ëŒ€ê¸° ì‹œê°„ ì¦ê°€
            return fetch_article_text(url, retry + 1)
        else:
            return None

# ----------------------------
# ê¸°ì‚¬ ì²˜ë¦¬ í•¨ìˆ˜ (ë³‘ë ¬ ì²˜ë¦¬ìš©)
# ----------------------------
def process_article(item):
    """ë‹¨ì¼ ê¸°ì‚¬ í¬ë¡¤ë§ ë° ê°ì • ë¶„ì„ (ë³‘ë ¬ ì²˜ë¦¬ìš©)"""
    try:
        url = item.get("DocumentIdentifier")
        if not url:
            return {
                "success": False,
                "error": "URL ì—†ìŒ",
                "url": None
            }
            
        # ê¸°ì‚¬ í¬ë¡¤ë§
        text = fetch_article_text(url)
        if not text:
            return {
                "success": False,
                "error": "í¬ë¡¤ë§ ì‹¤íŒ¨",
                "url": url
            }
            
        # ê°ì • ë¶„ì„
        result = finbert(text[:512])[0]
        label = result["label"].lower()
        score = result["score"]
        
        return {
            "success": True,
            "url": url,
            "label": label,
            "score": score,
            "is_negative": label == "negative"
        }
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "url": url if 'url' in locals() else None
        }

def get_date_range(start_date, end_date):
    """ì‹œì‘ì¼ê³¼ ì¢…ë£Œì¼ ì‚¬ì´ì˜ ëª¨ë“  ë‚ ì§œë¥¼ ìƒì„±"""
    start = datetime.strptime(start_date, "%Y-%m-%d")
    end = datetime.strptime(end_date, "%Y-%m-%d")
    date_list = []
    current = start
    while current <= end:
        date_list.append(current.strftime("%Y-%m-%d"))
        current += timedelta(days=1)
    return date_list

# ----------------------------
# ë©”ì¸ í•¨ìˆ˜
# ----------------------------
def main(start_date, end_date, debug=False, workers=MAX_WORKERS):
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    date_list = get_date_range(start_date, end_date)
    logger.info(f"ì²˜ë¦¬í•  ë‚ ì§œ: {date_list} ({len(date_list)}ì¼)")
    logger.info(f"ë³‘ë ¬ ì²˜ë¦¬ ìŠ¤ë ˆë“œ ìˆ˜: {workers}")
    
    # ì „ì²´ ì‹¤í–‰ ì‹œê°„ ì¸¡ì •
    total_start_time = time.time()
    total_articles = 0
    total_processed = 0
    total_failed = 0
    
    for date_str in date_list:
        date_start_time = time.time()
        logger.info(f"\n{'='*50}")
        logger.info(f"ğŸ“… {date_str} ì²˜ë¦¬ ì‹œì‘")
        
        # í•´ë‹¹ ë‚ ì§œì˜ JSON íŒŒì¼ ì°¾ê¸° (ìˆ˜ì •ëœ ê²½ë¡œ)
        prefix_path = f"{GDELT_PREFIX}full/{start_date}_to_{end_date}.json"
        blob = bucket_raw.blob(prefix_path)
        
        if not blob.exists():
            logger.warning(f"ğŸš« {prefix_path} ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            # ë‹¤ë¥¸ í˜•ì‹ì˜ ê²½ë¡œë„ ì‹œë„
            alt_path = f"{GDELT_PREFIX}{date_str}/accumulated.json"
            blob = bucket_raw.blob(alt_path)
            
            if not blob.exists():
                logger.warning(f"ğŸš« {date_str} ë‚ ì§œì˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                continue
        
        try:
            # ë°ì´í„° ë¡œë“œ
            content = blob.download_as_bytes()
            items = json.load(BytesIO(content))
            
            if not items:
                logger.warning(f"âš ï¸ {date_str}ì— ì²˜ë¦¬í•  ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
                continue
                
            # íŠ¹ì • ë‚ ì§œ í•„í„°ë§ (í•„ìš”í•œ ê²½ìš°)
            if start_date != end_date:
                filtered_items = []
                for item in items:
                    if 'DATE' in item:
                        item_date = item['DATE'][:8]  # YYYYMMDDHHMMSS í˜•ì‹ì—ì„œ YYYYMMDD ì¶”ì¶œ
                        item_date_formatted = f"{item_date[:4]}-{item_date[4:6]}-{item_date[6:8]}"
                        if item_date_formatted == date_str:
                            filtered_items.append(item)
                items = filtered_items
                
            total_articles += len(items)
            logger.info(f"ğŸ“° {date_str} - ì´ {len(items)}ê°œ ê¸°ì‚¬ ë¶„ì„ ì‹œì‘")
            
            results = []
            details = []
            failed_urls = []
            
            # ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ tqdm ì§„í–‰ í‘œì‹œì¤„
            with tqdm(total=len(items), desc=f"{date_str} ë¶„ì„ ì¤‘", unit="ê¸°ì‚¬") as pbar:
                with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:
                    # ì‘ì—… ì œì¶œ
                    future_to_item = {executor.submit(process_article, item): item for item in items}
                    
                    # ê²°ê³¼ ì²˜ë¦¬
                    for future in concurrent.futures.as_completed(future_to_item):
                        result = future.result()
                        pbar.update(1)
                        
                        if result["success"]:
                            total_processed += 1
                            details.append([result["url"], result["label"], result["score"]])
                            
                            if result["is_negative"]:
                                results.append(result["score"])
                        else:
                            total_failed += 1
                            failed_urls.append({
                                "url": result["url"],
                                "error": result["error"]
                            })
                        
                        # ì£¼ê¸°ì  ìƒíƒœ ë³´ê³  (10%ë§ˆë‹¤)
                        if pbar.n % max(1, len(items) // 10) == 0 or pbar.n == len(items):
                            elapsed = time.time() - date_start_time
                            progress = pbar.n / len(items) * 100
                            remaining = (elapsed / pbar.n) * (len(items) - pbar.n) if pbar.n > 0 else 0
                            logger.info(f"ğŸ”„ ì§„í–‰ë¥ : {progress:.1f}% ({pbar.n}/{len(items)}) - "
                                      f"ì„±ê³µ: {len(details)}, ì‹¤íŒ¨: {len(failed_urls)} - "
                                      f"ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {remaining/60:.1f}ë¶„")
            
            # ê²°ê³¼ ê³„ì‚°
            total_count = len(details)
            negative_ratio = len(results) / total_count if total_count > 0 else 0
            avg_negative_score = sum(results) / len(results) if results else 0
            anxiety_index = negative_ratio * avg_negative_score
            
            logger.info(f"ğŸ“Š {date_str} ë¶„ì„ ê²°ê³¼:")
            logger.info(f"  â€¢ ì´ ê¸°ì‚¬: {len(items)}ê°œ")
            logger.info(f"  â€¢ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ëœ ê¸°ì‚¬: {total_count}ê°œ")
            logger.info(f"  â€¢ ë¶€ì • ê¸°ì‚¬ ë¹„ìœ¨: {negative_ratio:.3f}")
            logger.info(f"  â€¢ í‰ê·  ë¶€ì • ì ìˆ˜: {avg_negative_score:.3f}")
            logger.info(f"  â€¢ ë¶ˆì•ˆ ì§€ìˆ˜: {anxiety_index:.3f}")
            
            # íŒŒì¼ ì €ì¥
            save_folder = f"news/{date_str}"
            save_filename = "news_anxiety_index.csv"
            local_path = os.path.join(LOCAL_TEMP_DIR, f"{date_str}_{save_filename}")
            
            with open(local_path, "w", encoding="utf-8") as f:
                f.write("negative_ratio,average_negative_score,anxiety_index\n")
                f.write(f"{negative_ratio:.3f},{avg_negative_score:.3f},{anxiety_index:.3f}\n\n")
                f.write("url,label,score\n")
                for row in details:
                    f.write(f"{row[0]},{row[1]},{row[2]:.3f}\n")
            
            # GCSì— ì—…ë¡œë“œ
            blob = bucket_index.blob(f"{save_folder}/{save_filename}")
            blob.upload_from_filename(local_path)
            logger.info(f"âœ… ì €ì¥ ì™„ë£Œ â†’ gs://{BUCKET_INDEX}/{save_folder}/{save_filename}")
            
            # ì‹¤íŒ¨ URL ë¡œê·¸ (í•„ìš”í•œ ê²½ìš°)
            if failed_urls:
                fail_log_path = f"{LOG_DATE_DIR}/gdelt_finbert_failed_{date_str}.json"
                with open(fail_log_path, 'w') as f:
                    json.dump(failed_urls, f, indent=2)
                logger.info(f"ğŸ“ ì‹¤íŒ¨ URL ì €ì¥ë¨: {fail_log_path}")
            
            # ë‚ ì§œë³„ ì²˜ë¦¬ ì‹œê°„
            date_elapsed_time = time.time() - date_start_time
            logger.info(f"â±ï¸ {date_str} ì²˜ë¦¬ ì™„ë£Œ: {date_elapsed_time:.1f}ì´ˆ ({date_elapsed_time/60:.1f}ë¶„)")
            
        except Exception as e:
            logger.error(f"âŒ {date_str} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            if debug:
                logger.exception("ìƒì„¸ ì˜¤ë¥˜:")
            continue
    
    # ì „ì²´ ì‹¤í–‰ ê²°ê³¼ ìš”ì•½
    total_elapsed_time = time.time() - total_start_time
    logger.info("\n" + "="*50)
    logger.info("ğŸ“Š ì „ì²´ ì‹¤í–‰ ê²°ê³¼")
    logger.info(f"â€¢ ì²˜ë¦¬ ë‚ ì§œ: {len(date_list)}ì¼")
    logger.info(f"â€¢ ì´ ê¸°ì‚¬ ìˆ˜: {total_articles}ê°œ")
    logger.info(f"â€¢ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ëœ ê¸°ì‚¬: {total_processed}ê°œ")
    logger.info(f"â€¢ ì‹¤íŒ¨í•œ ê¸°ì‚¬: {total_failed}ê°œ")
    logger.info(f"â€¢ ì„±ê³µë¥ : {(total_processed/total_articles)*100:.1f}% (ì„±ê³µ/ì „ì²´)")
    logger.info(f"â€¢ ì´ ì†Œìš” ì‹œê°„: {total_elapsed_time:.1f}ì´ˆ ({total_elapsed_time/60:.1f}ë¶„)")
    logger.info("="*50)

# ----------------------------
# CLI ì¸í„°í˜ì´ìŠ¤
# ----------------------------
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="GDELT ë‰´ìŠ¤ ê°ì • ë¶„ì„ê¸° (ë³‘ë ¬ ì²˜ë¦¬)")
    parser.add_argument("--start", required=True, help="ì‹œì‘ì¼ (YYYY-MM-DD)")
    parser.add_argument("--end", required=True, help="ì¢…ë£Œì¼ (YYYY-MM-DD)")
    parser.add_argument("--debug", action="store_true", help="ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™”")
    parser.add_argument("--workers", type=int, default=MAX_WORKERS, help=f"ë³‘ë ¬ ì²˜ë¦¬ ìŠ¤ë ˆë“œ ìˆ˜ (ê¸°ë³¸ê°’: {MAX_WORKERS})")
    args = parser.parse_args()
    
    if args.debug:
        logger.setLevel(logging.DEBUG)
        logger.info("ğŸ”§ ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™”ë¨")
    
    try:
        main(args.start, args.end, args.debug, args.workers)
    except Exception as e:
        if args.debug:
            logger.exception("âŒ ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ:")
        else:
            logger.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        sys.exit(1)
