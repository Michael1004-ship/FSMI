import argparse
from datetime import datetime, timedelta
import json
import os
from io import BytesIO
from newspaper import Article
import newspaper
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
from google.cloud import storage
import logging
import sys

# ----------------------------
# ì„¤ì •
# ----------------------------
BUCKET_RAW = "emotion-raw-data"
BUCKET_INDEX = "emotion-index-data"
MODEL_NAME = "yiyanghkust/finbert-tone"
LOCAL_TEMP_DIR = "/tmp"
GDELT_PREFIX = "news/gdelt/"

# ----------------------------
# ë¡œê·¸ ì„¤ì •
# ----------------------------
LOG_ROOT = "/home/hwangjeongmun691/logs"
today = datetime.utcnow().strftime("%Y-%m-%d")
LOG_DATE_DIR = f"{LOG_ROOT}/{today}"
os.makedirs(LOG_DATE_DIR, exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f"{LOG_DATE_DIR}/gdelt_finbert.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("gdelt_finbert")

# ----------------------------
# ëª¨ë¸ ë¡œë”© (FinBERT)
# ----------------------------
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)
finbert = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)

# ----------------------------
# GCS ì—°ê²°
# ----------------------------
client = storage.Client()
bucket_raw = client.bucket(BUCKET_RAW)
bucket_index = storage.Client().bucket(BUCKET_INDEX)

# ----------------------------
# ë³¸ë¬¸ í¬ë¡¤ë§ í•¨ìˆ˜
# ----------------------------
def fetch_article_text(url):
    try:
        config = newspaper.Config()
        config.browser_user_agent = "Mozilla/5.0"
        article = Article(url, config=config)
        article.download()
        article.parse()
        return article.text.strip()
    except Exception as e:
        logger.warning(f"[X] í¬ë¡¤ë§ ì‹¤íŒ¨: {url} | {e}")
        return None

def get_date_range(start_date, end_date):
    """ì‹œì‘ì¼ê³¼ ì¢…ë£Œì¼ ì‚¬ì´ì˜ ëª¨ë“  ë‚ ì§œë¥¼ ìƒì„±"""
    start = datetime.strptime(start_date, "%Y-%m-%d")
    end = datetime.strptime(end_date, "%Y-%m-%d")
    date_list = []
    current = start
    while current <= end:
        date_list.append(current.strftime("%Y-%m-%d"))
        current += timedelta(days=1)
    return date_list

# ----------------------------
# ë©”ì¸ í•¨ìˆ˜
# ----------------------------
def main(start_date, end_date, debug=False):
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    date_list = get_date_range(start_date, end_date)
    logger.info(f"ì²˜ë¦¬í•  ë‚ ì§œ: {date_list}")

    for date_str in date_list:
        # í•´ë‹¹ ë‚ ì§œì˜ í´ë”ì—ì„œ JSON íŒŒì¼ ì°¾ê¸°
        prefix_path = f"{GDELT_PREFIX}full/{date_str}_to_{date_str}.json"
        blob = bucket_raw.blob(prefix_path)
        
        if not blob.exists():
            logger.warning(f"ğŸš« {date_str} ë‚ ì§œì˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            continue

        try:
            content = blob.download_as_bytes()
            items = json.load(BytesIO(content))
            
            if not items:
                logger.warning(f"âš ï¸ {date_str}ì— ì²˜ë¦¬í•  ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
                continue

            logger.info(f"ğŸ“… {date_str} - ì´ {len(items)}ê°œ ê¸°ì‚¬ ë¶„ì„ ì‹œì‘")
            results = []
            details = []

            for item in items:
                if "DocumentIdentifier" in item:
                    url = item["DocumentIdentifier"]
                    text = fetch_article_text(url)
                    if not text:
                        continue
                    try:
                        result = finbert(text[:512])[0]
                        label = result["label"].lower()
                        score = result["score"]
                        details.append([url, label, score])
                        if label == "negative":
                            results.append(score)
                    except Exception as e:
                        logger.error(f"FinBERT ë¶„ì„ ì‹¤íŒ¨: {e}")
                        continue

            total_count = len(details)
            negative_ratio = len(results) / total_count if total_count > 0 else 0
            avg_negative_score = sum(results) / len(results) if results else 0
            anxiety_index = negative_ratio * avg_negative_score

            # ì €ì¥
            save_folder = f"news/{date_str}"
            save_filename = "news_anxiety_index.csv"
            local_path = os.path.join(LOCAL_TEMP_DIR, f"{date_str}_{save_filename}")
            with open(local_path, "w", encoding="utf-8") as f:
                f.write("negative_ratio,average_negative_score,anxiety_index\n")
                f.write(f"{negative_ratio:.3f},{avg_negative_score:.3f},{anxiety_index:.3f}\n\n")
                f.write("url,label,score\n")
                for row in details:
                    f.write(f"{row[0]},{row[1]},{row[2]:.3f}\n")

            blob = bucket_index.blob(f"{save_folder}/{save_filename}")
            blob.upload_from_filename(local_path)
            logger.info(f"âœ… ì €ì¥ ì™„ë£Œ â†’ gs://{BUCKET_INDEX}/{save_folder}/{save_filename}")

        except Exception as e:
            logger.error(f"âŒ {date_str} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            if debug:
                logger.exception("ìƒì„¸ ì˜¤ë¥˜:")
            continue

# ----------------------------
# CLI ì¸í„°í˜ì´ìŠ¤
# ----------------------------
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="GDELT ë‰´ìŠ¤ ê°ì • ë¶„ì„ê¸°")
    parser.add_argument("--start", required=True, help="ì‹œì‘ì¼ (YYYY-MM-DD)")
    parser.add_argument("--end", required=True, help="ì¢…ë£Œì¼ (YYYY-MM-DD)")
    parser.add_argument("--debug", action="store_true", help="ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™”")
    args = parser.parse_args()
    
    try:
        main(args.start, args.end, args.debug)
    except Exception as e:
        if args.debug:
            logger.exception("ì˜¤ë¥˜ ë°œìƒ:")
        else:
            logger.error(f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        sys.exit(1)
