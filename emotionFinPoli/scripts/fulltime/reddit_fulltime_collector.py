from datetime import datetime, timedelta
import os
import json
import logging
import pandas as pd
import praw
from google.cloud import storage
from time import sleep
import sys
import argparse
from dotenv import load_dotenv
import concurrent.futures
from tqdm import tqdm
import random

# âœ… í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ë¡œê·¸ ë””ë ‰í† ë¦¬ ì„¤ì •
LOG_ROOT = "/home/hwangjeongmun691/logs"
today = datetime.utcnow().strftime("%Y-%m-%d")
LOG_DATE_DIR = f"{LOG_ROOT}/{today}"

# ìŠ¤í¬ë¦½íŠ¸ ì´ë¦„ ê¸°ë°˜ìœ¼ë¡œ ë¡œê·¸ íŒŒì¼ ì„¤ì •
script_name = os.path.basename(__file__)
log_file = f"{LOG_DATE_DIR}/{script_name.replace('.py', '.log')}"

# ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs(LOG_DATE_DIR, exist_ok=True)

# âœ… ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("reddit_fulltime_collector")

# âœ… ì„¤ì •
SUBREDDITS = [
    # ê²½ì œ ê´€ë ¨
    "economics", "economy", "MacroEconomics", "EconMonitor",
    # ê¸ˆìœµ ê´€ë ¨
    "finance", "investing", "financialindependence", "personalfinance",
    "wallstreetbets", "stocks", "StockMarket", "dividends",
    # ê°ì • ê¸°ë°˜ íë¦„
    "anxiety", "depression", "offmychest"
]

BUCKET_NAME = "emotion-raw-data"
POST_LIMIT = 1000  # ì„œë¸Œë ˆë”§ ì „ì²´ ìˆ˜ì§‘ ê¸°ì¤€

# ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì •
MAX_WORKERS = 5  # ë™ì‹œ ì²˜ë¦¬í•  ìµœëŒ€ ì„œë¸Œë ˆë”§ ìˆ˜
REQUEST_DELAY = 1.0  # ê¸°ë³¸ ìš”ì²­ ê°„ ì§€ì—° ì‹œê°„(ì´ˆ)
MAX_RETRIES = 3  # API ìš”ì²­ ì¬ì‹œë„ íšŸìˆ˜

# âœ… ì£¼ì˜ ì•Œë¦¼
logger.warning("\033[91mâš ï¸  Reddit APIëŠ” ìµœëŒ€ 1ë…„ ì´ë‚´ ê²Œì‹œë¬¼ë§Œ ìˆ˜ì§‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\033[0m")

# âœ… Reddit API ì´ˆê¸°í™” (.env íŒŒì¼ì—ì„œ ë¡œë“œ)
def init_reddit_api():
    """Reddit API ì´ˆê¸°í™” (ì¬ì‹œë„ ë¡œì§ í¬í•¨)"""
    for attempt in range(MAX_RETRIES):
        try:
            reddit = praw.Reddit(
                client_id=os.getenv("REDDIT_CLIENT_ID"),
                client_secret=os.getenv("REDDIT_CLIENT_SECRET"),
                user_agent=os.getenv("REDDIT_USER_AGENT"),
            )
            # API ì ‘ì† í…ŒìŠ¤íŠ¸
            reddit.user.me()
            return reddit
        except Exception as e:
            if attempt < MAX_RETRIES - 1:
                sleep_time = 2 ** attempt  # ì§€ìˆ˜ ë°±ì˜¤í”„
                logger.warning(f"Reddit API ì—°ê²° ì‹¤íŒ¨ ({attempt + 1}/{MAX_RETRIES}): {e}")
                logger.warning(f"{sleep_time}ì´ˆ í›„ ì¬ì‹œë„...")
                sleep(sleep_time)
            else:
                logger.error(f"Reddit API ì—°ê²° ì‹¤íŒ¨: {e}")
                raise
    
    return None

# âœ… ì €ì¥ í•¨ìˆ˜
def save_to_gcs(df, date_range):
    """ë°ì´í„° GCSì— ì €ì¥"""
    # GDELTì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ íŒŒì¼ëª… ì§€ì •
    file_path = f"sns/reddit/full/{date_range}.json"
    
    storage_client = storage.Client()
    bucket = storage_client.bucket(BUCKET_NAME)
    blob = bucket.blob(file_path)
    
    # ê¸°ì¡´ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸í•˜ì—¬ ë³‘í•©
    if blob.exists():
        try:
            existing_data = json.loads(blob.download_as_string())
            combined_data = existing_data + df.to_dict(orient='records')
            blob.upload_from_string(json.dumps(combined_data, ensure_ascii=False), content_type='application/json')
            logger.info(f"ğŸ”„ ê¸°ì¡´ íŒŒì¼ ì—…ë°ì´íŠ¸ ì™„ë£Œ: gs://{BUCKET_NAME}/{file_path}")
        except Exception as e:
            logger.error(f"ê¸°ì¡´ íŒŒì¼ ë³‘í•© ì¤‘ ì˜¤ë¥˜: {e}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ìƒˆ íŒŒì¼ë¡œ ì €ì¥
            blob.upload_from_string(json.dumps(df.to_dict(orient='records'), ensure_ascii=False), content_type='application/json')
            logger.info(f"âœ… ìƒˆ íŒŒì¼ ì €ì¥ ì™„ë£Œ: gs://{BUCKET_NAME}/{file_path}")
    else:
        # ìƒˆ íŒŒì¼ ìƒì„±
        blob.upload_from_string(json.dumps(df.to_dict(orient='records'), ensure_ascii=False), content_type='application/json')
        logger.info(f"âœ… ìƒˆ íŒŒì¼ ìƒì„± ì™„ë£Œ: gs://{BUCKET_NAME}/{file_path}")
    
    return len(df)

# âœ… ì„œë¸Œë ˆë”§ ë°ì´í„° ìˆ˜ì§‘ í•¨ìˆ˜ (ë³‘ë ¬ ì²˜ë¦¬ìš©)
def collect_subreddit(reddit, subreddit_name, start_dt, end_dt, time_filter):
    """ë‹¨ì¼ ì„œë¸Œë ˆë”§ì˜ ë°ì´í„° ìˆ˜ì§‘ (ë³‘ë ¬ ì²˜ë¦¬ìš©)"""
    posts = []
    collected_count = 0
    matched_count = 0
    error = None
    start_time = datetime.now()
    
    try:
        logger.info(f"ğŸ” r/{subreddit_name} ìˆ˜ì§‘ ì‹œì‘ (ì‹œê°„ í•„í„°: {time_filter})")
        
        for submission in reddit.subreddit(subreddit_name).top(limit=POST_LIMIT, time_filter=time_filter):
            collected_count += 1
            
            # ìš”ì²­ ê°„ ì§€ì—°
            delay = REQUEST_DELAY + random.uniform(0, 0.5)  # 0.5ì´ˆ ëœë¤ ì¶”ê°€
            sleep(delay)
            
            # ë‚ ì§œ ë²”ìœ„ í•„í„°ë§
            created_dt = datetime.utcfromtimestamp(submission.created_utc)
            if start_dt <= created_dt <= end_dt:
                posts.append({
                    "id": submission.id,
                    "title": submission.title,
                    "selftext": submission.selftext,
                    "created_utc": submission.created_utc,
                    "score": submission.score,
                    "num_comments": submission.num_comments,
                    "url": submission.url,
                    "subreddit": subreddit_name
                })
                matched_count += 1
                
        elapsed = (datetime.now() - start_time).total_seconds()
        
        return {
            "subreddit": subreddit_name,
            "collected": collected_count,
            "matched": matched_count,
            "posts": posts,
            "time_taken": elapsed,
            "success": True,
            "time_filter": time_filter
        }
    except Exception as e:
        elapsed = (datetime.now() - start_time).total_seconds()
        logger.error(f"r/{subreddit_name} ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
        return {
            "subreddit": subreddit_name,
            "collected": collected_count,
            "matched": matched_count,
            "posts": posts,
            "time_taken": elapsed,
            "success": False,
            "error": str(e),
            "time_filter": time_filter
        }

def run(start_str, end_str, max_workers=MAX_WORKERS, debug=False):
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ë‚ ì§œ ë²”ìœ„ íŒŒì‹±
    start_dt = datetime.strptime(start_str, "%Y-%m-%d")
    end_dt = datetime.strptime(end_str, "%Y-%m-%d")
    
    logger.info(f"ğŸš€ Reddit ì „ì²´ ìˆ˜ì§‘ ì‹œì‘: {start_str} ~ {end_str}")
    logger.info(f"ğŸ“‹ ì´ {len(SUBREDDITS)}ê°œ ì„œë¸Œë ˆë”§ ìˆ˜ì§‘ ì˜ˆì •")
    logger.info(f"ğŸ§µ ë³‘ë ¬ ì²˜ë¦¬ ìŠ¤ë ˆë“œ ìˆ˜: {max_workers}")
    
    # GDELTì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ íŒŒì¼ëª… ìƒì„±
    date_range = f"{start_str}_to_{end_str}"
    
    # ì „ì²´ ì§„í–‰ ìƒí™© ì¶”ì 
    total_subreddits = len(SUBREDDITS)
    total_posts_collected = 0
    start_time = datetime.now()
    subreddit_results = {}
    
    # Reddit API ì´ˆê¸°í™”
    try:
        reddit = init_reddit_api()
        if not reddit:
            logger.error("âŒ Reddit API ì´ˆê¸°í™” ì‹¤íŒ¨")
            return
    except Exception as e:
        logger.error(f"âŒ Reddit API ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return
    
    # ìˆ˜ì§‘ ì‹œê°„ í•„í„° ì„¤ì •
    # Reddit APIëŠ” íŠ¹ì • ë‚ ì§œ ë²”ìœ„ ì§€ì •ì´ ì œí•œì ì´ë¯€ë¡œ ì‹œê°„ í•„í„°ë¡œ ëŒ€ëµ ì ‘ê·¼
    time_filter = 'all'  # ê¸°ë³¸ê°’
    
    # í˜„ì¬ ì‹œì ìœ¼ë¡œë¶€í„° ë‚ ì§œ ì°¨ì´ ê³„ì‚°
    days_diff = (datetime.utcnow().date() - start_dt.date()).days
    
    if days_diff <= 1:
        time_filter = 'day'
    elif days_diff <= 7:
        time_filter = 'week'
    elif days_diff <= 30:
        time_filter = 'month'
    elif days_diff <= 90:
        time_filter = 'month'
    elif days_diff <= 365:
        time_filter = 'year'
    else:
        time_filter = 'all'
        logger.warning(f"âš ï¸ 1ë…„ ì´ìƒ ì§€ë‚œ ë°ì´í„°ëŠ” ì™„ì „íˆ ìˆ˜ì§‘ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    logger.info(f"ğŸ•’ ì„ íƒëœ ì‹œê°„ í•„í„°: {time_filter} (í˜„ì¬ ì‹œì ë¶€í„° {days_diff}ì¼ ì „)")
    
    # ë³‘ë ¬ ì²˜ë¦¬ë¥¼ í†µí•œ ì„œë¸Œë ˆë”§ ë°ì´í„° ìˆ˜ì§‘
    all_posts = []
    failed_subreddits = []
    
    with tqdm(total=len(SUBREDDITS), desc="ì„œë¸Œë ˆë”§ ìˆ˜ì§‘ ì¤‘", unit="ê°œ") as pbar:
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            # ì„œë¸Œë ˆë”§ë³„ ì‘ì—… ì œì¶œ
            future_to_subreddit = {
                executor.submit(collect_subreddit, reddit, sub, start_dt, end_dt, time_filter): sub 
                for sub in SUBREDDITS
            }
            
            # ê²°ê³¼ ì²˜ë¦¬
            for future in concurrent.futures.as_completed(future_to_subreddit):
                subreddit = future_to_subreddit[future]
                try:
                    result = future.result()
                    subreddit_results[subreddit] = result
                    
                    if result["success"]:
                        all_posts.extend(result["posts"])
                        logger.info(f"âœ… r/{subreddit} ìˆ˜ì§‘ ì™„ë£Œ: ê²€ìƒ‰ {result['collected']}ê°œ ì¤‘ {result['matched']}ê°œ ìˆ˜ì§‘")
                    else:
                        failed_subreddits.append(subreddit)
                        logger.error(f"âŒ r/{subreddit} ìˆ˜ì§‘ ì‹¤íŒ¨: {result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
                except Exception as e:
                    subreddit_results[subreddit] = {
                        "success": False,
                        "error": str(e)
                    }
                    failed_subreddits.append(subreddit)
                    logger.error(f"âŒ r/{subreddit} ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {str(e)}")
                
                # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
                pbar.update(1)
                
                # ì£¼ê¸°ì  ìƒíƒœ ë³´ê³ 
                if pbar.n % max(1, len(SUBREDDITS) // 5) == 0 or pbar.n == len(SUBREDDITS):
                    elapsed = (datetime.now() - start_time).total_seconds()
                    remaining = (elapsed / pbar.n) * (len(SUBREDDITS) - pbar.n) if pbar.n > 0 else 0
                    success_count = len([r for r in subreddit_results.values() if r.get("success", False)])
                    total_collected = sum([r.get("matched", 0) for r in subreddit_results.values()])
                    
                    logger.info(f"ğŸ”„ ì§„í–‰ë¥ : {pbar.n/len(SUBREDDITS)*100:.1f}% ({pbar.n}/{len(SUBREDDITS)}) - "
                               f"ì„±ê³µ: {success_count}, ì‹¤íŒ¨: {len(failed_subreddits)} - "
                               f"ìˆ˜ì§‘ ê²Œì‹œë¬¼: {total_collected}ê°œ - "
                               f"ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {remaining/60:.1f}ë¶„")
    
    # ê²°ê³¼ ì²˜ë¦¬
    if all_posts:
        # DataFrame ìƒì„±
        df = pd.DataFrame(all_posts)
        
        # ì¤‘ë³µ ì œê±°
        df = df.drop_duplicates(subset=["id"])
        
        # GCSì— ì €ì¥
        total_saved = save_to_gcs(df, date_range)
        total_posts_collected = len(df)
        
        logger.info(f"ğŸ“Š ì´ {total_posts_collected}ê°œ ê²Œì‹œë¬¼ ìˆ˜ì§‘ë¨ (ì¤‘ë³µ ì œê±° í›„)")
    else:
        logger.warning("âš ï¸ ìˆ˜ì§‘ëœ ê²Œì‹œë¬¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    # ì „ì²´ ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½
    total_elapsed = (datetime.now() - start_time).total_seconds()
    logger.info("\n" + "="*50)
    logger.info(f"ğŸ“ˆ Reddit ìˆ˜ì§‘ ìµœì¢… ê²°ê³¼:")
    logger.info(f"  â€¢ ìˆ˜ì§‘ ê¸°ê°„: {start_str} ~ {end_str}")
    logger.info(f"  â€¢ ì´ ì„œë¸Œë ˆë”§ ìˆ˜: {len(SUBREDDITS)}ê°œ")
    logger.info(f"  â€¢ ì„±ê³µí•œ ì„œë¸Œë ˆë”§: {len(SUBREDDITS) - len(failed_subreddits)}ê°œ")
    logger.info(f"  â€¢ ì‹¤íŒ¨í•œ ì„œë¸Œë ˆë”§: {len(failed_subreddits)}ê°œ")
    logger.info(f"  â€¢ ì´ ìˆ˜ì§‘ ê²Œì‹œë¬¼: {total_posts_collected}ê°œ")
    logger.info(f"  â€¢ ì´ ì†Œìš” ì‹œê°„: {total_elapsed/60:.1f}ë¶„")
    
    # ì„œë¸Œë ˆë”§ë³„ ìˆ˜ì§‘ ìƒì„¸
    logger.info("\nğŸ” ì„œë¸Œë ˆë”§ë³„ ìˆ˜ì§‘ ìƒì„¸:")
    for sub, result in subreddit_results.items():
        if not result.get("success", False):
            logger.info(f"  â€¢ r/{sub}: ì˜¤ë¥˜ ë°œìƒ - {result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
        else:
            success_rate = (result['matched'] / result['collected'] * 100) if result['collected'] > 0 else 0
            logger.info(f"  â€¢ r/{sub}: {result['matched']}ê°œ ìˆ˜ì§‘ / {result['collected']}ê°œ íƒìƒ‰ "
                      f"({success_rate:.1f}%) - ì†Œìš” ì‹œê°„: {result['time_taken']/60:.1f}ë¶„")
    
    # ì‹¤íŒ¨í•œ ì„œë¸Œë ˆë”§ ë¡œê·¸
    if failed_subreddits:
        fail_log_path = f"{LOG_DATE_DIR}/reddit_failed_subreddits_{date_range}.json"
        with open(fail_log_path, 'w') as f:
            failures = {sub: subreddit_results.get(sub, {"error": "No data"}) for sub in failed_subreddits}
            json.dump(failures, f, indent=2)
        logger.info(f"ğŸ“ ì‹¤íŒ¨ ì„œë¸Œë ˆë”§ ë¡œê·¸ ì €ì¥ë¨: {fail_log_path}")
    
    logger.info("="*50)
    logger.info("ğŸ‰ ì „ì²´ Reddit ìˆ˜ì§‘ ì™„ë£Œ!")
    
    return total_posts_collected

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Reddit ë°ì´í„° ìˆ˜ì§‘ê¸° (ë³‘ë ¬ ì²˜ë¦¬)")
    parser.add_argument("--start", required=True, help="ì‹œì‘ì¼ (YYYY-MM-DD)")
    parser.add_argument("--end", required=True, help="ì¢…ë£Œì¼ (YYYY-MM-DD)")
    parser.add_argument("--debug", action="store_true", help="ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™”")
    parser.add_argument("--workers", type=int, default=MAX_WORKERS, help=f"ë³‘ë ¬ ì²˜ë¦¬ ìŠ¤ë ˆë“œ ìˆ˜ (ê¸°ë³¸ê°’: {MAX_WORKERS})")
    args = parser.parse_args()
    
    try:
        # ë‚ ì§œ í˜•ì‹ ê²€ì¦
        start_date = datetime.strptime(args.start, "%Y-%m-%d")
        end_date = datetime.strptime(args.end, "%Y-%m-%d")
        
        if end_date < start_date:
            logger.error("âŒ ì¢…ë£Œì¼ì´ ì‹œì‘ì¼ë³´ë‹¤ ì•ì„¤ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            sys.exit(1)
            
        logger.info(f"ğŸ“… ìˆ˜ì§‘ ê¸°ê°„: {args.start} ~ {args.end}")
        
        # Reddit API ì—°ê²° í™•ì¸
        if not all([os.getenv("REDDIT_CLIENT_ID"), 
                   os.getenv("REDDIT_CLIENT_SECRET"), 
                   os.getenv("REDDIT_USER_AGENT")]):
            logger.error("âŒ Reddit API ì¸ì¦ ì •ë³´ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            sys.exit(1)
        
        if args.debug:
            logger.setLevel(logging.DEBUG)
            logger.info("ğŸ”§ ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™”ë¨")
        
        # ì‹¤í–‰
        run(args.start, args.end, args.workers, args.debug)
        
    except ValueError as e:
        logger.error(f"âŒ ë‚ ì§œ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤: {str(e)}")
        sys.exit(1)
    except Exception as e:
        if args.debug:
            logger.exception("ì˜¤ë¥˜ ë°œìƒ:")
        else:
            logger.error(f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        sys.exit(1)