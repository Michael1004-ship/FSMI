from datetime import datetime, timedelta
# scripts/gdelt_full_collector.py
import os
import requests
import zipfile
import io
import pandas as pd

import logging
import json
from google.cloud import storage
import sys
import argparse
import concurrent.futures
from tqdm import tqdm

# ë¡œê·¸ ë””ë ‰í† ë¦¬ ì„¤ì •
LOG_ROOT = "/home/hwangjeongmun691/logs"
today = datetime.utcnow().strftime("%Y-%m-%d")
LOG_DATE_DIR = f"{LOG_ROOT}/{today}"

# ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs(LOG_DATE_DIR, exist_ok=True)

# ìŠ¤í¬ë¦½íŠ¸ ì´ë¦„ ê¸°ë°˜ìœ¼ë¡œ ë¡œê·¸ íŒŒì¼ ì„¤ì •
script_name = os.path.basename(__file__)
log_file = f"{LOG_DATE_DIR}/{script_name.replace('.py', '.log')}"

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("gdelt_fulltime_collector")

TOP_DOMAINS = [
    "nytimes.com", "wsj.com", "bloomberg.com", "cnbc.com", "cnn.com",
    "ft.com", "reuters.com", "finance.yahoo.com", "forbes.com", "marketwatch.com"
]

FINANCE_KEYWORDS = [
    "market", "stock", "stocks", "equity", "shares", "finance", "financing",
    "interest rate", "interest rates", "bond", "bonds", "treasury",
    "investment", "investments", "investor", "investors",
    "fed", "federal reserve", "monetary policy",
    "nasdaq", "dow jones", "s&p", "sp500", "volatility",
    "bank", "banking", "credit", "debt",
    "economic", "economy", "gdp", "growth", "recession", "inflation",
    "deflation", "stagflation", "employment", "unemployment", "jobless",
    "labor market", "consumer confidence", "cpi", "ppi", "retail sales",
    "housing market", "housing prices", "industrial production"
]

BUCKET_NAME = "emotion-raw-data"

# GKG ì»¬ëŸ¼ ì •ì˜
GKG_COLUMNS = [
    "GKGRECORDID", "DATE", "SourceCollectionIdentifier", "SourceCommonName",
    "DocumentIdentifier", "Counts", "V2Counts", "Themes", "V2Themes",
    "Locations", "V2Locations", "Persons", "V2Persons", "Organizations",
    "V2Organizations", "Tone", "Dates", "GCAM", "SharingImage",
    "RelatedImages", "SocialImageEmbeds", "SocialVideoEmbeds",
    "Quotations", "AllNames", "Amounts", "TranslationInfo",
    "Extras", "OriginalXML"
]

# ì „ì²´ URL ë¦¬ìŠ¤íŠ¸ ìƒì„±
MASTER_URL = "http://data.gdeltproject.org/gdeltv2/masterfilelist.txt"

# ë³‘ë ¬ ì²˜ë¦¬ ê´€ë ¨ ì„¤ì •
MAX_WORKERS = 10  # ë™ì‹œ ì²˜ë¦¬í•  ìµœëŒ€ ìŠ¤ë ˆë“œ ìˆ˜

def get_urls_between(start_date, end_date):
    logger.info("ğŸ” masterfilelist.txt ë¡œë“œ ì¤‘...")
    res = requests.get(MASTER_URL)
    lines = res.text.strip().split('\n')
    urls = []
    for line in lines:
        if ".gkg.csv.zip" in line:
            parts = line.strip().split()
            url = parts[2]
            timestamp = url.split("/")[-1].split(".")[0]
            dt = datetime.strptime(timestamp, "%Y%m%d%H%M%S")
            if start_date <= dt <= end_date:
                urls.append(url)
    logger.info(f"ì´ {len(urls)}ê°œì˜ GKG URL ìˆ˜ì§‘ ì˜ˆì •")
    return urls

def download_and_parse_csv(url):
    """ë‹¨ì¼ URL ë‹¤ìš´ë¡œë“œ ë° íŒŒì‹± ì²˜ë¦¬ (ë³‘ë ¬ ì²˜ë¦¬ìš©)"""
    try:
        res = requests.get(url, timeout=60)
        res.raise_for_status()
        with zipfile.ZipFile(io.BytesIO(res.content)) as z:
            for name in z.namelist():
                with z.open(name) as f:
                    chunk_size = 10000
                    chunks = [chunk for chunk in pd.read_csv(f, sep='\t', header=None, 
                                                          dtype=str, chunksize=chunk_size,
                                                          encoding='latin-1',  
                                                          on_bad_lines='skip')]
                    df = pd.concat(chunks, ignore_index=True)
                    
                    # ì»¬ëŸ¼ ì´ë¦„ ì„¤ì •
                    if len(df.columns) == len(GKG_COLUMNS):
                        df.columns = GKG_COLUMNS
                    elif len(df.columns) == len(GKG_COLUMNS) - 1:
                        df.columns = GKG_COLUMNS[:-1]
                    else:
                        df.columns = [f"Column_{i}" for i in range(len(df.columns))]
                    
                    # ê¸ˆìœµ ë‰´ìŠ¤ í•„í„°ë§
                    filtered_df = df[df['DocumentIdentifier'].str.contains('http', na=False)]
                    filtered_df = filtered_df[filtered_df['DocumentIdentifier'].str.contains('|'.join(TOP_DOMAINS), case=False, na=False)]
                    filtered_df = filtered_df[filtered_df['V2Themes'].str.contains('|'.join(FINANCE_KEYWORDS), case=False, na=False)]
                    filtered_df = filtered_df.drop_duplicates(subset=['DocumentIdentifier'])
                    
                    return {
                        "url": url,
                        "success": True,
                        "data": filtered_df,
                        "original_size": len(df),
                        "filtered_size": len(filtered_df)
                    }
    except Exception as e:
        return {
            "url": url,
            "success": False,
            "error": str(e)
        }

def save_to_gcs(df, file_name):
    storage_client = storage.Client()
    bucket = storage_client.bucket(BUCKET_NAME)
    blob = bucket.blob(file_name)
    blob.upload_from_string(json.dumps(df.to_dict(orient='records'), ensure_ascii=False), content_type='application/json')
    logger.info(f"âœ… ì €ì¥ ì™„ë£Œ: gs://{BUCKET_NAME}/{file_name}")

def run(start_str, end_str, max_workers=MAX_WORKERS):
    start_dt = datetime.strptime(start_str, "%Y-%m-%d")
    end_dt = datetime.strptime(end_str, "%Y-%m-%d")

    # URL ìˆ˜ì§‘
    logger.info(f"ğŸ” {start_str}ë¶€í„° {end_str}ê¹Œì§€ì˜ GDELT URL ìˆ˜ì§‘ ì¤‘...")
    urls = get_urls_between(start_dt, end_dt + timedelta(days=1))
    logger.info(f"ğŸ“‹ ì´ {len(urls)}ê°œì˜ GDELT URL ìˆ˜ì§‘ ì™„ë£Œ")
    
    if len(urls) == 0:
        logger.warning("âŒ ìˆ˜ì§‘í•  URLì´ ì—†ìŠµë‹ˆë‹¤. ë‚ ì§œ ë²”ìœ„ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return
    
    # ë³‘ë ¬ ì²˜ë¦¬ë¥¼ í†µí•œ URL ë‹¤ìš´ë¡œë“œ ë° íŒŒì‹±
    logger.info(f"â³ ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘ (ìµœëŒ€ {max_workers}ê°œ ìŠ¤ë ˆë“œ)")
    start_time = datetime.now()
    
    all_data = []
    failed_urls = []
    success_count = 0
    total_original_size = 0
    total_filtered_size = 0
    
    # ì§„í–‰ ìƒí™© í‘œì‹œë¥¼ ìœ„í•œ tqdm ì„¤ì •
    with tqdm(total=len(urls), desc="GDELT íŒŒì¼ ì²˜ë¦¬ ì¤‘", unit="íŒŒì¼") as pbar:
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            # URLë³„ë¡œ ì‘ì—… ì œì¶œ
            future_to_url = {executor.submit(download_and_parse_csv, url): url for url in urls}
            
            # ê²°ê³¼ ì²˜ë¦¬
            for future in concurrent.futures.as_completed(future_to_url):
                url = future_to_url[future]
                try:
                    result = future.result()
                    if result["success"]:
                        success_count += 1
                        all_data.append(result["data"])
                        total_original_size += result["original_size"]
                        total_filtered_size += result["filtered_size"]
                        
                        # ì„±ê³µ ë¡œê·¸ëŠ” ìƒì„¸ ë ˆë²¨ì—ì„œë§Œ ì¶œë ¥
                        logger.debug(f"âœ“ íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ: {url} - ì›ë³¸: {result['original_size']}í–‰, í•„í„°ë§ í›„: {result['filtered_size']}í–‰")
                    else:
                        failed_urls.append({"url": url, "error": result["error"]})
                        logger.warning(f"âš ï¸ ì²˜ë¦¬ ì‹¤íŒ¨: {url} - {result['error']}")
                except Exception as e:
                    failed_urls.append({"url": url, "error": str(e)})
                    logger.warning(f"âš ï¸ ì²˜ë¦¬ ì‹¤íŒ¨: {url} - {str(e)}")
                
                # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
                pbar.update(1)
                
                # ì£¼ê¸°ì  ìƒíƒœ ë³´ê³  (10%ë§ˆë‹¤)
                if pbar.n % max(1, len(urls) // 10) == 0 or pbar.n == len(urls):
                    elapsed = (datetime.now() - start_time).total_seconds()
                    progress = pbar.n / len(urls) * 100
                    remaining = (elapsed / pbar.n) * (len(urls) - pbar.n) if pbar.n > 0 else 0
                    logger.info(f"ğŸ”„ ì§„í–‰ë¥ : {progress:.1f}% ({pbar.n}/{len(urls)}) - "
                               f"ì„±ê³µ: {success_count}, ì‹¤íŒ¨: {len(failed_urls)} - "
                               f"ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {remaining/60:.1f}ë¶„")
    
    # ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½
    elapsed_time = (datetime.now() - start_time).total_seconds()
    logger.info(f"â±ï¸ ì²˜ë¦¬ ì‹œê°„: {elapsed_time:.1f}ì´ˆ ({elapsed_time/60:.1f}ë¶„)")
    logger.info(f"ğŸ“Š ì²˜ë¦¬ ê²°ê³¼: ì´ {len(urls)}ê°œ ì¤‘ {success_count}ê°œ ì„±ê³µ, {len(failed_urls)}ê°œ ì‹¤íŒ¨")
    
    if all_data:
        try:
            final_df = pd.concat(all_data, ignore_index=True)
            logger.info(f"ğŸ“Š ìµœì¢… ë°ì´í„°ì…‹: {len(final_df)}ê°œ ê¸ˆìœµ ë‰´ìŠ¤ (ì¤‘ë³µ ì œê±° ì „)")
            
            # ì¤‘ë³µ ì œê±°
            final_df = final_df.drop_duplicates(subset=['DocumentIdentifier'])
            logger.info(f"ğŸ“Š ì¤‘ë³µ ì œê±° í›„: {len(final_df)}ê°œ ê¸ˆìœµ ë‰´ìŠ¤")
            
            # ë‚ ì§œë³„ ë°ì´í„° ë¶„í¬ ë¶„ì„
            if 'DATE' in final_df.columns:
                df_copy = final_df.copy()
                df_copy['date_only'] = df_copy['DATE'].str[:8]  # YYYYMMDDHHMMSS í˜•ì‹ì—ì„œ YYYYMMDD ì¶”ì¶œ
                date_counts = df_copy['date_only'].value_counts().sort_index()
                logger.info("ğŸ“… ë‚ ì§œë³„ ë‰´ìŠ¤ ë¶„í¬:")
                for date, count in date_counts.items():
                    date_formatted = f"{date[:4]}-{date[4:6]}-{date[6:8]}"
                    logger.info(f"  â€¢ {date_formatted}: {count}ê°œ")
            
            # GCSì— ì €ì¥
            fname = f"news/gdelt/full/{start_str}_to_{end_str}.json"
            logger.info(f"ğŸ’¾ GCSì— ì €ì¥ ì¤‘: {fname}")
            save_to_gcs(final_df, fname)
            logger.info(f"ğŸ‰ ëª¨ë“  ì²˜ë¦¬ ì™„ë£Œ - {len(urls)}ê°œ íŒŒì¼ì—ì„œ {len(final_df)}ê°œ ë‰´ìŠ¤ ì €ì¥")
            
            # í•„í„°ë§ íš¨ìœ¨ì„± ë³´ê³ 
            if total_original_size > 0:
                filter_ratio = total_filtered_size / total_original_size * 100
                logger.info(f"ğŸ” í•„í„°ë§ íš¨ìœ¨: ì›ë³¸ {total_original_size}í–‰ â†’ í•„í„°ë§ í›„ {total_filtered_size}í–‰ ({filter_ratio:.2f}%)")
        except Exception as e:
            logger.error(f"âŒ ìµœì¢… ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    else:
        logger.warning("âš ï¸ ì €ì¥í•  ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. í•„í„°ë§ ì¡°ê±´ì„ í™•ì¸í•˜ì„¸ìš”.")
    
    # ì‹¤íŒ¨í•œ URL ë¡œê·¸
    if failed_urls:
        logger.warning(f"âš ï¸ {len(failed_urls)}ê°œ URL ì²˜ë¦¬ ì‹¤íŒ¨")
        # ì‹¤íŒ¨ ëª©ë¡ì„ íŒŒì¼ë¡œ ì €ì¥
        fail_log_path = f"{LOG_DATE_DIR}/gdelt_failed_urls_{start_str}_to_{end_str}.json"
        with open(fail_log_path, 'w') as f:
            json.dump(failed_urls, f, indent=2)
        logger.info(f"ğŸ“ ì‹¤íŒ¨ ëª©ë¡ ì €ì¥ë¨: {fail_log_path}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="GDELT ë°ì´í„° ìˆ˜ì§‘ê¸° (ë³‘ë ¬ ì²˜ë¦¬)")
    parser.add_argument("--start", required=True, help="ì‹œì‘ì¼ (YYYY-MM-DD)")
    parser.add_argument("--end", required=True, help="ì¢…ë£Œì¼ (YYYY-MM-DD)")
    parser.add_argument("--debug", action="store_true", help="ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™”")
    parser.add_argument("--workers", type=int, default=MAX_WORKERS, help=f"ë³‘ë ¬ ì²˜ë¦¬ ìŠ¤ë ˆë“œ ìˆ˜ (ê¸°ë³¸ê°’: {MAX_WORKERS})")
    args = parser.parse_args()
    
    if args.debug:
        logger.setLevel(logging.DEBUG)
        logger.info("ğŸ”§ ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™”ë¨")
    
    logger.info(f"ğŸ§µ ë³‘ë ¬ ì²˜ë¦¬ ìŠ¤ë ˆë“œ ìˆ˜: {args.workers}")
    
    try:
        run(args.start, args.end, args.workers)
    except Exception as e:
        if args.debug:
            logger.exception("âŒ ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ:")
        else:
            logger.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        sys.exit(1)