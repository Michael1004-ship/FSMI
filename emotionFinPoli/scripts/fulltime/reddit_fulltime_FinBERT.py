from datetime import datetime, timedelta
import os
import json
import pandas as pd
from io import BytesIO
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
from google.cloud import storage
import time
import logging
import argparse
import sys

# ----------------------------
# ë¡œê¹… ì„¤ì •
# ----------------------------

# TARGET_DATE ì„¤ì • ë¶€ë¶„ì„ ë‹¤ìŒìœ¼ë¡œ êµì²´
parser = argparse.ArgumentParser(description="Reddit FinBERT ë¶„ì„ê¸°")
parser.add_argument("--date", required=True, help="ë¶„ì„ ëŒ€ìƒ ë‚ ì§œ (ì˜ˆ: 2024-03-01)")
args = parser.parse_args()
TARGET_DATE = args.date

# ë¡œê·¸ ë””ë ‰í† ë¦¬ ì„¤ì • ë¶€ë¶„ì„ ë‹¤ìŒìœ¼ë¡œ êµì²´
LOG_ROOT = "/home/hwangjeongmun691/logs"
LOG_DATE_DIR = os.path.join(LOG_ROOT, TARGET_DATE)
os.makedirs(LOG_DATE_DIR, exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f"{LOG_DATE_DIR}/reddit_finbert.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("reddit_finbert")

# ----------------------------
# ì„¤ì •
# ----------------------------

BUCKET_RAW = "emotion-raw-data"
BUCKET_INDEX = "emotion-index-data"
REDDIT_PREFIX = "sns/reddit/"
SLEEP_SECONDS = 1.0
SAVE_FILENAME = "reddit_anxiety_index.csv"

# ----------------------------
# ëª¨ë¸ ë¡œë”© (FinBERT)
# ----------------------------

logger.info("FinBERT ëª¨ë¸ ë¡œë”© ì¤‘...")
tokenizer = AutoTokenizer.from_pretrained("yiyanghkust/finbert-tone")
model = AutoModelForSequenceClassification.from_pretrained("yiyanghkust/finbert-tone")
finbert = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)
logger.info("ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

# ----------------------------
# GCS ì—°ê²°
# ----------------------------

client = storage.Client()
bucket_raw = client.bucket(BUCKET_RAW)
bucket_index = client.bucket(BUCKET_INDEX)

# ----------------------------
# ì„œë¸Œë ˆë”§ ëª©ë¡
# ----------------------------

subreddits = [
    "dividends", "EconMonitor", "economics",
    "economy", "finance", "financialindependence", "investing", "MacroEconomics",
    "personalfinance", "StockMarket", "stocks", "wallstreetbets"
]

def get_date_range(start_date, end_date):
    """ì‹œì‘ì¼ê³¼ ì¢…ë£Œì¼ ì‚¬ì´ì˜ ëª¨ë“  ë‚ ì§œë¥¼ ìƒì„±"""
    start = datetime.strptime(start_date, "%Y-%m-%d")
    end = datetime.strptime(end_date, "%Y-%m-%d")
    date_list = []
    current = start
    while current <= end:
        date_list.append(current.strftime("%Y-%m-%d"))
        current += timedelta(days=1)
    return date_list

def main(start_date, end_date):
    # ë‚ ì§œ ë²”ìœ„ë¡œ ëœ JSON íŒŒì¼ ê²½ë¡œ
    date_range_str = f"{start_date}_to_{end_date}"
    json_blob_path = f"{REDDIT_PREFIX}full/{date_range_str}.json"
    
    logger.info(f"ğŸ“¦ íŒŒì¼ ê²€ìƒ‰ ì¤‘: gs://{BUCKET_RAW}/{json_blob_path}")
    blob = bucket_raw.blob(json_blob_path)
    
    if not blob.exists():
        logger.error("ğŸš« í•´ë‹¹ ë‚ ì§œì˜ Reddit ë°ì´í„° íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return

    # ë°ì´í„° ë¡œë“œ
    content = blob.download_as_bytes()
    all_data = json.load(BytesIO(content))
    
    # ë‚ ì§œë³„ë¡œ ë°ì´í„° ë¶„ë¥˜
    date_data_map = {}
    for item in all_data:
        created_utc = datetime.fromtimestamp(item.get("created_utc", 0))
        date_str = created_utc.strftime("%Y-%m-%d")
        if date_str not in date_data_map:
            date_data_map[date_str] = []
        date_data_map[date_str].append(item)

    # ê° ë‚ ì§œë³„ ì²˜ë¦¬
    for target_date, data in date_data_map.items():
        logger.info(f"\n=== {target_date} ì²˜ë¦¬ ì‹œì‘ ({len(data)}ê°œ í•­ëª©) ===")
        
        # ë¡œê·¸ ë””ë ‰í† ë¦¬ ì„¤ì •
        LOG_DATE_DIR = os.path.join(LOG_ROOT, target_date)
        os.makedirs(LOG_DATE_DIR, exist_ok=True)

        results = []
        all_scores = []
        failed_items = []

        # ì„œë¸Œë ˆë”§ë³„ë¡œ ë°ì´í„° ë¶„ë¥˜ ë° ì²˜ë¦¬
        subreddit_data = {}
        for item in data:
            subreddit = item.get("subreddit", "unknown")
            if subreddit not in subreddit_data:
                subreddit_data[subreddit] = []
            subreddit_data[subreddit].append(item)

        for subreddit in subreddits:
            if subreddit not in subreddit_data:
                logger.warning(f"[!] {subreddit} ë°ì´í„° ì—†ìŒ")
                continue

            subreddit_items = subreddit_data[subreddit]
            logger.info(f"\nì²˜ë¦¬ ì¤‘: r/{subreddit} ({len(subreddit_items)}ê°œ í•­ëª©)")

            neg_scores = []
            for idx, item in enumerate(subreddit_items, 1):
                try:
                    text = item.get("title", "") + " " + item.get("selftext", "")
                    text = text.strip()
                    if not text:
                        continue
                    
                    result = finbert(text[:512])[0]
                    label = result["label"].lower()
                    score = result["score"]
                    
                    if label == "negative":
                        neg_scores.append(score)
                        all_scores.append(score)
                    
                    time.sleep(SLEEP_SECONDS)
                    
                except Exception as e:
                    failed_items.append({
                        "subreddit": subreddit,
                        "id": item.get("id", "unknown"),
                        "error": str(e)
                    })
                    continue

            avg_score = sum(neg_scores) / len(neg_scores) if neg_scores else 0
            results.append({"subreddit": subreddit, "anxiety_score": avg_score})

        # Anxiety Index ê³„ì‚°
        total_texts = sum([len(json.load(BytesIO(bucket_raw.blob(f"{REDDIT_PREFIX}{sr}/{target_date}/accumulated.json").download_as_bytes())))
                        for sr in subreddits if bucket_raw.blob(f"{REDDIT_PREFIX}{sr}/{target_date}/accumulated.json").exists()])
        
        negative_ratio = len(all_scores) / total_texts if total_texts > 0 else 0
        average_negative_score = sum(all_scores) / len(all_scores) if all_scores else 0
        anxiety_index = negative_ratio * average_negative_score

        # ì €ì¥
        save_path = f"reddit/{target_date}/{SAVE_FILENAME}"
        temp_file = f"/tmp/reddit_anxiety_index_{target_date}.csv"

        with open(temp_file, "w", encoding="utf-8") as f:
            f.write("negative_ratio,average_negative_score,anxiety_index\n")
            f.write(f"{negative_ratio:.3f},{average_negative_score:.3f},{anxiety_index:.3f}\n\n")
            f.write("subreddit,anxiety_score\n")
            for row in results:
                f.write(f"{row['subreddit']},{row['anxiety_score']:.3f}\n")

        blob = bucket_index.blob(save_path)
        blob.upload_from_filename(temp_file)
        logger.info(f"âœ… ì €ì¥ ì™„ë£Œ â†’ gs://{BUCKET_INDEX}/{save_path}")

        # ì‹¤íŒ¨ ë¡œê·¸ ì €ì¥
        if failed_items:
            log_path = f"reddit/{target_date}/failed_items.json"
            temp_log_file = f"/tmp/failed_items_{target_date}.json"
            with open(temp_log_file, "w") as f:
                json.dump(failed_items, f, ensure_ascii=False, indent=2)
            log_blob = bucket_index.blob(log_path)
            log_blob.upload_from_filename(temp_log_file)
            logger.warning(f"âš ï¸ ì‹¤íŒ¨ ë¡œê·¸ ì €ì¥ë¨ â†’ gs://{BUCKET_INDEX}/{log_path}")

# ----------------------------
# CLI ì¸í„°í˜ì´ìŠ¤
# ----------------------------
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Reddit FinBERT ê°ì • ë¶„ì„ê¸°")
    parser.add_argument("--start", required=True, help="ì‹œì‘ì¼ (YYYY-MM-DD)")
    parser.add_argument("--end", required=True, help="ì¢…ë£Œì¼ (YYYY-MM-DD)")
    parser.add_argument("--debug", action="store_true", help="ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™”")
    args = parser.parse_args()
    
    try:
        # ë‚ ì§œ í˜•ì‹ ê²€ì¦
        start_date = datetime.strptime(args.start, "%Y-%m-%d")
        end_date = datetime.strptime(args.end, "%Y-%m-%d")
        
        if end_date < start_date:
            logger.error("âŒ ì¢…ë£Œì¼ì´ ì‹œì‘ì¼ë³´ë‹¤ ì•ì„¤ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            sys.exit(1)
            
        logger.info(f"ğŸ“… ë¶„ì„ ê¸°ê°„: {args.start} ~ {args.end}")
        logger.info("ğŸ”§ FinBERT ëª¨ë¸ ë¡œë”© ì¤‘...")
        
        main(args.start, args.end)
        
    except ValueError as e:
        logger.error(f"âŒ ë‚ ì§œ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤: {str(e)}")
        sys.exit(1)
    except Exception as e:
        if args.debug:
            logger.exception("ì˜¤ë¥˜ ë°œìƒ:")
        else:
            logger.error(f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        sys.exit(1)
