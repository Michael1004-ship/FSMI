from datetime import datetime, timedelta
import os
import json
import pandas as pd
from io import BytesIO
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
from google.cloud import storage
import time
import logging
import argparse
import sys
import concurrent.futures
from tqdm import tqdm
import random

# ----------------------------
# ì„¤ì •
# ----------------------------
BUCKET_RAW = "emotion-raw-data"
BUCKET_INDEX = "emotion-index-data"
REDDIT_PREFIX = "sns/reddit/"
SLEEP_SECONDS = 0.1  # ë³‘ë ¬ ì²˜ë¦¬í•˜ë¯€ë¡œ ì§€ì—° ì‹œê°„ ê°ì†Œ
SAVE_FILENAME = "reddit_anxiety_index.csv"

# ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì •
MAX_WORKERS = 8  # ë™ì‹œ ì²˜ë¦¬í•  ìµœëŒ€ ìŠ¤ë ˆë“œ ìˆ˜

# ----------------------------
# ë¡œê·¸ ì„¤ì •
# ----------------------------
LOG_ROOT = "/home/hwangjeongmun691/logs"
today = datetime.utcnow().strftime("%Y-%m-%d")
LOG_DATE_DIR = f"{LOG_ROOT}/{today}"
os.makedirs(LOG_DATE_DIR, exist_ok=True)

# ìŠ¤í¬ë¦½íŠ¸ ì´ë¦„ ê¸°ë°˜ìœ¼ë¡œ ë¡œê·¸ íŒŒì¼ ì„¤ì •
script_name = os.path.basename(__file__)
log_file = f"{LOG_DATE_DIR}/{script_name.replace('.py', '.log')}"

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("reddit_finbert")

# ----------------------------
# ëª¨ë¸ ë¡œë”© (FinBERT)
# ----------------------------
logger.info("ğŸ§  FinBERT ëª¨ë¸ ë¡œë”© ì¤‘...")
tokenizer = AutoTokenizer.from_pretrained("yiyanghkust/finbert-tone")
model = AutoModelForSequenceClassification.from_pretrained("yiyanghkust/finbert-tone")
finbert = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)
logger.info("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

# ----------------------------
# GCS ì—°ê²°
# ----------------------------
client = storage.Client()
bucket_raw = client.bucket(BUCKET_RAW)
bucket_index = client.bucket(BUCKET_INDEX)

# ----------------------------
# ì„œë¸Œë ˆë”§ ëª©ë¡
# ----------------------------
subreddits = [
    "dividends", "EconMonitor", "economics",
    "economy", "finance", "financialindependence", "investing", "MacroEconomics",
    "personalfinance", "StockMarket", "stocks", "wallstreetbets"
]

# ----------------------------
# ê°œë³„ í…ìŠ¤íŠ¸ ë¶„ì„ í•¨ìˆ˜ (ë³‘ë ¬ ì²˜ë¦¬ìš©)
# ----------------------------
def process_text(item, subreddit):
    """ë‹¨ì¼ Reddit ê²Œì‹œë¬¼ ë¶„ì„ (ë³‘ë ¬ ì²˜ë¦¬ìš©)"""
    try:
        text = item.get("title", "") + " " + item.get("selftext", "")
        text = text.strip()
        if not text:
            return {
                "success": False,
                "error": "í…ìŠ¤íŠ¸ ì—†ìŒ",
                "id": item.get("id", "unknown"),
                "subreddit": subreddit
            }
        
        # ê°ì • ë¶„ì„
        result = finbert(text[:512])[0]
        label = result["label"].lower()
        score = result["score"]
        
        # ì§§ì€ ì§€ì—° (ì¶©ëŒ ë°©ì§€)
        time.sleep(SLEEP_SECONDS + random.uniform(0, 0.1))
        
        return {
            "success": True,
            "id": item.get("id", "unknown"),
            "subreddit": subreddit,
            "label": label,
            "score": score,
            "is_negative": label == "negative"
        }
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "id": item.get("id", "unknown"),
            "subreddit": subreddit
        }

def get_date_range(start_date, end_date):
    """ì‹œì‘ì¼ê³¼ ì¢…ë£Œì¼ ì‚¬ì´ì˜ ëª¨ë“  ë‚ ì§œë¥¼ ìƒì„±"""
    start = datetime.strptime(start_date, "%Y-%m-%d")
    end = datetime.strptime(end_date, "%Y-%m-%d")
    date_list = []
    current = start
    while current <= end:
        date_list.append(current.strftime("%Y-%m-%d"))
        current += timedelta(days=1)
    return date_list

def process_subreddit(subreddit, items, executor):
    """ì„œë¸Œë ˆë”§ ë°ì´í„° ë³‘ë ¬ ì²˜ë¦¬"""
    logger.info(f"\nì²˜ë¦¬ ì¤‘: r/{subreddit} ({len(items)}ê°œ í•­ëª©)")
    
    # ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì‘ì—… ì œì¶œ
    futures = [executor.submit(process_text, item, subreddit) for item in items]
    
    neg_scores = []
    failed_items = []
    
    # ì§„í–‰ ìƒí™©ì„ ìœ„í•œ tqdm
    with tqdm(total=len(futures), desc=f"r/{subreddit}", unit="í…ìŠ¤íŠ¸") as pbar:
        for future in concurrent.futures.as_completed(futures):
            result = future.result()
            pbar.update(1)
            
            if result["success"]:
                if result["is_negative"]:
                    neg_scores.append(result["score"])
            else:
                failed_items.append({
                    "subreddit": result["subreddit"],
                    "id": result["id"],
                    "error": result["error"]
                })
    
    avg_score = sum(neg_scores) / len(neg_scores) if neg_scores else 0
    return {
        "subreddit": subreddit,
        "anxiety_score": avg_score,
        "negative_scores": neg_scores,
        "failed_items": failed_items
    }

# ----------------------------
# ë©”ì¸ í•¨ìˆ˜
# ----------------------------
def main(start_date, end_date, workers=MAX_WORKERS, debug=False):
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ë‚ ì§œ ë²”ìœ„ë¡œ ëœ JSON íŒŒì¼ ê²½ë¡œ
    date_range_str = f"{start_date}_to_{end_date}"
    json_blob_path = f"{REDDIT_PREFIX}full/{date_range_str}.json"
    
    logger.info(f"ğŸ“¦ íŒŒì¼ ê²€ìƒ‰ ì¤‘: gs://{BUCKET_RAW}/{json_blob_path}")
    blob = bucket_raw.blob(json_blob_path)
    
    if not blob.exists():
        logger.error("ğŸš« í•´ë‹¹ ë‚ ì§œì˜ Reddit ë°ì´í„° íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return

    # ì „ì²´ ì‹œì‘ ì‹œê°„
    total_start_time = time.time()
    
    # ë°ì´í„° ë¡œë“œ
    content = blob.download_as_bytes()
    all_data = json.load(BytesIO(content))
    logger.info(f"ì „ì²´ {len(all_data)}ê°œì˜ ê²Œì‹œë¬¼ ë¡œë“œë¨")
    
    # ë‚ ì§œë³„ë¡œ ë°ì´í„° ë¶„ë¥˜
    date_data_map = {}
    for item in all_data:
        created_utc = datetime.fromtimestamp(item.get("created_utc", 0))
        date_str = created_utc.strftime("%Y-%m-%d")
        if date_str not in date_data_map:
            date_data_map[date_str] = []
        date_data_map[date_str].append(item)
    
    # ì²˜ë¦¬í•  ë‚ ì§œ ëª©ë¡
    dates = list(date_data_map.keys())
    total_dates = len(dates)
    logger.info(f"ì´ {total_dates}ê°œ ë‚ ì§œ ì²˜ë¦¬ ì˜ˆì •: {', '.join(dates)}")

    # ê° ë‚ ì§œë³„ ì²˜ë¦¬
    for date_idx, target_date in enumerate(dates, 1):
        data = date_data_map[target_date]
        date_start_time = time.time()
        logger.info(f"\n{'='*50}")
        logger.info(f"ğŸ“… {target_date} ì²˜ë¦¬ ì‹œì‘ ({len(data)}ê°œ í•­ëª©) - {date_idx}/{total_dates}")
        
        # ë¡œê·¸ ë””ë ‰í† ë¦¬ ì„¤ì •
        target_log_dir = os.path.join(LOG_ROOT, target_date)
        os.makedirs(target_log_dir, exist_ok=True)

        results = []
        all_negative_scores = []
        all_failed_items = []

        # ì„œë¸Œë ˆë”§ë³„ë¡œ ë°ì´í„° ë¶„ë¥˜
        subreddit_data = {}
        for item in data:
            subreddit = item.get("subreddit", "unknown")
            if subreddit not in subreddit_data:
                subreddit_data[subreddit] = []
            subreddit_data[subreddit].append(item)

        # ThreadPoolExecutor ì„¤ì •
        with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:
            # ê° ì„œë¸Œë ˆë”§ ì²˜ë¦¬
            for subreddit in subreddits:
                if subreddit not in subreddit_data:
                    logger.warning(f"[!] {subreddit} ë°ì´í„° ì—†ìŒ")
                    continue

                subreddit_items = subreddit_data[subreddit]
                
                # ì„œë¸Œë ˆë”§ ì²˜ë¦¬
                subreddit_result = process_subreddit(subreddit, subreddit_items, executor)
                results.append({
                    "subreddit": subreddit, 
                    "anxiety_score": subreddit_result["anxiety_score"]
                })
                all_negative_scores.extend(subreddit_result["negative_scores"])
                all_failed_items.extend(subreddit_result["failed_items"])

        # Anxiety Index ê³„ì‚°
        total_texts = len(data)  # í•´ë‹¹ ë‚ ì§œì˜ ì „ì²´ í…ìŠ¤íŠ¸ ìˆ˜
        negative_ratio = len(all_negative_scores) / total_texts if total_texts > 0 else 0
        average_negative_score = sum(all_negative_scores) / len(all_negative_scores) if all_negative_scores else 0
        anxiety_index = negative_ratio * average_negative_score

        # ê²°ê³¼ ìš”ì•½
        logger.info(f"\nğŸ“Š {target_date} ë¶„ì„ ê²°ê³¼:")
        logger.info(f"  â€¢ ì´ ê²Œì‹œë¬¼: {total_texts}ê°œ")
        logger.info(f"  â€¢ ë¶€ì • ê²Œì‹œë¬¼: {len(all_negative_scores)}ê°œ")
        logger.info(f"  â€¢ ë¶€ì • ë¹„ìœ¨: {negative_ratio:.3f}")
        logger.info(f"  â€¢ í‰ê·  ë¶€ì • ì ìˆ˜: {average_negative_score:.3f}")
        logger.info(f"  â€¢ ë¶ˆì•ˆ ì§€ìˆ˜: {anxiety_index:.3f}")
        logger.info(f"  â€¢ ì‹¤íŒ¨ ìˆ˜: {len(all_failed_items)}ê°œ")

        # ì €ì¥
        save_path = f"reddit/{target_date}/{SAVE_FILENAME}"
        temp_file = f"/tmp/reddit_anxiety_index_{target_date}.csv"

        with open(temp_file, "w", encoding="utf-8") as f:
            f.write("negative_ratio,average_negative_score,anxiety_index\n")
            f.write(f"{negative_ratio:.3f},{average_negative_score:.3f},{anxiety_index:.3f}\n\n")
            f.write("subreddit,anxiety_score\n")
            for row in results:
                f.write(f"{row['subreddit']},{row['anxiety_score']:.3f}\n")

        blob = bucket_index.blob(save_path)
        blob.upload_from_filename(temp_file)
        logger.info(f"âœ… ì €ì¥ ì™„ë£Œ â†’ gs://{BUCKET_INDEX}/{save_path}")

        # ì‹¤íŒ¨ ë¡œê·¸ ì €ì¥
        if all_failed_items:
            log_path = f"reddit/{target_date}/failed_items.json"
            temp_log_file = f"/tmp/failed_items_{target_date}.json"
            with open(temp_log_file, "w") as f:
                json.dump(all_failed_items, f, ensure_ascii=False, indent=2)
            log_blob = bucket_index.blob(log_path)
            log_blob.upload_from_filename(temp_log_file)
            logger.warning(f"âš ï¸ ì‹¤íŒ¨ ë¡œê·¸ ì €ì¥ë¨ â†’ gs://{BUCKET_INDEX}/{log_path}")
        
        # ë‚ ì§œë³„ ì†Œìš” ì‹œê°„ ê³„ì‚°
        date_elapsed = time.time() - date_start_time
        logger.info(f"â±ï¸ {target_date} ì²˜ë¦¬ ì™„ë£Œ: {date_elapsed:.1f}ì´ˆ ({date_elapsed/60:.1f}ë¶„)")
    
    # ì „ì²´ ì†Œìš” ì‹œê°„ ê³„ì‚°
    total_elapsed = time.time() - total_start_time
    logger.info(f"\nğŸ‰ ëª¨ë“  ë‚ ì§œ ì²˜ë¦¬ ì™„ë£Œ! ì´ ì†Œìš” ì‹œê°„: {total_elapsed:.1f}ì´ˆ ({total_elapsed/60:.1f}ë¶„)")

# ----------------------------
# CLI ì¸í„°í˜ì´ìŠ¤
# ----------------------------
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Reddit FinBERT ê°ì • ë¶„ì„ê¸°")
    parser.add_argument("--start", required=True, help="ì‹œì‘ì¼ (YYYY-MM-DD)")
    parser.add_argument("--end", required=True, help="ì¢…ë£Œì¼ (YYYY-MM-DD)")
    parser.add_argument("--debug", action="store_true", help="ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™”")
    parser.add_argument("--workers", type=int, default=MAX_WORKERS, help=f"ë³‘ë ¬ ì²˜ë¦¬ ìŠ¤ë ˆë“œ ìˆ˜ (ê¸°ë³¸ê°’: {MAX_WORKERS})")
    args = parser.parse_args()
    
    try:
        # ë‚ ì§œ í˜•ì‹ ê²€ì¦
        start_date = datetime.strptime(args.start, "%Y-%m-%d")
        end_date = datetime.strptime(args.end, "%Y-%m-%d")
        
        if end_date < start_date:
            logger.error("âŒ ì¢…ë£Œì¼ì´ ì‹œì‘ì¼ë³´ë‹¤ ì•ì„¤ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            sys.exit(1)
            
        logger.info(f"ğŸ“… ë¶„ì„ ê¸°ê°„: {args.start} ~ {args.end}")
        logger.info(f"ğŸ§µ ë³‘ë ¬ ì²˜ë¦¬ ìŠ¤ë ˆë“œ ìˆ˜: {args.workers}")
        
        if args.debug:
            logger.setLevel(logging.DEBUG)
            logger.info("ğŸ”§ ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™”")
        
        main(args.start, args.end, args.workers, args.debug)
        
    except ValueError as e:
        logger.error(f"âŒ ë‚ ì§œ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤: {str(e)}")
        sys.exit(1)
    except Exception as e:
        if args.debug:
            logger.exception("ì˜¤ë¥˜ ë°œìƒ:")
        else:
            logger.error(f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        sys.exit(1)
