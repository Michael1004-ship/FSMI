from datetime import datetime, timedelta, time
# scripts/reddit_realtime_collector.py
import os
import json

import logging
import pandas as pd
import praw
from google.cloud import storage
from dotenv import load_dotenv

# âœ… í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# âœ… ì„¤ì •
SUBREDDITS = [
    # ê²½ì œ ê´€ë ¨
    "economics", "economy", "MacroEconomics", "EconMonitor",
    # ê¸ˆìœµ ê´€ë ¨
    "finance", "investing", "financialindependence", "personalfinance",
    "wallstreetbets", "stocks", "StockMarket", "dividends"
]

BUCKET_NAME = "emotion-raw-data"
STATE_FILE = "reddit_last_run.txt"
POST_LIMIT = 1000  # ê° ì„œë¸Œë ˆë”§ë‹¹ ìµœëŒ€ ìˆ˜ì§‘ ìˆ˜ (ê¸°ì¡´ 100 â†’ 1000)

# âœ… ë¡œê¹… ì„¤ì •
# ë¡œê·¸ ë””ë ‰í† ë¦¬ ì„¤ì •
LOG_ROOT = "/home/hwangjeongmun691/logs"
today = datetime.utcnow().strftime("%Y-%m-%d")
LOG_DATE_DIR = f"{LOG_ROOT}/{today}"

# ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs(LOG_DATE_DIR, exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f"{LOG_DATE_DIR}/reddit_collector.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("reddit_collector")

# âœ… Reddit API ì´ˆê¸°í™” (.env íŒŒì¼ì—ì„œ ë¡œë“œ)
reddit = praw.Reddit(
    client_id=os.getenv("REDDIT_CLIENT_ID"),
    client_secret=os.getenv("REDDIT_CLIENT_SECRET"),
    user_agent=os.getenv("REDDIT_USER_AGENT"),
)

# âœ… ëˆ„ì  ë°ì´í„° ë¡œë“œ
def load_existing_data(storage_client, subreddit, date_str):
    file_path = f"sns/reddit/{subreddit}/{date_str}/accumulated.json"
    bucket = storage_client.bucket(BUCKET_NAME)
    blob = bucket.blob(file_path)
    if blob.exists():
        content = blob.download_as_string()
        return pd.DataFrame(json.loads(content))
    return pd.DataFrame(columns=['id', 'title', 'selftext', 'created_utc', 'score'])

# âœ… ì €ì¥
def save_to_gcs(df, subreddit, date_str):
    file_path = f"sns/reddit/{subreddit}/{date_str}/accumulated.json"
    storage_client = storage.Client()
    bucket = storage_client.bucket(BUCKET_NAME)
    blob = bucket.blob(file_path)
    blob.upload_from_string(json.dumps(df.to_dict(orient='records'), ensure_ascii=False), content_type='application/json')
    logger.info(f"âœ… ì €ì¥ ì™„ë£Œ: gs://{BUCKET_NAME}/{file_path}")

# âœ… ë©”ì¸ ì‹¤í–‰

def run():
    # í˜„ì¬ ì‹œê°„ê³¼ ì‹œê°„ëŒ€ ì„¤ì •
    now = datetime.utcnow()
    hour = now.hour
    date_str = now.strftime("%Y-%m-%d")
    
    if hour < 13:
        # ì˜¤ì „ ìˆ˜ì§‘ â†’ ì „ë‚  19:00 ~ ì˜¤ëŠ˜ 12:30
        start = datetime.combine((now - timedelta(days=1)).date(), time(hour=19))
        end = datetime.combine(now.date(), time(hour=12, minute=30))
    else:
        # ì˜¤í›„ ìˆ˜ì§‘ â†’ ì˜¤ëŠ˜ 12:30 ~ ì˜¤ëŠ˜ 19:00
        start = datetime.combine(now.date(), time(hour=12, minute=30))
        end = datetime.combine(now.date(), time(hour=19))
    # âœ… ë”± ì´ ë‘ êµ¬ê°„ë§Œ ë§¤ì¼ ë°˜ë³µ ìˆ˜ì§‘ë˜ë„ë¡ ì„¤ê³„ë¨
    # âœ… GDELTì²˜ëŸ¼ "now -1ì‹œê°„" ë³´ì • ì—†ìŒ
    # âœ… ì‹œê°„ ëˆ„ë½ì´ë‚˜ ì¤‘ë³µ ì—†ì´, ê¹”ë”í•˜ê²Œ "2íšŒ ìˆ˜ì§‘ = í•˜ë£¨ ì „ì²´ ìˆ˜ì§‘" ì™„ì„±
    
    logger.info(f"Reddit ìˆ˜ì§‘ ì‹œì‘: {date_str}, ì‹œê°„ ë²”ìœ„: {start} ~ {end}")

    for sub in SUBREDDITS:
        logger.info(f"ğŸ“¥ ì„œë¸Œë ˆë”§: r/{sub}")
        posts = []

        for submission in reddit.subreddit(sub).new(limit=POST_LIMIT):
            # ğŸ” ìˆ˜ì§‘ ì‹œê°„ í•„í„°
            created_time = datetime.utcfromtimestamp(submission.created_utc)
            if not (start <= created_time <= end):
                continue
                
            posts.append({
                "id": submission.id,
                "title": submission.title,
                "selftext": submission.selftext,
                "created_utc": created_time.isoformat(),
                "score": submission.score
            })

        new_df = pd.DataFrame(posts)
        if new_df.empty:
            logger.info(f"ğŸ”¸ ì‹ ê·œ ê¸€ ì—†ìŒ: r/{sub}")
            continue

        storage_client = storage.Client()
        existing_df = load_existing_data(storage_client, sub, date_str)
        combined_df = pd.concat([existing_df, new_df], ignore_index=True).drop_duplicates(subset=['id'])
        save_to_gcs(combined_df, sub, date_str)

    logger.info("ğŸ‰ Reddit ìˆ˜ì§‘ ì™„ë£Œ!")

if __name__ == "__main__":
    run()