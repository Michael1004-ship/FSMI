# scripts/reddit_realtime_collector.py
import os
import json
import datetime
import logging
import pandas as pd
import praw
from google.cloud import storage
from dotenv import load_dotenv

# âœ… í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# âœ… ì„¤ì •
SUBREDDITS = [
    # ê²½ì œ ê´€ë ¨
    "economics", "economy", "MacroEconomics", "EconMonitor",
    # ê¸ˆìœµ ê´€ë ¨
    "finance", "investing", "financialindependence", "personalfinance",
    "wallstreetbets", "stocks", "StockMarket", "dividends"
]

BUCKET_NAME = "emotion-raw-data"
STATE_FILE = "reddit_last_run.txt"
POST_LIMIT = 100  # ê° ì„œë¸Œë ˆë”§ë‹¹ ìµœëŒ€ ìˆ˜ì§‘ ìˆ˜

# âœ… ë¡œê¹… ì„¤ì •
# ë¡œê·¸ ë””ë ‰í† ë¦¬ ì„¤ì •
LOG_ROOT = "/home/hwangjeongmun691/logs"
today = datetime.utcnow().strftime("%Y-%m-%d")
LOG_DATE_DIR = f"{LOG_ROOT}/{today}"

# ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs(LOG_DATE_DIR, exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f"{LOG_DATE_DIR}/reddit_collector.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("reddit_collector")

# âœ… Reddit API ì´ˆê¸°í™” (.env íŒŒì¼ì—ì„œ ë¡œë“œ)
reddit = praw.Reddit(
    client_id=os.getenv("REDDIT_CLIENT_ID"),
    client_secret=os.getenv("REDDIT_CLIENT_SECRET"),
    user_agent=os.getenv("REDDIT_USER_AGENT"),
)

# âœ… ëˆ„ì  ë°ì´í„° ë¡œë“œ
def load_existing_data(storage_client, subreddit, date_str):
    file_path = f"sns/reddit/{subreddit}/{date_str}/accumulated.json"
    bucket = storage_client.bucket(BUCKET_NAME)
    blob = bucket.blob(file_path)
    if blob.exists():
        content = blob.download_as_string()
        return pd.DataFrame(json.loads(content))
    return pd.DataFrame(columns=['id', 'title', 'selftext', 'created_utc', 'score'])

# âœ… ì €ì¥
def save_to_gcs(df, subreddit, date_str):
    file_path = f"sns/reddit/{subreddit}/{date_str}/accumulated.json"
    storage_client = storage.Client()
    bucket = storage_client.bucket(BUCKET_NAME)
    blob = bucket.blob(file_path)
    blob.upload_from_string(json.dumps(df.to_dict(orient='records'), ensure_ascii=False), content_type='application/json')
    logger.info(f"âœ… ì €ì¥ ì™„ë£Œ: gs://{BUCKET_NAME}/{file_path}")

# âœ… ë©”ì¸ ì‹¤í–‰

def run():
    now = datetime.utcnow()
    date_str = now.strftime("%Y-%m-%d")
    logger.info(f"Reddit ìˆ˜ì§‘ ì‹œì‘: {date_str}")

    for sub in SUBREDDITS:
        logger.info(f"ğŸ“¥ ì„œë¸Œë ˆë”§: r/{sub}")
        posts = []

        for submission in reddit.subreddit(sub).new(limit=POST_LIMIT):
            posts.append({
                "id": submission.id,
                "title": submission.title,
                "selftext": submission.selftext,
                "created_utc": datetime.datetime.utcfromtimestamp(submission.created_utc).isoformat(),
                "score": submission.score
            })

        new_df = pd.DataFrame(posts)
        if new_df.empty:
            logger.info(f"ğŸ”¸ ì‹ ê·œ ê¸€ ì—†ìŒ: r/{sub}")
            continue

        storage_client = storage.Client()
        existing_df = load_existing_data(storage_client, sub, date_str)
        combined_df = pd.concat([existing_df, new_df], ignore_index=True).drop_duplicates(subset=['id'])
        save_to_gcs(combined_df, sub, date_str)

    logger.info("ğŸ‰ Reddit ìˆ˜ì§‘ ì™„ë£Œ!")

if __name__ == "__main__":
    run()