# scripts/gdelt_realtime_collector.py
import os
import requests
import zipfile
import io
import pandas as pd
import datetime
import logging
import time
import json
from google.cloud import storage

# ë¡œê¹… ì„¤ì •
import os
from datetime import datetime

# ë¡œê·¸ ë””ë ‰í† ë¦¬ ì„¤ì •
LOG_ROOT = "/home/hwangjeongmun691/logs"
today = datetime.utcnow().strftime("%Y-%m-%d")
LOG_DATE_DIR = f"{LOG_ROOT}/{today}"

# ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs(LOG_DATE_DIR, exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f"{LOG_DATE_DIR}/gdelt_collector.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("gdelt_collector")

TOP_DOMAINS = [
    "nytimes.com", "wsj.com", "bloomberg.com", "cnbc.com", "cnn.com",
    "ft.com", "reuters.com", "finance.yahoo.com", "forbes.com", "marketwatch.com"
]

FINANCE_KEYWORDS = [
    "market", "stock", "stocks", "equity", "shares", "finance", "financing",
    "interest rate", "interest rates", "bond", "bonds", "treasury",
    "investment", "investments", "investor", "investors",
    "fed", "federal reserve", "monetary policy",
    "nasdaq", "dow jones", "s&p", "sp500", "volatility",
    "bank", "banking", "credit", "debt",
    "economic", "economy", "gdp", "growth", "recession", "inflation",
    "deflation", "stagflation", "employment", "unemployment", "jobless",
    "labor market", "consumer confidence", "cpi", "ppi", "retail sales",
    "housing market", "housing prices", "industrial production"
]

BUCKET_NAME = "emotion-raw-data"

# âœ… ìˆ˜ì§‘ëœ ì‹œê°„ ê¸°ë¡ìš© íŒŒì¼
STATE_FILE = "last_collected.txt"

# âœ… 15ë¶„ ë‹¨ìœ„ ì‹œê°„ ìƒì„±ê¸°
def generate_time_stamps(start_time, end_time):
    stamps = []
    current = start_time
    while current <= end_time:
        stamps.append(current.strftime("%Y%m%d%H%M%S"))
        current += datetime.timedelta(minutes=15)
    return stamps

# âœ… GDELT URL ìƒì„±
def build_gkg_url(timestamp):
    return f"http://data.gdeltproject.org/gdeltv2/{timestamp}.gkg.csv.zip"

# âœ… zip íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë° íŒŒì‹±
def download_and_parse_csv(zip_url):
    try:
        logger.info(f"ë‹¤ìš´ë¡œë“œ: {zip_url}")
        res = requests.get(zip_url, timeout=60)
        res.raise_for_status()
        with zipfile.ZipFile(io.BytesIO(res.content)) as z:
            for name in z.namelist():
                with z.open(name) as f:
                    chunk_size = 10000
                    chunks = [chunk for chunk in pd.read_csv(f, sep='\t', header=None, dtype=str, chunksize=chunk_size)]
                    return pd.concat(chunks, ignore_index=True)
    except Exception as e:
        logger.warning(f"{zip_url} ì‹¤íŒ¨: {e}")
        return None

# âœ… ì»¬ëŸ¼ ì´ë¦„ ì„¤ì •
GKG_COLUMNS = [
    "GKGRECORDID", "DATE", "SourceCollectionIdentifier", "SourceCommonName",
    "DocumentIdentifier", "Counts", "V2Counts", "Themes", "V2Themes",
    "Locations", "V2Locations", "Persons", "V2Persons", "Organizations",
    "V2Organizations", "Tone", "Dates", "GCAM", "SharingImage",
    "RelatedImages", "SocialImageEmbeds", "SocialVideoEmbeds",
    "Quotations", "AllNames", "Amounts", "TranslationInfo",
    "Extras", "OriginalXML"
]

def set_column_names(df):
    if len(df.columns) == len(GKG_COLUMNS):
        df.columns = GKG_COLUMNS
    elif len(df.columns) == len(GKG_COLUMNS) - 1:
        df.columns = GKG_COLUMNS[:-1]
    else:
        df.columns = [f"Column_{i}" for i in range(len(df.columns))]
    return df

# âœ… í•„í„°ë§

def filter_finance_news(df):
    df = df[df['DocumentIdentifier'].str.contains('http', na=False)]
    df = df[df['DocumentIdentifier'].str.contains('|'.join(TOP_DOMAINS), case=False, na=False)]
    df = df[df['V2Themes'].str.contains('|'.join(FINANCE_KEYWORDS), case=False, na=False)]
    return df.drop_duplicates(subset=['DocumentIdentifier'])

# âœ… GCS ëˆ„ì  ì €ì¥

def load_existing_data(storage_client, date_str):
    file_name = f"news/gdelt/{date_str}/accumulated.json"
    bucket = storage_client.bucket(BUCKET_NAME)
    blob = bucket.blob(file_name)
    if blob.exists():
        content = blob.download_as_string()
        return pd.DataFrame(json.loads(content))
    return pd.DataFrame(columns=['DATE', 'DocumentIdentifier', 'SourceCommonName', 'V2Themes', 'Tone'])

def save_accumulated_to_gcs(df, date_str):
    file_name = f"news/gdelt/{date_str}/accumulated.json"
    storage_client = storage.Client()
    bucket = storage_client.bucket(BUCKET_NAME)
    blob = bucket.blob(file_name)
    blob.upload_from_string(json.dumps(df.to_dict(orient='records'), ensure_ascii=False), content_type='application/json')
    logger.info(f"âœ… ëˆ„ì  ì €ì¥ ì™„ë£Œ: gs://{BUCKET_NAME}/{file_name}")

# âœ… ë©”ì¸ ì‹¤í–‰

def run():
    now = datetime.datetime.utcnow()
    date_str = now.strftime("%Y-%m-%d")
    yesterday_str = (now - datetime.timedelta(days=1)).strftime("%Y-%m-%d")

    # ë§ˆì§€ë§‰ ìˆ˜ì§‘ ì‹œê°„ ë¶ˆëŸ¬ì˜¤ê¸°
    if os.path.exists(STATE_FILE):
        with open(STATE_FILE, 'r') as f:
            last_ts = datetime.datetime.strptime(f.read().strip(), "%Y%m%d%H%M%S")
    else:
        last_ts = now.replace(hour=0, minute=0, second=0, microsecond=0)

    logger.info(f"ğŸ” ìˆ˜ì§‘ ë²”ìœ„: {last_ts} ~ {now}")

    # ì‹œê°„ëŒ€ ë¦¬ìŠ¤íŠ¸ ìƒì„±
    stamps = generate_time_stamps(last_ts, now)

    all_dfs = []
    for stamp in stamps:
        url = build_gkg_url(stamp)
        df = download_and_parse_csv(url)
        if df is not None:
            df = set_column_names(df)
            df = filter_finance_news(df)
            all_dfs.append(df)

    storage_client = storage.Client()
    
    if all_dfs:
        new_df = pd.concat(all_dfs, ignore_index=True)
        logger.info(f"ğŸ“° ì‹ ê·œ ë‰´ìŠ¤ ìˆ˜ì§‘ ìˆ˜: {len(new_df)}")

        existing_df = load_existing_data(storage_client, date_str)
        combined_df = pd.concat([existing_df, new_df], ignore_index=True).drop_duplicates(subset=['DocumentIdentifier'])
        save_accumulated_to_gcs(combined_df, date_str)

        # ìˆ˜ì§‘ ì‹œê°„ ì—…ë°ì´íŠ¸
        with open(STATE_FILE, 'w') as f:
            f.write(now.strftime("%Y%m%d%H%M%S"))

    else:
        logger.info("ìƒˆë¡œìš´ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. ìµœê·¼ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        
        # ì˜¤ëŠ˜ ë°ì´í„° í™•ì¸
        today_data_exists = storage_client.bucket(BUCKET_NAME).blob(f"news/gdelt/{date_str}/accumulated.json").exists()
        
        if not today_data_exists:
            # ì–´ì œ ë°ì´í„° í™•ì¸
            yesterday_blob = storage_client.bucket(BUCKET_NAME).blob(f"news/gdelt/{yesterday_str}/accumulated.json")
            
            if yesterday_blob.exists():
                logger.info(f"ì–´ì œ({yesterday_str}) ë°ì´í„°ë¥¼ ì˜¤ëŠ˜({date_str}) í´ë”ì— ë³µì‚¬í•©ë‹ˆë‹¤.")
                content = yesterday_blob.download_as_string()
                
                # ì˜¤ëŠ˜ í´ë”ì— ì €ì¥
                today_blob = storage_client.bucket(BUCKET_NAME).blob(f"news/gdelt/{date_str}/accumulated.json")
                today_blob.upload_from_string(content, content_type='application/json')
                logger.info(f"âœ… ë°ì´í„° ë³µì‚¬ ì™„ë£Œ: gs://{BUCKET_NAME}/news/gdelt/{date_str}/accumulated.json")
            else:
                # ìµœê·¼ 3ì¼ ë‚´ ë°ì´í„° ì°¾ê¸°
                found_data = False
                for i in range(2, 7):
                    check_date = (now - datetime.timedelta(days=i)).strftime("%Y-%m-%d")
                    check_blob = storage_client.bucket(BUCKET_NAME).blob(f"news/gdelt/{check_date}/accumulated.json")
                    
                    if check_blob.exists():
                        logger.info(f"{check_date} ë°ì´í„°ë¥¼ ì˜¤ëŠ˜({date_str}) í´ë”ì— ë³µì‚¬í•©ë‹ˆë‹¤.")
                        content = check_blob.download_as_string()
                        today_blob = storage_client.bucket(BUCKET_NAME).blob(f"news/gdelt/{date_str}/accumulated.json")
                        today_blob.upload_from_string(content, content_type='application/json')
                        logger.info(f"âœ… ë°ì´í„° ë³µì‚¬ ì™„ë£Œ: gs://{BUCKET_NAME}/news/gdelt/{date_str}/accumulated.json")
                        found_data = True
                        break
                
                if not found_data:
                    # ìµœê·¼ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ë¹ˆ JSON íŒŒì¼ ìƒì„±
                    logger.warning("ìµœê·¼ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ë¹ˆ íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤.")
                    empty_df = pd.DataFrame(columns=['DATE', 'DocumentIdentifier', 'SourceCommonName', 'V2Themes', 'Tone'])
                    save_accumulated_to_gcs(empty_df, date_str)

if __name__ == "__main__":
    run()
