# scripts/gdelt_full_collector.py
import os
import requests
import zipfile
import io
import pandas as pd
import datetime
import logging
import json
from google.cloud import storage
import sys

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("gdelt_full_collector.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("gdelt_full_collector")

TOP_DOMAINS = [
    "nytimes.com", "wsj.com", "bloomberg.com", "cnbc.com", "cnn.com",
    "ft.com", "reuters.com", "finance.yahoo.com", "forbes.com", "marketwatch.com"
]

FINANCE_KEYWORDS = [
    "market", "stock", "stocks", "equity", "shares", "finance", "financing",
    "interest rate", "interest rates", "bond", "bonds", "treasury",
    "investment", "investments", "investor", "investors",
    "fed", "federal reserve", "monetary policy",
    "nasdaq", "dow jones", "s&p", "sp500", "volatility",
    "bank", "banking", "credit", "debt",
    "economic", "economy", "gdp", "growth", "recession", "inflation",
    "deflation", "stagflation", "employment", "unemployment", "jobless",
    "labor market", "consumer confidence", "cpi", "ppi", "retail sales",
    "housing market", "housing prices", "industrial production"
]

BUCKET_NAME = "emotion-raw-data"

# GKG ì»¬ëŸ¼ ì •ì˜
GKG_COLUMNS = [
    "GKGRECORDID", "DATE", "SourceCollectionIdentifier", "SourceCommonName",
    "DocumentIdentifier", "Counts", "V2Counts", "Themes", "V2Themes",
    "Locations", "V2Locations", "Persons", "V2Persons", "Organizations",
    "V2Organizations", "Tone", "Dates", "GCAM", "SharingImage",
    "RelatedImages", "SocialImageEmbeds", "SocialVideoEmbeds",
    "Quotations", "AllNames", "Amounts", "TranslationInfo",
    "Extras", "OriginalXML"
]

# ì „ì²´ URL ë¦¬ìŠ¤íŠ¸ ìƒì„±
MASTER_URL = "http://data.gdeltproject.org/gdeltv2/masterfilelist.txt"

def get_urls_between(start_date, end_date):
    logger.info("ğŸ” masterfilelist.txt ë¡œë“œ ì¤‘...")
    res = requests.get(MASTER_URL)
    lines = res.text.strip().split('\n')
    urls = []
    for line in lines:
        if ".gkg.csv.zip" in line:
            parts = line.strip().split()
            url = parts[2]
            timestamp = url.split("/")[-1].split(".")[0]
            dt = datetime.datetime.strptime(timestamp, "%Y%m%d%H%M%S")
            if start_date <= dt <= end_date:
                urls.append(url)
    logger.info(f"ì´ {len(urls)}ê°œì˜ GKG URL ìˆ˜ì§‘ ì˜ˆì •")
    return urls

def download_and_parse_csv(zip_url):
    try:
        logger.info(f"ë‹¤ìš´ë¡œë“œ: {zip_url}")
        res = requests.get(zip_url, timeout=60)
        res.raise_for_status()
        with zipfile.ZipFile(io.BytesIO(res.content)) as z:
            for name in z.namelist():
                with z.open(name) as f:
                    chunk_size = 10000
                    chunks = [chunk for chunk in pd.read_csv(f, sep='\t', header=None, dtype=str, chunksize=chunk_size)]
                    return pd.concat(chunks, ignore_index=True)
    except Exception as e:
        logger.warning(f"{zip_url} ì‹¤íŒ¨: {e}")
        return None

def set_column_names(df):
    if len(df.columns) == len(GKG_COLUMNS):
        df.columns = GKG_COLUMNS
    elif len(df.columns) == len(GKG_COLUMNS) - 1:
        df.columns = GKG_COLUMNS[:-1]
    else:
        df.columns = [f"Column_{i}" for i in range(len(df.columns))]
    return df

def filter_finance_news(df):
    df = df[df['DocumentIdentifier'].str.contains('http', na=False)]
    df = df[df['DocumentIdentifier'].str.contains('|'.join(TOP_DOMAINS), case=False, na=False)]
    df = df[df['V2Themes'].str.contains('|'.join(FINANCE_KEYWORDS), case=False, na=False)]
    return df.drop_duplicates(subset=['DocumentIdentifier'])

def save_to_gcs(df, file_name):
    storage_client = storage.Client()
    bucket = storage_client.bucket(BUCKET_NAME)
    blob = bucket.blob(file_name)
    blob.upload_from_string(json.dumps(df.to_dict(orient='records'), ensure_ascii=False), content_type='application/json')
    logger.info(f"âœ… ì €ì¥ ì™„ë£Œ: gs://{BUCKET_NAME}/{file_name}")

def run(start_str, end_str):
    start_dt = datetime.datetime.strptime(start_str, "%Y-%m-%d")
    end_dt = datetime.datetime.strptime(end_str, "%Y-%m-%d")

    # URL ìˆ˜ì§‘ ì§„í–‰ ìƒí™© í‘œì‹œ
    logger.info(f"ğŸ” {start_str}ë¶€í„° {end_str}ê¹Œì§€ì˜ GDELT URL ìˆ˜ì§‘ ì¤‘...")
    urls = get_urls_between(start_dt, end_dt + datetime.timedelta(days=1))
    logger.info(f"ğŸ“‹ ì´ {len(urls)}ê°œì˜ GDELT URL ìˆ˜ì§‘ ì™„ë£Œ")
    
    # ë¹ˆ ìƒíƒœ í™•ì¸
    if len(urls) == 0:
        logger.warning("âŒ ìˆ˜ì§‘í•  URLì´ ì—†ìŠµë‹ˆë‹¤. ë‚ ì§œ ë²”ìœ„ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return
        
    all_dfs = []
    processed_count = 0
    start_time = datetime.datetime.now()
    
    # ì§„í–‰ë¥  í‘œì‹œ
    logger.info(f"â³ GDELT ë°ì´í„° ë‹¤ìš´ë¡œë“œ ë° ë¶„ì„ ì‹œì‘ ({len(urls)}ê°œ íŒŒì¼)")
    
    for i, url in enumerate(urls):
        # ì£¼ê¸°ì ìœ¼ë¡œ ì§„í–‰ ìƒí™© ë³´ê³ 
        processed_count = i + 1
        if processed_count % 10 == 0 or processed_count == 1 or processed_count == len(urls):
            elapsed = (datetime.datetime.now() - start_time).total_seconds()
            progress = processed_count / len(urls) * 100
            remaining = elapsed / processed_count * (len(urls) - processed_count) if processed_count > 0 else 0
            
            logger.info(f"ğŸ”„ ì§„í–‰ë¥ : {progress:.1f}% ({processed_count}/{len(urls)}) - " 
                        f"ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {remaining/60:.1f}ë¶„")
        
        df = download_and_parse_csv(url)
        if df is not None:
            logger.info(f"âœ“ íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ: {url} - í–‰ ìˆ˜: {len(df)}")
            df = set_column_names(df)
            filtered_df = filter_finance_news(df)
            logger.info(f"  â†’ í•„í„°ë§ í›„ ê¸ˆìœµ ë‰´ìŠ¤: {len(filtered_df)}/{len(df)} ({len(filtered_df)/len(df)*100:.1f}%)")
            all_dfs.append(filtered_df)
            
    # ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½
    if all_dfs:
        final_df = pd.concat(all_dfs, ignore_index=True)
        logger.info(f"ğŸ“Š ìµœì¢… ë°ì´í„°ì…‹: {len(final_df)}ê°œ ê¸ˆìœµ ë‰´ìŠ¤ (ì¤‘ë³µ ì œê±° í›„)")
        
        # ë‚ ì§œë³„ ë°ì´í„° ë¶„í¬ ë¶„ì„
        if 'DATE' in final_df.columns:
            df_copy = final_df.copy()
            df_copy['date_only'] = df_copy['DATE'].str[:8]  # YYYYMMDDHHMMSS í˜•ì‹ì—ì„œ YYYYMMDD ì¶”ì¶œ
            date_counts = df_copy['date_only'].value_counts().sort_index()
            logger.info("ğŸ“… ë‚ ì§œë³„ ë‰´ìŠ¤ ë¶„í¬:")
            for date, count in date_counts.items():
                date_formatted = f"{date[:4]}-{date[4:6]}-{date[6:8]}"
                logger.info(f"  â€¢ {date_formatted}: {count}ê°œ")
        
        fname = f"news/gdelt/full/{start_str}_to_{end_str}.json"
        logger.info(f"ğŸ’¾ GCSì— ì €ì¥ ì¤‘: {fname}")
        save_to_gcs(final_df, fname)
        logger.info(f"ğŸ‰ ëª¨ë“  ì²˜ë¦¬ ì™„ë£Œ - {len(urls)}ê°œ íŒŒì¼ì—ì„œ {len(final_df)}ê°œ ë‰´ìŠ¤ ì €ì¥")
    else:
        logger.warning("âš ï¸ ì €ì¥í•  ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. í•„í„°ë§ ì¡°ê±´ì„ í™•ì¸í•˜ì„¸ìš”.")

if __name__ == "__main__":
    # ëª…ë ¹ì¤„ ì¸ì í™•ì¸
    if len(sys.argv) >= 3:
        start = sys.argv[1]
        end = sys.argv[2]
    else:
        # ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
        print("GDELT ë°ì´í„° ìˆ˜ì§‘ ë„êµ¬")
        print("-" * 30)
        start = input("ì‹œì‘ì¼ (YYYY-MM-DD í˜•ì‹): ")
        end = input("ì¢…ë£Œì¼ (YYYY-MM-DD í˜•ì‹): ")
    
    # ë‚ ì§œ í˜•ì‹ ê²€ì¦ (YYYY-MM-DD)
    try:
        datetime.datetime.strptime(start, "%Y-%m-%d")
        datetime.datetime.strptime(end, "%Y-%m-%d")
    except ValueError:
        print("ì˜¤ë¥˜: ë‚ ì§œ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. YYYY-MM-DD í˜•ì‹ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
        sys.exit(1)
    
    logger.info(f"ìˆ˜ì§‘ ê¸°ê°„: {start} ~ {end}")
    run(start, end)