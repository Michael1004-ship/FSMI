from datetime import datetime
import json
import openai
import subprocess
import os
import time
from dotenv import load_dotenv
import logging

# 로그 디렉토리 설정
LOG_ROOT = "/home/hwangjeongmun691/logs"
today = datetime.utcnow().strftime("%Y-%m-%d")
LOG_DATE_DIR = f"{LOG_ROOT}/{today}"

# 디렉토리 생성
os.makedirs(LOG_DATE_DIR, exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f"{LOG_DATE_DIR}/gpt_report.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("gpt_report")

# ✅ Load OpenAI API key from .env
logger.info(f"⏱️ {datetime.now().strftime('%H:%M:%S')} - 환경 변수 로드 중...")
load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")
if openai.api_key:
    logger.info("✅ API 키 로드 완료")
else:
    logger.error("❌ API 키를 찾을 수 없습니다. .env 파일을 확인하세요.")

# ✅ Set current UTC date
date_str = datetime.utcnow().strftime("%Y-%m-%d")
# date_str = "2025-04-08"
sources = ["news", "reddit"]
logger.info(f"📅 처리 날짜: {date_str}")

# ✅ GCS & local path formats
base_gcs = "gs://emotion-index-data/{source}/{date}/{filename}"
local_tmp = "/tmp/{source}_{filename}"
report_local_path = "/tmp/gpt_report_combined.txt"
appendix_local_path = "/tmp/gpt_report_appendix.txt"
report_gcs_path = f"gs://emotion-index-data/final_anxiety_index/{date_str}/gpt_report_combined.txt"
appendix_gcs_path = f"gs://emotion-index-data/final_anxiety_index/{date_str}/gpt_report_appendix.txt"

required_files = [
    "cluster_labels.json",
    "cluster_representative_texts.json",
    "emotion_ratio.json"
]

logger.info(f"🔢 총 처리 단계: {len(sources) * len(required_files) + 4} 단계")
progress_count = 0

# ✅ 4. GCS 파일 다운로드
def download_gcs_files(source):
    global progress_count
    logger.info(f"\n{'='*50}")
    logger.info(f"📥 {source.upper()} 데이터 다운로드 시작")
    logger.info(f"{'='*50}")
    
    for i, fname in enumerate(required_files):
        gcs_path = base_gcs.format(source=source, date=date_str, filename=fname)
        local_path = local_tmp.format(source=source, filename=fname)
        logger.info(f"⏱️ {datetime.now().strftime('%H:%M:%S')} - [{i+1}/{len(required_files)}] 다운로드 중: {gcs_path}")
        try:
            start_time = time.time()
            subprocess.run(["gsutil", "cp", gcs_path, local_path], check=True)
            elapsed = time.time() - start_time
            progress_count += 1
            overall_progress = (progress_count / (len(sources) * len(required_files) + 4)) * 100
            logger.info(f"✅ 다운로드 완료: {local_path} (소요시간: {elapsed:.2f}초)")
            logger.info(f"📊 전체 진행률: {overall_progress:.1f}%")
        except subprocess.CalledProcessError as e:
            logger.error(f"❌ 다운로드 실패: {gcs_path}")
            logger.error(f"  오류 코드: {e.returncode}")
            logger.error(f"  오류 내용: {e.output if hasattr(e, 'output') else '알 수 없음'}")
            continue
    logger.info(f"{'='*50}")

# ✅ 5. JSON 파일 로드
def load_json(source):
    logger.info(f"\n📂 {source.upper()} JSON 파일 로드 중...")
    data = {}
    for fname in required_files:
        path = local_tmp.format(source=source, filename=fname)
        try:
            with open(path, "r", encoding="utf-8") as f:
                key = fname.replace(".json", "")
                data[key] = json.load(f)
                logger.info(f"✅ {fname} 로드 완료 ({len(str(data[key]))} 바이트)")
        except FileNotFoundError:
            logger.error(f"❌ 파일을 찾을 수 없음: {path}")
        except json.JSONDecodeError:
            logger.error(f"❌ JSON 파싱 오류: {path}")
    return data

# ✅ 6. 보고서 구성 (대표 문장 출력 + GPT 요약 포함)
def build_report_text(news_data, reddit_data):
    logger.info(f"\n📝 보고서 구성 중...")
    
    def format_emotion_ratio(ratio):
        return "\n".join([f"- {emotion}: {info['percentage']}%" for emotion, info in ratio.items()])

    # ✅ 📍 여기서 보고서의 전체 구조 커스터마이징 가능
    text = f"""📅 Date: {date_str}
📡 Source: News + Reddit

📎 Visuals Included:
- news_umap_plot.png
- news_emotion_distribution.png
- reddit_umap_plot.png
- reddit_emotion_distribution.png

---

📊 Emotion Ratio (News):
{format_emotion_ratio(news_data['emotion_ratio'])}

📊 Emotion Ratio (Reddit):
{format_emotion_ratio(reddit_data['emotion_ratio'])}

---

📌 Summary (Generated by GPT):
"""
    logger.info(f"✅ 보고서 본문 구성 완료 ({len(text)} 자)")
    return text

# ✅ 7. GPT 요약 요청
def generate_summary(news_ratio, reddit_ratio):
    global progress_count
    logger.info(f"\n{'='*50}")
    logger.info(f"🧠 GPT 요약 생성 요청 중...")
    logger.info(f"{'='*50}")
    
    # ✅ 📍 여기서 GPT 문체나 프롬프트 문구 커스터마이징 가능
    prompt = f"""
You are a financial sentiment analyst.

Based on the following emotion ratio summaries, write:
1. An overall summary of the emotional state across both news and Reddit.
2. A comparative interpretation of how the emotions differ between the two sources and what that implies.

News emotion ratio:
{json.dumps(news_ratio, indent=2)}

Reddit emotion ratio:
{json.dumps(reddit_ratio, indent=2)}

Be concise, analytical, and professional. Write in English. Avoid redundancy.
"""
    logger.info(f"📤 GPT 프롬프트 길이: {len(prompt)} 자")
    
    try:
        start_time = time.time()
        logger.info(f"⏱️ {datetime.now().strftime('%H:%M:%S')} - API 호출 중...")
        response = openai.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "You are a professional financial sentiment analyst."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.4
        )
        elapsed = time.time() - start_time
        
        # 수정된 부분: 객체 속성으로 접근
        summary = response.choices[0].message.content
        
        progress_count += 1
        overall_progress = (progress_count / (len(sources) * len(required_files) + 4)) * 100
        
        logger.info(f"✅ GPT 응답 수신 완료 (소요시간: {elapsed:.2f}초)")
        logger.info(f"📊 전체 진행률: {overall_progress:.1f}%")
        logger.info(f"📝 요약 길이: {len(summary)} 자")
        return summary
    except Exception as e:
        logger.error(f"❌ GPT API 호출 실패: {e}")
        logger.error(f"  상세 오류: {str(e)}")
        return "API 호출 중 오류가 발생했습니다."

# ✅ 8. 보고서 저장 + 업로드
def save_report(text):
    global progress_count
    logger.info(f"\n{'='*50}")
    logger.info(f"💾 보고서 저장 및 업로드 중...")
    logger.info(f"{'='*50}")
    
    try:
        # 로컬 저장 - 인코딩 추가
        with open(report_local_path, "w", encoding="utf-8") as f:
            f.write(text)
        logger.info(f"✅ 로컬 저장 완료: {report_local_path} ({len(text)} 바이트)")
        
        # GCS 업로드
        start_time = time.time()
        subprocess.run(["gsutil", "cp", report_local_path, report_gcs_path], check=True)
        elapsed = time.time() - start_time
        
        progress_count += 1
        overall_progress = (progress_count / (len(sources) * len(required_files) + 4)) * 100
        
        logger.info(f"✅ GCS 업로드 완료: {report_gcs_path} (소요시간: {elapsed:.2f}초)")
        logger.info(f"📊 전체 진행률: {overall_progress:.1f}%")
    except Exception as e:
        logger.error(f"❌ 보고서 저장/업로드 실패: {e}")

# ✅ 9. 시각화 이미지 복사
def copy_visuals():
    global progress_count
    logger.info(f"\n{'='*50}")
    logger.info(f"🖼️ 시각화 이미지 복사 중...")
    logger.info(f"{'='*50}")
    
    total_files = len(sources) * 2
    copied = 0
    
    for src in sources:
        for vis in ["umap_plot.png", "emotion_distribution.png"]:
            src_path = f"gs://emotion-index-data/{src}/{date_str}/{vis}"
            dest_name = f"{src}_{vis}"
            dest_path = f"gs://emotion-index-data/final_anxiety_index/{date_str}/{dest_name}"
            
            logger.info(f"⏱️ {datetime.now().strftime('%H:%M:%S')} - [{copied+1}/{total_files}] 복사 중: {dest_name}")
            
            try:
                start_time = time.time()
                subprocess.run(["gsutil", "cp", src_path, dest_path], check=True)
                elapsed = time.time() - start_time
                copied += 1
                logger.info(f"✅ 복사 완료: {dest_path} (소요시간: {elapsed:.2f}초)")
            except subprocess.CalledProcessError as e:
                logger.error(f"❌ 이미지 복사 실패: {src_path} -> {dest_path}")
                logger.error(f"  오류 코드: {e.returncode}")
    
    progress_count += 1
    overall_progress = (progress_count / (len(sources) * len(required_files) + 4)) * 100
    logger.info(f"📊 전체 진행률: {overall_progress:.1f}%")

# ✅ 10. 전체 실행
logger.info(f"\n{'='*50}")
logger.info(f"🚀 GPT 리포트 생성 시작: {date_str}")
logger.info(f"{'='*50}")

start_total_time = time.time()
news_data, reddit_data = {}, {}

for src in sources:
    download_gcs_files(src)
    if src == "news":
        news_data = load_json(src)
    else:
        reddit_data = load_json(src)

text_body = build_report_text(news_data, reddit_data)
gpt_summary = generate_summary(news_data['emotion_ratio'], reddit_data['emotion_ratio'])
full_report = text_body + gpt_summary

save_report(full_report)
copy_visuals()

total_elapsed = time.time() - start_total_time
logger.info(f"\n{'='*50}")
logger.info(f"✅ 모든 작업 완료!")
logger.info(f"⏱️ 총 소요시간: {total_elapsed:.2f}초 ({total_elapsed/60:.2f}분)")
logger.info(f"📄 최종 보고서: {report_gcs_path}")
logger.info(f"{'='*50}\n")
