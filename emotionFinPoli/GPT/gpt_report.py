from datetime import datetime
import json
import openai
import subprocess
import os
import time
from dotenv import load_dotenv
import logging

# ë¡œê·¸ ë””ë ‰í† ë¦¬ ì„¤ì •
LOG_ROOT = "/home/hwangjeongmun691/logs"
today = datetime.utcnow().strftime("%Y-%m-%d")
LOG_DATE_DIR = f"{LOG_ROOT}/{today}"

# ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs(LOG_DATE_DIR, exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f"{LOG_DATE_DIR}/gpt_report.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("gpt_report")

# âœ… Load OpenAI API key from .env
logger.info(f"â±ï¸ {datetime.now().strftime('%H:%M:%S')} - í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ì¤‘...")
load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")
if openai.api_key:
    logger.info("âœ… API í‚¤ ë¡œë“œ ì™„ë£Œ")
else:
    logger.error("âŒ API í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")

# âœ… Set current UTC date
date_str = datetime.utcnow().strftime("%Y-%m-%d")
# date_str = "2025-04-08"
sources = ["news", "reddit"]
logger.info(f"ğŸ“… ì²˜ë¦¬ ë‚ ì§œ: {date_str}")

# âœ… GCS & local path formats
base_gcs = "gs://emotion-index-data/{source}/{date}/{filename}"
local_tmp = "/tmp/{source}_{filename}"
report_local_path = "/tmp/gpt_report_combined.txt"
appendix_local_path = "/tmp/gpt_report_appendix.txt"
report_gcs_path = f"gs://emotion-index-data/final_anxiety_index/{date_str}/gpt_report_combined.txt"
appendix_gcs_path = f"gs://emotion-index-data/final_anxiety_index/{date_str}/gpt_report_appendix.txt"

required_files = [
    "cluster_labels.json",
    "cluster_representative_texts.json",
    "emotion_ratio.json"
]

logger.info(f"ğŸ”¢ ì´ ì²˜ë¦¬ ë‹¨ê³„: {len(sources) * len(required_files) + 4} ë‹¨ê³„")
progress_count = 0

# âœ… 4. GCS íŒŒì¼ ë‹¤ìš´ë¡œë“œ
def download_gcs_files(source):
    global progress_count
    logger.info(f"\n{'='*50}")
    logger.info(f"ğŸ“¥ {source.upper()} ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì‹œì‘")
    logger.info(f"{'='*50}")
    
    for i, fname in enumerate(required_files):
        gcs_path = base_gcs.format(source=source, date=date_str, filename=fname)
        local_path = local_tmp.format(source=source, filename=fname)
        logger.info(f"â±ï¸ {datetime.now().strftime('%H:%M:%S')} - [{i+1}/{len(required_files)}] ë‹¤ìš´ë¡œë“œ ì¤‘: {gcs_path}")
        try:
            start_time = time.time()
            subprocess.run(["gsutil", "cp", gcs_path, local_path], check=True)
            elapsed = time.time() - start_time
            progress_count += 1
            overall_progress = (progress_count / (len(sources) * len(required_files) + 4)) * 100
            logger.info(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {local_path} (ì†Œìš”ì‹œê°„: {elapsed:.2f}ì´ˆ)")
            logger.info(f"ğŸ“Š ì „ì²´ ì§„í–‰ë¥ : {overall_progress:.1f}%")
        except subprocess.CalledProcessError as e:
            logger.error(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {gcs_path}")
            logger.error(f"  ì˜¤ë¥˜ ì½”ë“œ: {e.returncode}")
            logger.error(f"  ì˜¤ë¥˜ ë‚´ìš©: {e.output if hasattr(e, 'output') else 'ì•Œ ìˆ˜ ì—†ìŒ'}")
            continue
    logger.info(f"{'='*50}")

# âœ… 5. JSON íŒŒì¼ ë¡œë“œ
def load_json(source):
    logger.info(f"\nğŸ“‚ {source.upper()} JSON íŒŒì¼ ë¡œë“œ ì¤‘...")
    data = {}
    for fname in required_files:
        path = local_tmp.format(source=source, filename=fname)
        try:
            with open(path, "r", encoding="utf-8") as f:
                key = fname.replace(".json", "")
                data[key] = json.load(f)
                logger.info(f"âœ… {fname} ë¡œë“œ ì™„ë£Œ ({len(str(data[key]))} ë°”ì´íŠ¸)")
        except FileNotFoundError:
            logger.error(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {path}")
        except json.JSONDecodeError:
            logger.error(f"âŒ JSON íŒŒì‹± ì˜¤ë¥˜: {path}")
    return data

# âœ… 6. ë³´ê³ ì„œ êµ¬ì„± (ëŒ€í‘œ ë¬¸ì¥ ì¶œë ¥ + GPT ìš”ì•½ í¬í•¨)
def build_report_text(news_data, reddit_data):
    logger.info(f"\nğŸ“ ë³´ê³ ì„œ êµ¬ì„± ì¤‘...")
    
    def format_emotion_ratio(ratio):
        return "\n".join([f"- {emotion}: {info['percentage']}%" for emotion, info in ratio.items()])

    # âœ… ğŸ“ ì—¬ê¸°ì„œ ë³´ê³ ì„œì˜ ì „ì²´ êµ¬ì¡° ì»¤ìŠ¤í„°ë§ˆì´ì§• ê°€ëŠ¥
    text = f"""ğŸ“… Date: {date_str}
ğŸ“¡ Source: News + Reddit

ğŸ“ Visuals Included:
- news_umap_plot.png
- news_emotion_distribution.png
- reddit_umap_plot.png
- reddit_emotion_distribution.png

---

ğŸ“Š Emotion Ratio (News):
{format_emotion_ratio(news_data['emotion_ratio'])}

ğŸ“Š Emotion Ratio (Reddit):
{format_emotion_ratio(reddit_data['emotion_ratio'])}

---

ğŸ“Œ Summary (Generated by GPT):
"""
    logger.info(f"âœ… ë³´ê³ ì„œ ë³¸ë¬¸ êµ¬ì„± ì™„ë£Œ ({len(text)} ì)")
    return text

# âœ… 7. GPT ìš”ì•½ ìš”ì²­
def generate_summary(news_ratio, reddit_ratio):
    global progress_count
    logger.info(f"\n{'='*50}")
    logger.info(f"ğŸ§  GPT ìš”ì•½ ìƒì„± ìš”ì²­ ì¤‘...")
    logger.info(f"{'='*50}")
    
    # âœ… ğŸ“ ì—¬ê¸°ì„œ GPT ë¬¸ì²´ë‚˜ í”„ë¡¬í”„íŠ¸ ë¬¸êµ¬ ì»¤ìŠ¤í„°ë§ˆì´ì§• ê°€ëŠ¥
    prompt = f"""
You are a financial sentiment analyst.

Based on the following emotion ratio summaries, write:
1. An overall summary of the emotional state across both news and Reddit.
2. A comparative interpretation of how the emotions differ between the two sources and what that implies.

News emotion ratio:
{json.dumps(news_ratio, indent=2)}

Reddit emotion ratio:
{json.dumps(reddit_ratio, indent=2)}

Be concise, analytical, and professional. Write in English. Avoid redundancy.
"""
    logger.info(f"ğŸ“¤ GPT í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)} ì")
    
    try:
        start_time = time.time()
        logger.info(f"â±ï¸ {datetime.now().strftime('%H:%M:%S')} - API í˜¸ì¶œ ì¤‘...")
        response = openai.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "You are a professional financial sentiment analyst."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.4
        )
        elapsed = time.time() - start_time
        
        # ìˆ˜ì •ëœ ë¶€ë¶„: ê°ì²´ ì†ì„±ìœ¼ë¡œ ì ‘ê·¼
        summary = response.choices[0].message.content
        
        progress_count += 1
        overall_progress = (progress_count / (len(sources) * len(required_files) + 4)) * 100
        
        logger.info(f"âœ… GPT ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ (ì†Œìš”ì‹œê°„: {elapsed:.2f}ì´ˆ)")
        logger.info(f"ğŸ“Š ì „ì²´ ì§„í–‰ë¥ : {overall_progress:.1f}%")
        logger.info(f"ğŸ“ ìš”ì•½ ê¸¸ì´: {len(summary)} ì")
        return summary
    except Exception as e:
        logger.error(f"âŒ GPT API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        logger.error(f"  ìƒì„¸ ì˜¤ë¥˜: {str(e)}")
        return "API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

# âœ… 8. ë³´ê³ ì„œ ì €ì¥ + ì—…ë¡œë“œ
def save_report(text):
    global progress_count
    logger.info(f"\n{'='*50}")
    logger.info(f"ğŸ’¾ ë³´ê³ ì„œ ì €ì¥ ë° ì—…ë¡œë“œ ì¤‘...")
    logger.info(f"{'='*50}")
    
    try:
        # ë¡œì»¬ ì €ì¥ - ì¸ì½”ë”© ì¶”ê°€
        with open(report_local_path, "w", encoding="utf-8") as f:
            f.write(text)
        logger.info(f"âœ… ë¡œì»¬ ì €ì¥ ì™„ë£Œ: {report_local_path} ({len(text)} ë°”ì´íŠ¸)")
        
        # GCS ì—…ë¡œë“œ
        start_time = time.time()
        subprocess.run(["gsutil", "cp", report_local_path, report_gcs_path], check=True)
        elapsed = time.time() - start_time
        
        progress_count += 1
        overall_progress = (progress_count / (len(sources) * len(required_files) + 4)) * 100
        
        logger.info(f"âœ… GCS ì—…ë¡œë“œ ì™„ë£Œ: {report_gcs_path} (ì†Œìš”ì‹œê°„: {elapsed:.2f}ì´ˆ)")
        logger.info(f"ğŸ“Š ì „ì²´ ì§„í–‰ë¥ : {overall_progress:.1f}%")
    except Exception as e:
        logger.error(f"âŒ ë³´ê³ ì„œ ì €ì¥/ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")

# âœ… 9. ì‹œê°í™” ì´ë¯¸ì§€ ë³µì‚¬
def copy_visuals():
    global progress_count
    logger.info(f"\n{'='*50}")
    logger.info(f"ğŸ–¼ï¸ ì‹œê°í™” ì´ë¯¸ì§€ ë³µì‚¬ ì¤‘...")
    logger.info(f"{'='*50}")
    
    total_files = len(sources) * 2
    copied = 0
    
    for src in sources:
        for vis in ["umap_plot.png", "emotion_distribution.png"]:
            src_path = f"gs://emotion-index-data/{src}/{date_str}/{vis}"
            dest_name = f"{src}_{vis}"
            dest_path = f"gs://emotion-index-data/final_anxiety_index/{date_str}/{dest_name}"
            
            logger.info(f"â±ï¸ {datetime.now().strftime('%H:%M:%S')} - [{copied+1}/{total_files}] ë³µì‚¬ ì¤‘: {dest_name}")
            
            try:
                start_time = time.time()
                subprocess.run(["gsutil", "cp", src_path, dest_path], check=True)
                elapsed = time.time() - start_time
                copied += 1
                logger.info(f"âœ… ë³µì‚¬ ì™„ë£Œ: {dest_path} (ì†Œìš”ì‹œê°„: {elapsed:.2f}ì´ˆ)")
            except subprocess.CalledProcessError as e:
                logger.error(f"âŒ ì´ë¯¸ì§€ ë³µì‚¬ ì‹¤íŒ¨: {src_path} -> {dest_path}")
                logger.error(f"  ì˜¤ë¥˜ ì½”ë“œ: {e.returncode}")
    
    progress_count += 1
    overall_progress = (progress_count / (len(sources) * len(required_files) + 4)) * 100
    logger.info(f"ğŸ“Š ì „ì²´ ì§„í–‰ë¥ : {overall_progress:.1f}%")

# âœ… 10. ì „ì²´ ì‹¤í–‰
logger.info(f"\n{'='*50}")
logger.info(f"ğŸš€ GPT ë¦¬í¬íŠ¸ ìƒì„± ì‹œì‘: {date_str}")
logger.info(f"{'='*50}")

start_total_time = time.time()
news_data, reddit_data = {}, {}

for src in sources:
    download_gcs_files(src)
    if src == "news":
        news_data = load_json(src)
    else:
        reddit_data = load_json(src)

text_body = build_report_text(news_data, reddit_data)
gpt_summary = generate_summary(news_data['emotion_ratio'], reddit_data['emotion_ratio'])
full_report = text_body + gpt_summary

save_report(full_report)
copy_visuals()

total_elapsed = time.time() - start_total_time
logger.info(f"\n{'='*50}")
logger.info(f"âœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
logger.info(f"â±ï¸ ì´ ì†Œìš”ì‹œê°„: {total_elapsed:.2f}ì´ˆ ({total_elapsed/60:.2f}ë¶„)")
logger.info(f"ğŸ“„ ìµœì¢… ë³´ê³ ì„œ: {report_gcs_path}")
logger.info(f"{'='*50}\n")
