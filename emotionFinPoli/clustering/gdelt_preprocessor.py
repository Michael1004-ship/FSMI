from datetime import datetime
import os
import json
import time
import logging
import sys
import psutil
from io import BytesIO
from typing import List, Dict, Tuple
from tqdm import tqdm

import newspaper
from newspaper import Article
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
from google.cloud import storage

# ----------------------------
# ì„¤ì •
# ----------------------------
BUCKET_RAW = "emotion-raw-data"
BUCKET_INDEX = "emotion-index-data"
GDELT_PREFIX = "news/gdelt/"
SLEEP_SECONDS = 2.5
TARGET_DATE = datetime.utcnow().strftime("%Y-%m-%d")  # UTC ê¸°ì¤€ ì˜¤ëŠ˜
SAVE_FILENAME = "news_anxiety_index.csv"

# ----------------------------
# ë¡œê¹… ì„¤ì •
# ----------------------------
# ë¡œê·¸ ë””ë ‰í† ë¦¬ ì„¤ì •
LOG_ROOT = "/home/hwangjeongmun691/logs"
today = datetime.utcnow().strftime("%Y-%m-%d")
LOG_DATE_DIR = f"{LOG_ROOT}/{today}"

# ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs(LOG_DATE_DIR, exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f"{LOG_DATE_DIR}/gdelt_preprocessor.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("gdelt_preprocessor")

# ----------------------------
# ëª¨ë¸ ë¡œë”© (FinBERT)
# ----------------------------
tokenizer = AutoTokenizer.from_pretrained("yiyanghkust/finbert-tone")
model = AutoModelForSequenceClassification.from_pretrained("yiyanghkust/finbert-tone")
finbert = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)

# ----------------------------
# GCS í´ë¼ì´ì–¸íŠ¸
# ----------------------------
client = storage.Client()
bucket_raw = client.bucket(BUCKET_RAW)
bucket_index = client.bucket(BUCKET_INDEX)

def get_memory_usage() -> float:
    """í˜„ì¬ í”„ë¡œì„¸ìŠ¤ì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ MB ë‹¨ìœ„ë¡œ ë°˜í™˜"""
    process = psutil.Process(os.getpid())
    memory_info = process.memory_info()
    return memory_info.rss / 1024 / 1024  # MB ë‹¨ìœ„ë¡œ ë³€í™˜

def fetch_article_urls_from_accumulated_json(date: str) -> List[str]:
    """GCSì—ì„œ accumulated.json íŒŒì¼ì„ ì½ì–´ URL ëª©ë¡ ì¶”ì¶œ"""
    start_time = time.time()
    logger.info(f"ğŸ” URL ìˆ˜ì§‘ ì‹œì‘ (ë‚ ì§œ: {date})")
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê¸°ë¡
    initial_memory = get_memory_usage()
    logger.debug(f"ğŸ“Š ì´ˆê¸° ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {initial_memory:.2f} MB")
    
    blob_path = f"{GDELT_PREFIX}{date}/accumulated.json"
    blob = bucket_raw.blob(blob_path)
    
    if not blob.exists():
        logger.error(f"âŒ íŒŒì¼ ì—†ìŒ: gs://{BUCKET_RAW}/{blob_path}")
        raise FileNotFoundError(f"âŒ íŒŒì¼ ì—†ìŒ: gs://{BUCKET_RAW}/{blob_path}")
    
    # íŒŒì¼ í¬ê¸° í™•ì¸
    try:
        blob.reload()
        file_size_kb = blob.size / 1024 if blob.size is not None else 0
        logger.info(f"ğŸ“¦ íŒŒì¼ í¬ê¸°: {file_size_kb:.2f} KB")
    except Exception as e:
        logger.warning(f"âš ï¸ íŒŒì¼ í¬ê¸° í™•ì¸ ì˜¤ë¥˜: {e}")
        file_size_kb = 0
    
    # ë‹¤ìš´ë¡œë“œ ì‹œê°„ ì¸¡ì •
    download_start = time.time()
    logger.info(f"â³ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì¤‘...")
    content = blob.download_as_bytes()
    download_time = time.time() - download_start
    logger.info(f"â±ï¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {download_time:.2f}ì´ˆ ({file_size_kb/download_time:.2f} KB/ì´ˆ)")
    
    # JSON íŒŒì‹±
    parse_start = time.time()
    items = json.load(BytesIO(content))
    parse_time = time.time() - parse_start
    logger.info(f"ğŸ“Š JSON íŒŒì‹± ì™„ë£Œ: {parse_time:.2f}ì´ˆ, {len(items)}ê°œ í•­ëª©")
    
    # URL ì¶”ì¶œ
    urls = [item["DocumentIdentifier"] for item in items if "DocumentIdentifier" in item]
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
    current_memory = get_memory_usage()
    logger.debug(f"ğŸ“Š URL ì¶”ì¶œ í›„ ë©”ëª¨ë¦¬: {current_memory:.2f} MB (ì¦ê°€: {current_memory - initial_memory:.2f} MB)")
    
    # ì‹¤í–‰ ì‹œê°„ ê³„ì‚°
    total_time = time.time() - start_time
    logger.info(f"ğŸ”— ì´ {len(urls)}ê°œì˜ ê¸°ì‚¬ URL ìˆ˜ì§‘ë¨ (ì†Œìš” ì‹œê°„: {total_time:.2f}ì´ˆ)")
    
    return urls

def fetch_article_text(url: str) -> str:
    """ì‹ ë¬¸ ê¸°ì‚¬ URLì—ì„œ ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    try:
        config = newspaper.Config()
        config.browser_user_agent = "Mozilla/5.0"
        article = Article(url, config=config)
        
        # ë‹¤ìš´ë¡œë“œ ì‹œê°„ ì¸¡ì •
        download_start = time.time()
        article.download()
        download_time = time.time() - download_start
        
        # íŒŒì‹± ì‹œê°„ ì¸¡ì •
        parse_start = time.time()
        article.parse()
        parse_time = time.time() - parse_start
        
        text = article.text.strip()
        logger.debug(f"âœ“ í¬ë¡¤ë§ ì„±ê³µ: ë‹¤ìš´ë¡œë“œ {download_time:.2f}ì´ˆ, íŒŒì‹± {parse_time:.2f}ì´ˆ, {len(text)} ê¸€ì")
        return text
    except Exception as e:
        logger.warning(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {url} - {e}")
        return None

def process_articles(urls: List[str]) -> Tuple[List[str], List[str], Dict]:
    """URL ëª©ë¡ì—ì„œ ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ ë° ì²˜ë¦¬ í†µê³„ ìƒì„±"""
    start_time = time.time()
    initial_memory = get_memory_usage()
    
    logger.info(f"ğŸ”„ ê¸°ì‚¬ í¬ë¡¤ë§ ì‹œì‘ (ì´ {len(urls)}ê°œ URL)")
    
    texts = []
    failed_urls = []
    stats = {
        "total": len(urls),
        "success": 0,
        "failed": 0,
        "empty": 0,
        "total_chars": 0,
        "avg_chars_per_article": 0
    }
    
    # tqdmìœ¼ë¡œ ì§„í–‰ë¥  í‘œì‹œ
    for i, url in enumerate(tqdm(urls, desc="ê¸°ì‚¬ ì²˜ë¦¬")):
        # 100ê°œë§ˆë‹¤ ì§„í–‰ ìƒí™© ë¡œê¹…
        if i > 0 and i % 100 == 0:
            elapsed = time.time() - start_time
            progress = i / len(urls) * 100
            urls_per_second = i / elapsed
            estimated_total = elapsed / i * len(urls)
            remaining = max(0, estimated_total - elapsed)
            
            logger.info(f"  â†’ {i}/{len(urls)} ì²˜ë¦¬ ì¤‘ ({progress:.1f}%) - "
                       f"ì†ë„: {urls_per_second:.1f}ê°œ/ì´ˆ, ë‚¨ì€ ì‹œê°„: {remaining/60:.1f}ë¶„")
            logger.info(f"  â†’ ì„±ê³µ: {stats['success']}ê°œ, ì‹¤íŒ¨: {stats['failed']}ê°œ")
        
        logger.debug(f"[{i+1}/{len(urls)}] URL ì²˜ë¦¬ ì¤‘: {url}")
        
        text = fetch_article_text(url)
        time.sleep(SLEEP_SECONDS)
        
        if text:
            if len(text) > 0:
                texts.append(text)
                stats["success"] += 1
                stats["total_chars"] += len(text)
            else:
                stats["empty"] += 1
                failed_urls.append(f"{url} (ë¹ˆ í…ìŠ¤íŠ¸)")
        else:
            stats["failed"] += 1
            failed_urls.append(url)
    
    # í†µê³„ ë§ˆë¬´ë¦¬
    process_time = time.time() - start_time
    final_memory = get_memory_usage()
    
    if stats["success"] > 0:
        stats["avg_chars_per_article"] = stats["total_chars"] / stats["success"]
    
    logger.info(f"â±ï¸ í¬ë¡¤ë§ ì™„ë£Œ: {process_time:.2f}ì´ˆ (í‰ê·  {process_time/len(urls):.2f}ì´ˆ/URL)")
    logger.info(f"ğŸ“Š ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {final_memory:.2f} MB (ì¦ê°€: {final_memory - initial_memory:.2f} MB)")
    
    return texts, failed_urls, stats

def save_texts_to_gcs(texts: List[str], date: str) -> float:
    """ì¶”ì¶œí•œ í…ìŠ¤íŠ¸ë¥¼ GCSì— ì €ì¥"""
    start_time = time.time()
    logger.info(f"ğŸ’¾ GCS ì €ì¥ ì‹œì‘: {len(texts)}ê°œ í…ìŠ¤íŠ¸")
    
    # JSON ë³€í™˜ ì‹œê°„ ì¸¡ì •
    json_start = time.time()
    json_content = json.dumps(texts, ensure_ascii=False, indent=2)
    json_time = time.time() - json_start
    
    content_size_kb = len(json_content) / 1024
    logger.info(f"â±ï¸ JSON ë³€í™˜: {json_time:.2f}ì´ˆ, í¬ê¸°: {content_size_kb:.2f} KB")
    
    # GCS ì—…ë¡œë“œ
    upload_start = time.time()
    output_path = f"{GDELT_PREFIX}{date}/news_text.json"
    blob = bucket_raw.blob(output_path)
    
    blob.upload_from_string(json_content, content_type='application/json')
    
    upload_time = time.time() - upload_start
    total_time = time.time() - start_time
    
    logger.info(f"â±ï¸ ì—…ë¡œë“œ ì™„ë£Œ: {upload_time:.2f}ì´ˆ ({content_size_kb/upload_time:.1f} KB/ì´ˆ)")
    logger.info(f"â±ï¸ ì´ ì €ì¥ ì‹œê°„: {total_time:.2f}ì´ˆ")
    logger.info(f"ğŸ“¤ ì €ì¥ ì™„ë£Œ: gs://{BUCKET_RAW}/{output_path} ({len(texts)}ê°œ í…ìŠ¤íŠ¸)")
    
    return content_size_kb

def save_failed_urls(failed_urls: List[str], date: str):
    """ì‹¤íŒ¨í•œ URL ëª©ë¡ì„ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥"""
    if not failed_urls:
        logger.info("ğŸ‘ ì‹¤íŒ¨í•œ URL ì—†ìŒ")
        return
    
    output_path = f"{GDELT_PREFIX}{date}/failed_urls.txt"
    blob = bucket_raw.blob(output_path)
    
    content = "\n".join(failed_urls)
    blob.upload_from_string(content, content_type='text/plain')
    
    logger.warning(f"âš ï¸ ì‹¤íŒ¨ URL ëª©ë¡ ì €ì¥: gs://{BUCKET_RAW}/{output_path} ({len(failed_urls)}ê°œ)")

def analyze_articles(urls: List[str]) -> Tuple[List, List]:
    results = []
    failed_urls = []
    details = []

    for i, url in enumerate(urls, 1):
        logger.info(f"[{i}/{len(urls)}] URL ì²˜ë¦¬ ì¤‘: {url}")
        text = fetch_article_text(url)
        time.sleep(SLEEP_SECONDS)

        if text:
            try:
                result = finbert(text[:512])[0]
                label = result["label"].lower()
                score = result["score"]
                if label == "negative":
                    results.append(score)
                details.append([url, label, score])
            except Exception as e:
                logger.error(f"FinBERT ì˜¤ë¥˜: {e}")
                failed_urls.append(url)
        else:
            failed_urls.append(url)

    return details, failed_urls

def save_results_to_gcs(details: List, failed_urls: List[str], date: str):
    total = len(details)
    negatives = [r for r in details if r[1] == "negative"]
    neg_ratio = len(negatives) / total if total > 0 else 0
    avg_neg_score = sum(r[2] for r in negatives) / len(negatives) if negatives else 0
    anxiety_index = neg_ratio * avg_neg_score

    temp_csv = "/tmp/news_anxiety_index.csv"
    with open(temp_csv, "w", encoding="utf-8") as f:
        f.write("negative_ratio,average_negative_score,anxiety_index\n")
        f.write(f"{neg_ratio:.3f},{avg_neg_score:.3f},{anxiety_index:.3f}\n\n")
        f.write("url,label,score\n")
        for row in details:
            f.write(f"{row[0]},{row[1]},{row[2]:.3f}\n")

    blob = bucket_index.blob(f"news/{date}/{SAVE_FILENAME}")
    blob.upload_from_filename(temp_csv)
    logger.info(f"âœ… ì§€ìˆ˜ ì €ì¥ ì™„ë£Œ â†’ gs://{BUCKET_INDEX}/news/{date}/{SAVE_FILENAME}")

    if failed_urls:
        temp_log = "/tmp/failed_urls.txt"
        with open(temp_log, "w") as f:
            for url in failed_urls:
                f.write(url + "\n")
        log_blob = bucket_index.blob(f"news/{date}/failed_urls.txt")
        log_blob.upload_from_filename(temp_log)
        logger.warning(f"âš ï¸ ì‹¤íŒ¨ URL ì €ì¥ ì™„ë£Œ â†’ gs://{BUCKET_INDEX}/news/{date}/failed_urls.txt")

if __name__ == "__main__":
    try:
        # ì „ì²´ ì‹œì‘ ì‹œê°„ ë° ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
        total_start_time = time.time()
        initial_memory = get_memory_usage()
        
        logger.info(f"ğŸš€ GDELT ë‰´ìŠ¤ ì „ì²˜ë¦¬ ì‹œì‘ (ë‚ ì§œ: {TARGET_DATE})")
        logger.info(f"ğŸ’» ì‹œìŠ¤í…œ ì •ë³´: CPU {psutil.cpu_percent()}%, ë©”ëª¨ë¦¬ {psutil.virtual_memory().percent}%")
        
        # 1. URL ìˆ˜ì§‘
        urls = fetch_article_urls_from_accumulated_json(TARGET_DATE)
        
        # 2. ê¸°ì‚¬ ì²˜ë¦¬
        texts, failed_urls, stats = process_articles(urls)
        
        # 3. ê²°ê³¼ ì €ì¥
        content_size_kb = save_texts_to_gcs(texts, TARGET_DATE)
        save_failed_urls(failed_urls, TARGET_DATE)
        
        # 4. ìµœì¢… ê²°ê³¼ ìš”ì•½
        total_time = time.time() - total_start_time
        final_memory = get_memory_usage()
        memory_diff = final_memory - initial_memory
        
        logger.info("=" * 60)
        logger.info(f"ğŸ‰ GDELT ë‰´ìŠ¤ ì „ì²˜ë¦¬ ì™„ë£Œ!")
        logger.info(f"â±ï¸ ì´ ì†Œìš” ì‹œê°„: {total_time:.2f}ì´ˆ ({total_time/60:.2f}ë¶„)")
        logger.info(f"ğŸ“Š ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {final_memory:.2f} MB (ì¦ê°€: {memory_diff:.2f} MB)")
        logger.info(f"ğŸ“ˆ ì²˜ë¦¬ ê²°ê³¼:")
        logger.info(f"  â€¢ ì²˜ë¦¬ëœ URL: {stats['total']}ê°œ")
        logger.info(f"  â€¢ ì„±ê³µ: {stats['success']}ê°œ ({stats['success']/stats['total']*100:.1f}%)")
        logger.info(f"  â€¢ ì‹¤íŒ¨: {stats['failed']}ê°œ ({stats['failed']/stats['total']*100:.1f}%)")
        logger.info(f"  â€¢ ë¹ˆ í…ìŠ¤íŠ¸: {stats['empty']}ê°œ")
        logger.info(f"  â€¢ ì´ í…ìŠ¤íŠ¸ í¬ê¸°: {content_size_kb:.2f} KB")
        if stats['success'] > 0:
            logger.info(f"  â€¢ í‰ê·  í…ìŠ¤íŠ¸ ê¸¸ì´: {stats['avg_chars_per_article']:.1f}ì")
        logger.info("=" * 60)
    
    except Exception as e:
        logger.error(f"âŒ í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        sys.exit(1)