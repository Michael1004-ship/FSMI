import os
import json
import time
import datetime
import logging
import psutil
import multiprocessing
from io import BytesIO
from typing import List, Dict, Tuple

import numpy as np
import pandas as pd
from tqdm import tqdm
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.feature_extraction.text import TfidfVectorizer
from google.cloud import storage
from sentence_transformers import SentenceTransformer
from joblib import Parallel, delayed

# ----------------------------
# Configuration
# ----------------------------
BUCKET_NAME = "emotion-raw-data"
OUTPUT_BUCKET = "emotion-index-data"
SUBREDDITS = [
    "anxiety", "depression", "dividends", "EconMonitor",
    "economics", "economy", "finance", "financialindependence",
    "investing", "MacroEconomics", "offmychest", "personalfinance",
    "StockMarket", "stocks", "wallstreetbets"
]
DATE = datetime.datetime.utcnow().strftime("%Y-%m-%d")  # ì´ ì¤„ì„ ì£¼ì„ ì²˜ë¦¬
# DATE = "2025-04-08"  # ê³ ì •ëœ ë‚ ì§œë¡œ ì„¤ì •
MODEL_NAME = "sentence-transformers/all-distilroberta-v1"
CPU_CORES = max(1, multiprocessing.cpu_count() - 1)  # 1ê°œ ì½”ì–´ëŠ” ë‚¨ê²¨ë‘ 

# ----------------------------
# Logging Setup
# ----------------------------
# ë¡œê·¸ ë””ë ‰í† ë¦¬ ì„¤ì •
LOG_ROOT = "/home/hwangjeongmun691/logs"
today = datetime.datetime.utcnow().strftime("%Y-%m-%d")
LOG_DATE_DIR = f"{LOG_ROOT}/{today}"

# ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs(LOG_DATE_DIR, exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f"{LOG_DATE_DIR}/reddit_clustering.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("reddit_clustering")

# ----------------------------
# Performance Monitoring
# ----------------------------
def get_memory_usage() -> float:
    """í˜„ì¬ í”„ë¡œì„¸ìŠ¤ì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ MB ë‹¨ìœ„ë¡œ ë°˜í™˜"""
    process = psutil.Process(os.getpid())
    memory_info = process.memory_info()
    return memory_info.rss / 1024 / 1024  # MB ë‹¨ìœ„ë¡œ ë³€í™˜

# ----------------------------
# GCSì—ì„œ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
# ----------------------------
def load_subreddit_texts(bucket_name: str, subreddit: str, date: str) -> List[str]:
    """ë‹¨ì¼ ì„œë¸Œë ˆë”§ì˜ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë¡œë“œ"""
    storage_client = storage.Client()
    bucket = storage_client.bucket(bucket_name)
    blob_path = f"sns/reddit/{subreddit}/{date}/reddit_text.json"
    blob = bucket.blob(blob_path)
    
    if blob.exists():
        logger.debug(f"ğŸ“¥ ë‹¤ìš´ë¡œë“œ ì¤‘: {blob_path}")
        content = blob.download_as_bytes()
        try:
            return json.load(BytesIO(content))
        except Exception as e:
            logger.warning(f"âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨: {blob_path} - {e}")
            return []
    else:
        logger.warning(f"âŒ íŒŒì¼ ì—†ìŒ: {blob_path}")
        return []

def load_all_reddit_texts(bucket_name: str, subreddits: List[str], date: str) -> Tuple[List[str], Dict[str, int]]:
    """ë³‘ë ¬ë¡œ ëª¨ë“  ì„œë¸Œë ˆë”§ì˜ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë¡œë“œ"""
    start_time = time.time()
    initial_memory = get_memory_usage()
    logger.info(f"ğŸ”„ Reddit í…ìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì‹œì‘ (ì„œë¸Œë ˆë”§ {len(subreddits)}ê°œ)")
    
    # ë³‘ë ¬ ì²˜ë¦¬ë¡œ ê° ì„œë¸Œë ˆë”§ ë°ì´í„° ë¡œë“œ
    logger.info(f"ğŸ’» ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘ (ì½”ì–´ {CPU_CORES}ê°œ ì‚¬ìš©)")
    
    results = Parallel(n_jobs=CPU_CORES)(
        delayed(load_subreddit_texts)(bucket_name, sub, date) 
        for sub in tqdm(subreddits, desc="ì„œë¸Œë ˆë”§ ë¡œë“œ")
    )
    
    # ê²°ê³¼ ì§‘ê³„
    all_texts = []
    subreddit_info = []
    subreddit_counts = {}
    
    for i, texts in enumerate(results):
        subreddit = subreddits[i]
        count = len(texts)
        subreddit_counts[subreddit] = count
        
        if count > 0:
            all_texts.extend(texts)
            subreddit_info.extend([subreddit] * count)
            
        logger.info(f"  â€¢ r/{subreddit}: {count}ê°œ ë¬¸ì¥")
    
    # ì„±ëŠ¥ ì¸¡ì •
    total_time = time.time() - start_time
    final_memory = get_memory_usage()
    memory_diff = final_memory - initial_memory
    
    logger.info(f"âœ… ì´ {len(all_texts)}ê°œ ë¬¸ì¥ ë¡œë“œ ì™„ë£Œ")
    logger.info(f"â±ï¸ ë¡œë“œ ì‹œê°„: {total_time:.2f}ì´ˆ")
    logger.info(f"ğŸ“Š ë©”ëª¨ë¦¬ ì‚¬ìš©: {final_memory:.2f} MB (ì¦ê°€: {memory_diff:.2f} MB)")
    
    return all_texts, subreddit_info, subreddit_counts

# ----------------------------
# ë¬¸ì¥ ì„ë² ë”©
# ----------------------------
def embed_texts(texts: List[str], model_name: str) -> np.ndarray:
    """ë¬¸ì¥ì„ ë²¡í„°ë¡œ ë³€í™˜"""
    start_time = time.time()
    initial_memory = get_memory_usage()
    logger.info(f"ğŸ§  ë¬¸ì¥ ì„ë² ë”© ì‹œì‘ (ëª¨ë¸: {model_name})")
    
    model = SentenceTransformer(model_name)
    logger.info(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_name}")
    
    # ë°°ì¹˜ í¬ê¸° ìµœì í™”
    batch_size = 32
    logger.info(f"âš™ï¸ ë°°ì¹˜ í¬ê¸°: {batch_size}, ì´ {len(texts)}ê°œ ë¬¸ì¥")
    
    # ì„ë² ë”© ê³„ì‚°
    embeddings = model.encode(
        texts, 
        show_progress_bar=True, 
        batch_size=batch_size,
        device='cuda' if hasattr(model, 'device') and model.device.type == 'cuda' else 'cpu'
    )
    
    # ì„±ëŠ¥ ì¸¡ì •
    total_time = time.time() - start_time
    final_memory = get_memory_usage()
    memory_diff = final_memory - initial_memory
    
    logger.info(f"âœ… ì„ë² ë”© ì™„ë£Œ: ë²¡í„° í¬ê¸° {embeddings.shape}")
    logger.info(f"â±ï¸ ì„ë² ë”© ì‹œê°„: {total_time:.2f}ì´ˆ ({len(texts)/total_time:.1f}ë¬¸ì¥/ì´ˆ)")
    logger.info(f"ğŸ“Š ë©”ëª¨ë¦¬ ì‚¬ìš©: {final_memory:.2f} MB (ì¦ê°€: {memory_diff:.2f} MB)")
    
    return embeddings

# ----------------------------
# Find Optimal K
# ----------------------------
def find_best_k(embeddings: np.ndarray, k_range=range(3, 11)) -> int:
    """ì‹¤ë£¨ì—£ ì ìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìµœì ì˜ í´ëŸ¬ìŠ¤í„° ìˆ˜ ê²°ì •"""
    start_time = time.time()
    logger.info(f"ğŸ” ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ íƒìƒ‰ ì‹œì‘ (ë²”ìœ„: {k_range.start}-{k_range.stop-1})")
    
    scores = []
    best_score = -1
    best_k = k_range.start
    
    for k in tqdm(k_range, desc="í´ëŸ¬ìŠ¤í„° ìˆ˜ í…ŒìŠ¤íŠ¸"):
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        labels = kmeans.fit_predict(embeddings)
        score = silhouette_score(embeddings, labels)
        scores.append(score)
        logger.info(f"  â€¢ k={k}, silhouette={score:.4f}")
        
        if score > best_score:
            best_score = score
            best_k = k
    
    total_time = time.time() - start_time
    logger.info(f"ğŸ† ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜: k={best_k} (score={best_score:.4f})")
    logger.info(f"â±ï¸ íƒìƒ‰ ì‹œê°„: {total_time:.2f}ì´ˆ")
    
    return best_k

# ----------------------------
# Extract Representative Texts (TF-IDF Based)
# ----------------------------
def extract_representative_texts(texts: List[str], labels: List[int], k: int, top_n: int = 5) -> dict:
    """TF-IDF ì ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ ëŒ€í‘œì ì¸ í…ìŠ¤íŠ¸ ì„ íƒ"""
    start_time = time.time()
    logger.info(f"ğŸ” í´ëŸ¬ìŠ¤í„° ëŒ€í‘œ ë¬¸ì¥ ì„ íƒ ì¤‘ (TF-IDF ê¸°ë°˜)...")
    
    result = {}
    
    for i in range(k):
        # í•´ë‹¹ í´ëŸ¬ìŠ¤í„°ì— ì†í•œ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
        cluster_indices = [j for j in range(len(texts)) if labels[j] == i]
        cluster_texts = [texts[j] for j in cluster_indices]
        
        if not cluster_texts:
            result[f"cluster_{i}_representative"] = ""
            result[f"cluster_{i}_size"] = 0
            logger.warning(f"  â€¢ í´ëŸ¬ìŠ¤í„° {i}ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤")
            continue
        
        # í´ëŸ¬ìŠ¤í„° í¬ê¸° ì €ì¥
        result[f"cluster_{i}_size"] = len(cluster_texts)
        logger.info(f"  â€¢ í´ëŸ¬ìŠ¤í„° {i} ì²˜ë¦¬ ì¤‘: {len(cluster_texts)}ê°œ í…ìŠ¤íŠ¸")
        
        try:
            # TF-IDF ê³„ì‚°
            vectorizer = TfidfVectorizer(stop_words='english')
            tfidf_matrix = vectorizer.fit_transform(cluster_texts)
            
            # ë¬¸ì„œë³„ TF-IDF ì ìˆ˜ í•©ê³„ (ë†’ì„ìˆ˜ë¡ ë” ëŒ€í‘œì )
            scores = tfidf_matrix.sum(axis=1).A1
            
            # ìƒìœ„ Nê°œ ë¬¸ì„œ ì„ íƒ
            top_indices = scores.argsort()[::-1][:top_n]
            
            # ëŒ€í‘œ í…ìŠ¤íŠ¸ ì €ì¥
            result[f"cluster_{i}_representative"] = cluster_texts[top_indices[0]]
            
            # ìƒìœ„ Nê°œ í…ìŠ¤íŠ¸ë„ ì €ì¥
            for j, idx in enumerate(top_indices):
                result[f"cluster_{i}_top{j+1}"] = cluster_texts[idx]
                
            logger.info(f"  â€¢ í´ëŸ¬ìŠ¤í„° {i}ì—ì„œ {top_n}ê°œ ëŒ€í‘œ í…ìŠ¤íŠ¸ ì„ íƒ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"í´ëŸ¬ìŠ¤í„° {i} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            result[f"cluster_{i}_representative"] = ""
            if len(cluster_texts) > 0:
                result[f"cluster_{i}_representative"] = cluster_texts[0]
    
    total_time = time.time() - start_time
    logger.info(f"âœ… ëŒ€í‘œ ë¬¸ì¥ ì„ íƒ ì™„ë£Œ (ì†Œìš” ì‹œê°„: {total_time:.2f}ì´ˆ)")
    
    return result

# ----------------------------
# GCS ì €ì¥ í•¨ìˆ˜
# ----------------------------
def save_to_gcs(bucket_name, gcs_path, content, content_type="text/csv"):
    """ë°ì´í„°ë¥¼ GCSì— ì €ì¥"""
    start_time = time.time()
    logger.info(f"ğŸ’¾ GCS ì €ì¥ ì‹œì‘: gs://{bucket_name}/{gcs_path}")
    
    storage_client = storage.Client()
    bucket = storage_client.bucket(bucket_name)
    blob = bucket.blob(gcs_path)
    blob.upload_from_string(content, content_type=content_type)
    
    total_time = time.time() - start_time
    logger.info(f"âœ… GCS ì €ì¥ ì™„ë£Œ: gs://{bucket_name}/{gcs_path} (ì‹œê°„: {total_time:.2f}ì´ˆ)")

# ----------------------------
# ì„ë² ë”© ì €ì¥ í•¨ìˆ˜
# ----------------------------
def save_embeddings(embeddings: np.ndarray, output_bucket: str, date: str):
    """ì„ë² ë”© ê²°ê³¼ë¥¼ NumPy í˜•ì‹ìœ¼ë¡œ GCSì— ì €ì¥"""
    start_time = time.time()
    logger.info(f"ğŸ’¾ ì„ë² ë”© ì €ì¥ ì‹œì‘ (NumPy í˜•ì‹)...")

    # NumPy í˜•ì‹ìœ¼ë¡œ ì €ì¥ (.npy)
    np_buffer = BytesIO()
    np.save(np_buffer, embeddings)
    np_buffer.seek(0)
    
    gcs_npy_path = f"reddit/{date}/embeddings.npy"
    save_to_gcs(output_bucket, gcs_npy_path, np_buffer.getvalue(), content_type="application/octet-stream")
    
    total_time = time.time() - start_time
    logger.info(f"âœ… ì„ë² ë”© ì €ì¥ ì™„ë£Œ: gs://{output_bucket}/{gcs_npy_path} (ì‹œê°„: {total_time:.2f}ì´ˆ)")

# ----------------------------
# Main Execution
# ----------------------------
if __name__ == "__main__":
    # ì „ì²´ ì‹œì‘ ì‹œê°„ ë° ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
    total_start_time = time.time()
    initial_memory = get_memory_usage()
    
    logger.info("=" * 60)
    logger.info("ğŸš€ Reddit í´ëŸ¬ìŠ¤í„°ë§ ì‹œì‘")
    logger.info(f"ğŸ“… ëŒ€ìƒ ë‚ ì§œ: {DATE}")
    logger.info(f"ğŸ’» ì‹œìŠ¤í…œ ì •ë³´: CPU {psutil.cpu_percent()}%, ë©”ëª¨ë¦¬ {psutil.virtual_memory().percent}%")
    logger.info("=" * 60)
    
    try:
        # 1. ëª¨ë“  ì„œë¸Œë ˆë”§ í…ìŠ¤íŠ¸ ë¡œë“œ
        texts, subreddit_info, subreddit_counts = load_all_reddit_texts(BUCKET_NAME, SUBREDDITS, DATE)
        
        if not texts:
            logger.error("âŒ ì²˜ë¦¬í•  í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.")
            exit(1)
        
        # 2. ë¬¸ì¥ ì„ë² ë”©
        embeddings = embed_texts(texts, MODEL_NAME)
        
        # ì„ë² ë”© ì €ì¥ (NumPy í˜•ì‹ë§Œ)
        save_embeddings(embeddings, OUTPUT_BUCKET, DATE)

        # 3. ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ ì°¾ê¸°
        best_k = find_best_k(embeddings)
        
        # 4. í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
        logger.info(f"ğŸ”„ KMeans í´ëŸ¬ìŠ¤í„°ë§ ì‹œì‘ (k={best_k})")
        kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)
        labels = kmeans.fit_predict(embeddings)
        
        # í´ëŸ¬ìŠ¤í„°ë³„ ë¶„í¬ ê³„ì‚°
        unique, counts = np.unique(labels, return_counts=True)
        for u, c in zip(unique, counts):
            logger.info(f"  â€¢ í´ëŸ¬ìŠ¤í„° {u}: {c}ê°œ ë¬¸ì¥ ({c/len(labels)*100:.1f}%)")
        
        # 5. ëŒ€í‘œ ë¬¸ì¥ ì¶”ì¶œ (TF-IDF ê¸°ë°˜)
        reps = extract_representative_texts(texts, labels, best_k)
        
        # 6. í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ì €ì¥
        cluster_data = []
        for i in range(len(texts)):
            cluster_data.append({
                "text": texts[i],
                "cluster": int(labels[i]),
                "subreddit": subreddit_info[i] if i < len(subreddit_info) else ""
            })
        
        # JSONìœ¼ë¡œ ì €ì¥
        cluster_data_json = json.dumps(cluster_data, ensure_ascii=False)
        save_to_gcs(OUTPUT_BUCKET, f"reddit/{DATE}/cluster_data.json", cluster_data_json, content_type="application/json")
        
        # ëŒ€í‘œ ë¬¸ì¥ ì €ì¥
        json_path = f"reddit/{DATE}/cluster_representative_texts.json"
        json_content = json.dumps(reps, indent=2, ensure_ascii=False)
        save_to_gcs(OUTPUT_BUCKET, json_path, json_content, content_type="application/json")
        
        # í´ëŸ¬ìŠ¤í„° í†µê³„ ìš”ì•½
        cluster_stats = {"clusters": {}}
        for i in range(best_k):
            cluster_size = len([label for label in labels if label == i])
            
            # ì´ í´ëŸ¬ìŠ¤í„°ì— ì†í•œ ì„œë¸Œë ˆë”§ë³„ ë¶„í¬ ê³„ì‚°
            subreddit_dist = {}
            if len(subreddit_info) == len(texts):
                for idx, label in enumerate(labels):
                    if label == i:
                        sub = subreddit_info[idx]
                        subreddit_dist[sub] = subreddit_dist.get(sub, 0) + 1
            
            cluster_stats["clusters"][str(i)] = {
                "size": cluster_size,
                "percentage": cluster_size / len(labels) * 100,
                "representative": reps.get(f"cluster_{i}_representative", "")[:100] if isinstance(reps.get(f"cluster_{i}_representative"), str) else "",
                "subreddit_distribution": subreddit_dist
            }
        
        stats_path = f"reddit/{DATE}/cluster_stats.json"
        stats_content = json.dumps(cluster_stats, indent=2)
        save_to_gcs(OUTPUT_BUCKET, stats_path, stats_content, content_type="application/json")
        
        # ì™„ë£Œ í†µê³„
        total_time = time.time() - total_start_time
        final_memory = get_memory_usage()
        memory_diff = final_memory - initial_memory
        
        logger.info("=" * 60)
        logger.info("ğŸ‰ Reddit í´ëŸ¬ìŠ¤í„°ë§ ì „ì²´ ì™„ë£Œ!")
        logger.info(f"â±ï¸ ì´ ì†Œìš” ì‹œê°„: {total_time:.2f}ì´ˆ ({total_time/60:.2f}ë¶„)")
        logger.info(f"ğŸ“Š ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {final_memory:.2f} MB (ì¦ê°€: {memory_diff:.2f} MB)")
        logger.info(f"ğŸ“Š ì²˜ë¦¬ ê²°ê³¼:")
        logger.info(f"  â€¢ ì´ ì²˜ë¦¬ ë¬¸ì¥: {len(texts)}ê°œ")
        logger.info(f"  â€¢ í´ëŸ¬ìŠ¤í„° ìˆ˜: {best_k}ê°œ")
        logger.info(f"  â€¢ ì €ì¥ëœ íŒŒì¼:")
        logger.info(f"    - gs://{OUTPUT_BUCKET}/reddit/{DATE}/cluster_data.json")
        logger.info(f"    - gs://{OUTPUT_BUCKET}/reddit/{DATE}/cluster_representative_texts.json")
        logger.info(f"    - gs://{OUTPUT_BUCKET}/reddit/{DATE}/cluster_stats.json")
        logger.info("=" * 60)
        
    except Exception as e:
        logger.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
