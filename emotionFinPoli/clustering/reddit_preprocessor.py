import os
import json
import datetime
import time
import sys
import psutil
import logging
from google.cloud import storage
from tqdm import tqdm

# âœ… ë¡œê¹… ì„¤ì •
from datetime import datetime

# ë¡œê·¸ ë””ë ‰í† ë¦¬ ì„¤ì •
LOG_ROOT = "/home/hwangjeongmun691/logs"
today = datetime.utcnow().strftime("%Y-%m-%d")
LOG_DATE_DIR = f"{LOG_ROOT}/{today}"

# ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs(LOG_DATE_DIR, exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f"{LOG_DATE_DIR}/reddit_preprocessor.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("reddit_preprocessor")

# âœ… ì„¤ì •
BUCKET_NAME = "emotion-raw-data"
SUBREDDITS = [
    "anxiety", "depression", "dividends", "EconMonitor",
    "economics", "economy", "finance", "financialindependence",
    "investing", "MacroEconomics", "offmychest", "personalfinance",
    "StockMarket", "stocks", "wallstreetbets"
]

# âœ… ì˜¤ëŠ˜ ë‚ ì§œ (UTC ê¸°ì¤€ â†’ í•œêµ­ ê¸°ì¤€ì´ë©´ +9 ì„¤ì •)
today = datetime.utcnow().strftime("%Y-%m-%d")

def get_memory_usage():
    process = psutil.Process(os.getpid())
    memory_info = process.memory_info()
    return memory_info.rss / 1024 / 1024  # MB

def preprocess_reddit_from_gcs(bucket_name, gcs_path):
    start_time = time.time()
    logger.info(f"ğŸ”„ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹œì‘: gs://{bucket_name}/{gcs_path}")
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê¸°ë¡
    initial_memory = get_memory_usage()
    logger.debug(f"ğŸ“Š ì´ˆê¸° ë©”ëª¨ë¦¬: {initial_memory:.2f} MB")
    
    storage_client = storage.Client()
    bucket = storage_client.bucket(bucket_name)
    blob = bucket.blob(gcs_path)

    if not blob.exists():
        raise FileNotFoundError(f"âŒ íŒŒì¼ ì—†ìŒ: gs://{bucket_name}/{gcs_path}")
    
    # íŒŒì¼ í¬ê¸° í™•ì¸ (ì˜ˆì™¸ ì²˜ë¦¬ ì¶”ê°€)
    try:
        blob.reload()  # ë©”íƒ€ë°ì´í„° ë¡œë“œ
        file_size_kb = blob.size / 1024 if blob.size is not None else 0
        logger.info(f"ğŸ“¦ íŒŒì¼ í¬ê¸°: {file_size_kb:.2f} KB")
    except Exception as e:
        logger.warning(f"âš ï¸ íŒŒì¼ í¬ê¸° í™•ì¸ ì˜¤ë¥˜: {e}")
        file_size_kb = 0
    
    # ë‹¤ìš´ë¡œë“œ ì‹œê°„ ì¸¡ì •
    download_start = time.time()
    raw_content = blob.download_as_string()
    download_time = time.time() - download_start
    logger.info(f"â±ï¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {download_time:.2f}ì´ˆ ({file_size_kb/download_time:.2f} KB/ì´ˆ)")
    
    # JSON íŒŒì‹± ì‹œê°„ ì¸¡ì •
    parse_start = time.time()
    raw_data = json.loads(raw_content)
    parse_time = time.time() - parse_start
    logger.info(f"ğŸ“Š JSON íŒŒì‹±: {parse_time:.2f}ì´ˆ, {len(raw_data)}ê°œ í•­ëª© ë¡œë“œë¨")
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
    after_load_memory = get_memory_usage()
    memory_diff = after_load_memory - initial_memory
    logger.debug(f"ğŸ“Š ë¡œë“œ í›„ ë©”ëª¨ë¦¬: {after_load_memory:.2f} MB (ì¦ê°€: {memory_diff:.2f} MB)")
    
    # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì‹œì‘
    process_start = time.time()
    logger.info(f"ğŸ”„ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì‹œì‘ (ì´ {len(raw_data)}ê°œ í•­ëª©)...")
    
    texts = []
    empty_count = 0
    success_count = 0
    
    # ì§„í–‰ ìƒí™© í‘œì‹œ
    for i, item in enumerate(tqdm(raw_data, desc=f"ì „ì²˜ë¦¬ ì§„í–‰")):
        # 1000ê°œë§ˆë‹¤ ë¡œê¹…
        if i > 0 and i % 1000 == 0:
            progress = i / len(raw_data) * 100
            elapsed = time.time() - process_start
            items_per_sec = i / elapsed
            estimated_total = elapsed / i * len(raw_data)
            remaining = max(0, estimated_total - elapsed)
            
            logger.info(f"  â†’ {i}/{len(raw_data)} ì²˜ë¦¬ ì¤‘ ({progress:.1f}%) - "
                      f"ì†ë„: {items_per_sec:.1f}ê°œ/ì´ˆ, ë‚¨ì€ ì‹œê°„: {remaining:.1f}ì´ˆ")
        
        try:
            title = item.get("title") or ""
            selftext = item.get("selftext") or ""
            full_text = f"{title.strip()} {selftext.strip()}".strip()
            
            if full_text:
                texts.append(full_text)
                success_count += 1
            else:
                empty_count += 1
        except Exception as e:
            logger.warning(f"âš ï¸ í•­ëª© {i} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
    
    process_time = time.time() - process_start
    total_time = time.time() - start_time
    
    # ì²˜ë¦¬ ê²°ê³¼ í†µê³„
    logger.info(f"â±ï¸ ì „ì²˜ë¦¬ ì™„ë£Œ: {process_time:.2f}ì´ˆ ({len(raw_data)/process_time:.1f}ê°œ/ì´ˆ)")
    logger.info(f"ğŸ“ˆ ì²˜ë¦¬ ê²°ê³¼: ì´ {len(raw_data)}ê°œ ì¤‘ {success_count}ê°œ ì„±ê³µ, {empty_count}ê°œ ë¹ˆ í•­ëª©")
    logger.info(f"â±ï¸ ì´ ì†Œìš” ì‹œê°„: {total_time:.2f}ì´ˆ")
    
    # í…ìŠ¤íŠ¸ ìƒ˜í”Œ ì¶œë ¥
    if texts:
        sample_text = texts[0][:100] + "..." if len(texts[0]) > 100 else texts[0]
        logger.debug(f"ğŸ“ ìƒ˜í”Œ í…ìŠ¤íŠ¸: {sample_text}")
    
    return texts

def save_texts_to_gcs(bucket_name, gcs_path, texts):
    start_time = time.time()
    logger.info(f"ğŸ’¾ GCS ì €ì¥ ì‹œì‘: {len(texts)}ê°œ í…ìŠ¤íŠ¸")
    
    # JSON ë³€í™˜ ì‹œê°„ ì¸¡ì •
    json_start = time.time()
    json_content = json.dumps(texts, ensure_ascii=False, indent=2)
    json_time = time.time() - json_start
    
    content_size_kb = len(json_content) / 1024
    logger.info(f"â±ï¸ JSON ë³€í™˜: {json_time:.2f}ì´ˆ, í¬ê¸°: {content_size_kb:.2f} KB")
    
    # GCS ì—…ë¡œë“œ ì‹œê°„ ì¸¡ì •
    upload_start = time.time()
    
    storage_client = storage.Client()
    bucket = storage_client.bucket(bucket_name)
    blob = bucket.blob(gcs_path)
    
    blob.upload_from_string(
        data=json_content,
        content_type='application/json'
    )
    
    upload_time = time.time() - upload_start
    total_time = time.time() - start_time
    
    logger.info(f"â±ï¸ ì—…ë¡œë“œ ì™„ë£Œ: {upload_time:.2f}ì´ˆ ({content_size_kb/upload_time:.1f} KB/ì´ˆ)")
    logger.info(f"â±ï¸ ì´ ì €ì¥ ì‹œê°„: {total_time:.2f}ì´ˆ")
    logger.info(f"ğŸ“¤ ì €ì¥ ì™„ë£Œ: gs://{bucket_name}/{gcs_path}")
    
    return content_size_kb

if __name__ == "__main__":
    # ì „ì²´ ì‹œì‘ ì‹œê°„
    total_start_time = time.time()
    initial_memory = get_memory_usage()
    
    logger.info(f"ğŸš€ ì „ì²´ ì„œë¸Œë ˆë”§ ì „ì²˜ë¦¬ ì‹œì‘ - ë‚ ì§œ: {today}")
    logger.info(f"ğŸ’» ì‹œìŠ¤í…œ ì •ë³´: CPU {psutil.cpu_percent()}%, ë©”ëª¨ë¦¬ {psutil.virtual_memory().percent}%")
    logger.info(f"ğŸ“‹ ì²˜ë¦¬ ëŒ€ìƒ: {len(SUBREDDITS)}ê°œ ì„œë¸Œë ˆë”§")
    
    # ì‹¤í–‰ ê²°ê³¼ í†µê³„ìš© ë³€ìˆ˜
    results = {
        "success": 0,
        "not_found": 0,
        "error": 0,
        "total_items": 0,
        "total_texts": 0,
        "total_size_kb": 0
    }
    
    subreddit_stats = {}
    
    # ì„œë¸Œë ˆë”§ ì²˜ë¦¬ ì§„í–‰ìœ¨ í‘œì‹œ
    for idx, sub in enumerate(SUBREDDITS):
        sub_start_time = time.time()
        
        # ì§„í–‰ìœ¨ í‘œì‹œ
        progress = (idx / len(SUBREDDITS)) * 100
        elapsed = time.time() - total_start_time
        remaining = 0 if idx == 0 else (elapsed / idx) * (len(SUBREDDITS) - idx)
        
        logger.info("=" * 60)
        logger.info(f"ğŸ”„ ì§„í–‰ë¥ : {progress:.1f}% ({idx+1}/{len(SUBREDDITS)}) - "
                  f"ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {remaining/60:.1f}ë¶„")
        logger.info(f"ğŸ“‚ ì²˜ë¦¬ ëŒ€ìƒ: r/{sub}")
        
        input_path = f"sns/reddit/{sub}/{today}/accumulated.json"
        output_path = f"sns/reddit/{sub}/{today}/reddit_text.json"
        
        logger.info(f"ğŸ“¥ ì…ë ¥: gs://{BUCKET_NAME}/{input_path}")
        logger.info(f"ğŸ“¤ ì¶œë ¥: gs://{BUCKET_NAME}/{output_path}")
        logger.info("-" * 60)

        try:
            texts = preprocess_reddit_from_gcs(BUCKET_NAME, input_path)
            size_kb = save_texts_to_gcs(BUCKET_NAME, output_path, texts)
            
            sub_time = time.time() - sub_start_time
            logger.info(f"âœ… r/{sub} ì „ì²˜ë¦¬ ì™„ë£Œ: {len(texts)}ê°œ í…ìŠ¤íŠ¸, ì†Œìš” ì‹œê°„: {sub_time:.2f}ì´ˆ")
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            results["success"] += 1
            results["total_texts"] += len(texts)
            results["total_size_kb"] += size_kb
            
            # ì„œë¸Œë ˆë”§ë³„ í†µê³„
            subreddit_stats[sub] = {
                "status": "success",
                "texts": len(texts),
                "size_kb": size_kb,
                "time": sub_time
            }
            
        except FileNotFoundError:
            logger.warning(f"âš ï¸ r/{sub}: accumulated.json ì—†ìŒ â†’ ê±´ë„ˆëœ€")
            results["not_found"] += 1
            subreddit_stats[sub] = {"status": "not_found", "time": time.time() - sub_start_time}
            
        except Exception as e:
            logger.error(f"âŒ r/{sub} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            results["error"] += 1
            subreddit_stats[sub] = {"status": "error", "error": str(e), "time": time.time() - sub_start_time}
    
    # ì´ ì†Œìš” ì‹œê°„ ê³„ì‚°
    total_time = time.time() - total_start_time
    final_memory = get_memory_usage()
    memory_diff = final_memory - initial_memory
    
    # ìµœì¢… ê²°ê³¼ ìš”ì•½
    logger.info("=" * 60)
    logger.info(f"ğŸ‰ ëª¨ë“  ì„œë¸Œë ˆë”§ ì „ì²˜ë¦¬ ì™„ë£Œ!")
    logger.info(f"â±ï¸ ì´ ì†Œìš” ì‹œê°„: {total_time:.2f}ì´ˆ ({total_time/60:.2f}ë¶„)")
    logger.info(f"ğŸ“Š ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {final_memory:.2f} MB (ì¦ê°€: {memory_diff:.2f} MB)")
    logger.info(f"ğŸ“ˆ ì²˜ë¦¬ ê²°ê³¼:")
    logger.info(f"  â€¢ ì„±ê³µ: {results['success']}/{len(SUBREDDITS)}ê°œ ì„œë¸Œë ˆë”§")
    logger.info(f"  â€¢ íŒŒì¼ ì—†ìŒ: {results['not_found']}ê°œ")
    logger.info(f"  â€¢ ì˜¤ë¥˜: {results['error']}ê°œ")
    logger.info(f"  â€¢ ì´ í…ìŠ¤íŠ¸ ìˆ˜: {results['total_texts']}ê°œ")
    logger.info(f"  â€¢ ì´ ë°ì´í„° í¬ê¸°: {results['total_size_kb']:.2f} KB")
    
    # ì„œë¸Œë ˆë”§ë³„ ìƒì„¸ ê²°ê³¼
    logger.info("\nğŸ” ì„œë¸Œë ˆë”§ë³„ ì²˜ë¦¬ ê²°ê³¼:")
    for sub, stats in subreddit_stats.items():
        status = "âœ… ì„±ê³µ" if stats["status"] == "success" else "âš ï¸ íŒŒì¼ ì—†ìŒ" if stats["status"] == "not_found" else "âŒ ì˜¤ë¥˜"
        details = ""
        if stats["status"] == "success":
            details = f"{stats['texts']}ê°œ í…ìŠ¤íŠ¸, {stats['size_kb']:.2f} KB"
        elif stats["status"] == "error":
            details = f"ì˜¤ë¥˜: {stats['error']}"
        
        logger.info(f"  â€¢ r/{sub}: {status} - {details} (ì†Œìš” ì‹œê°„: {stats['time']:.2f}ì´ˆ)")
    
    logger.info("=" * 60)
